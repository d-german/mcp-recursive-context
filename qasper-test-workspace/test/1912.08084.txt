# A Context-Aware Approach for Detecting Check-Worthy Claims in Political Debates

**Paper ID:** 1912.08084

## Abstract

In the context of investigative journalism, we address the problem of automatically identifying which claims in a given document are most worthy and should be prioritized for fact-checking. Despite its importance, this is a relatively understudied problem. Thus, we create a new dataset of political debates, containing statements that have been fact-checked by nine reputable sources, and we train machine learning models to predict which claims should be prioritized for fact-checking, i.e., we model the problem as a ranking task. Unlike previous work, which has looked primarily at sentences in isolation, in this paper we focus on a rich input representation modeling the context: relationship between the target statement and the larger context of the debate, interaction between the opponents, and reaction by the moderator and by the public. Our experiments show state-of-the-art results, outperforming a strong rivaling system by a margin, while also confirming the importance of the contextual information.

## Introduction

The current coverage of the political landscape in the press and in social media has led to an unprecedented situation. Like never before, a statement in an interview, a press release, a blog note, or a tweet can spread almost instantaneously and reach the public in no time. This proliferation speed has left little time for double-checking claims against the facts, which has proven critical in politics, e.g., during the 2016 presidential campaign in the USA, which was arguably impacted by fake news in social media and by false claims.

Investigative journalists and volunteers have been working hard trying to get to the root of a claim and to present solid evidence in favor or against it. Manual fact-checking has proven very time-consuming, and thus automatic methods have been proposed as a way to speed-up the process. For instance, there has been work on checking the factuality/credibility of a claim, of a news article, or of an information source BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7. However, less attention has been paid to other steps of the fact-checking pipeline, which is shown in Figure FIGREF1.

The process starts when a document is made public. First, an intrinsic analysis is carried out in which check-worthy text fragments are identified. Then, other documents that might support or rebut a claim in the document are retrieved from various sources. Finally, by comparing a claim against the retrieved evidence, a system can determine whether the claim is likely true or likely false. For instance, BIBREF8 do this on the basis of a knowledge graph derived from Wikipedia. The outcome could then be presented to a human expert for final judgment.

In this paper, we focus on the first step: predicting check-worthiness of claims. Our contributions can be summarized as follows:

New dataset: We build a new dataset of manually-annotated claims, extracted from the 2016 US presidential and vice-presidential debates, which we gathered from nine reputable sources such as CNN, NPR, and PolitiFact, and which we release to the research community.

Modeling the context: We develop a novel approach for automatically predicting which claims should be prioritized for fact-checking, based on a rich input representation. In particular, we model not only the textual content, but also the context: how the target claim relates to the current segment, to neighboring segments and sentences, and to the debate as a whole, and also how the opponents and the public react to it.

State-of-the-art results: We achieve state-of-the-art results, outperforming a strong rivaling system by a margin, while also demonstrating that this improvement is due primarily to our modeling of the context.

We model the problem as a ranking task, and we train both Support Vector Machines (SVM) and Feed-forward Neural Networks (FNN) obtaining state-of-the-art results. We also analyze the relevance of the specific feature groups and we show that modeling the context yields a significant boost in performance. Finally, we also analyze whether we can learn to predict which facts are check-worthy with respect to each of the individual media sources, thus capturing their biases. It is worth noting that while trained on political debates, many features of our model can be potentially applied to other kinds of information sources, e.g., interviews and news.

The rest of the paper is organized as follows: Section SECREF2 discusses related work. Section SECREF3 describes the process of gathering and annotating our political debates dataset. Section SECREF4 presents our supervised approach to predicting fact-checking worthiness, including the explanation of the model and the information sources we use. Section SECREF5 presents the evaluation setup and discusses the results. Section SECREF6 provides further analysis. Finally, Section SECREF7 presents the conclusions and outlines some possible directions for future research.

## Related Work

The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by students, university professors, and journalists. Unfortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement.

In contrast, we develop a new publicly-available dataset, based on manual annotations of political debates by nine highly-reputed fact-checking sources, where sentences are annotated in the context of the entire debate. This allows us to explore a novel approach, which focuses on the context.

Note also that the ClaimBuster dataset is annotated following guidelines from BIBREF9 rather than a real fact-checking website; yet, it was evaluated against CNN and PolitiFact BIBREF10. In contrast, we train and evaluate directly on annotations from fact-checking websites, and thus we learn to fit them better.

Beyond the document context, it has been proposed to mine check-worthy claims on the Web. For example, BIBREF11 searched for linguistic cues of disagreement between the author of a statement and what is believed, e.g., “falsely claimed that X”. The claims matching the patterns go through a statistical classifier, which marks the text of the claim. This procedure can be used to acquire a dataset of disputed claims from the Web.

Given a set of disputed claims, BIBREF12 approached the task as locating new claims on the Web that entail the ones that have already been collected. Thus, the task can be conformed as recognizing textual entailment, which is analyzed in detail in BIBREF13.

Finally, BIBREF14 argued that the top terms in claim vs. non-claim sentences are highly overlapping, which is a problem for bag-of-words approaches. Thus, they used a Convolutional Neural Network, where each word is represented by its embedding and each named entity is replaced by its tag, e.g., person, organization, location.

## The CW-USPD-2016 dataset on US Presidential Debates

We created a new dataset called CW-USPD-2016 (check-worthiness in the US presidential debates 2016) for finding check-worthy claims in context. In particular, we used four transcripts of the 2016 US election: one vice-presidential and three presidential debates. For each debate, we used the publicly-available manual analysis about it from nine reputable fact-checking sources, as shown in Table TABREF7. This could include not just a statement about factuality, but any free text that journalists decided to add, e.g., links to biographies or behavioral analysis of the opponents and moderators. We converted this to binary annotation about whether a particular sentence was annotated for factuality by a given source. Whenever one or more annotations were about part of a sentence, we selected the entire sentence, and when an annotation spanned over multiple sentences, we selected each of them.

Ultimately, we ended up with a dataset of four debates, with a total of 5,415 sentences. The agreement between the sources was low as Table TABREF8 shows: only one sentence was selected by all nine sources, 57 sentences by at least five, 197 by at least three, 388 by at least two, and 880 by at least one. The reason for this is that the different media aimed at annotating sentences according to their own editorial line, rather than trying to be exhaustive in any way. This suggests that the task of predicting which sentence would contain check-worthy claims will be challenging. Thus, below we focus on a ranking task rather than on absolute predictions. Moreover, we predict which sentence would be selected (i) by at least one of the media, or (ii) by a specific medium.

Note that the investigative journalists did not select the check-worthy claims in isolation. Our analysis shows that these include claims that were highly disputed during the debate, that were relevant to the topic introduced by the moderator, etc. We will make use of these contextual dependencies below, which is something that was not previously tried in related work.

## Modeling Check-Worthiness

We developed a rich input representation in order to model and to learn the check-worthiness concept. The feature types we implemented operate at the sentence- (S) and at the context-level (C), in either case targeting segments by the same speaker. The context features are novel and a contribution of this study. We also implemented a set of core features to compare to the state of the art. All of them are described below.

## Modeling Check-Worthiness ::: Sentence-Level Features

ClaimBuster-based (1,045 S features; core): First, in order to be able to compare our model and features directly to the previous state of the art, we re-implemented, to the best of our ability, the sentence-level features of ClaimBuster as described in BIBREF9, namely TF.IDF-weighted bag of words (998 features), part-of-speech tags (25 features), named entities as recognized by Alchemy API (20 features), sentiment score from Alchemy API (1 feature), and number of tokens in the target sentence (1 feature).

Apart from providing means of comparison to the state of the art, these features also make a solid contribution to the final system we build for check-worthiness estimation. However, note that we did not have access to the training data of ClaimBuster, which is not publicly available, and we thus train on our dataset (described above).

Sentiment (2 S features): Some sentences are highly negative, which can signal the presence of an interesting claim to check, as the two example sentences below show (from the 1st and the 2nd presidential debates):

We used the NRC sentiment lexicon BIBREF15 as a source of words and $n$-grams with positive/negative sentiment, and we counted the number of positive and of negative words in the target sentence. These features are different from those in the CB features above, where these lexicons were not used.

Named entities (NE) (1 S feature): Sentences that contain named entity mentions are more likely to contain a claim that is worth fact-checking as they discuss particular people, organizations, and locations. Thus, we have a feature that counts the number of named entity mentions in the target sentence; we use the NLTK toolkit for named entity recognition BIBREF16. Unlike the CB features above, here we only have one feature; we also use a different toolkit for named entity recognition.

Linguistic features (9 S features): We count the number of words in each sentence that belong to each of the following lexicons: Language Bias lexicon BIBREF17, Opinion Negative and Positive Words BIBREF18, Factives and Assertive Predicates BIBREF19, Hedges BIBREF20, Implicatives BIBREF21, and Strong and Weak subjective words. Some examples are shown in Table TABREF12.

Tense (1 S feature): Most of the check-worthy claims mention past events. In order to detect when the speaker is making a reference to the past or s/he is talking about his/her future vision and plans, we include a feature with three values—indicating whether the text is in past, present, or future tense. The feature is extracted from the verbal expressions, using POS tags and a list of auxiliary verbs and phrases such as will, have to, etc.

Length (1 S feature): Shorter sentences are generally less likely to contain a check-worthy claim. Thus, we have a feature for the length of the sentence in terms of characters. Note that this feature was not part of the CB features, as there length was modeled in terms of tokens, but here we do so using characters.

## Modeling Check-Worthiness ::: Contextual Features

Position (3 C features): A sentence on the boundaries of a speaker's segment could contain a reaction to another statement or could provoke a reaction, which in turn could signal a check-worthy claim. Thus, we added information about the position of the target sentence in its segment: whether it is first/last, as well as its reciprocal rank in the list of sentences in that segment.

Segment sizes (3 C features): The size of the segment belonging to one speaker might indicate whether the target sentence is part of a long speech, makes a short comment or is in the middle of a discussion with lots of interruptions. The size of the previous and of the next segments is also important in modeling the dialogue flow. Thus, we include three features with the sizes of the previous, the current and the next segments.

Metadata (8 C features): Check-worthy claims often contain mutual accusations between the opponents, as the following example shows (from the 2nd presidential debate):

Thus, we use a feature that indicates whether the target sentence mentions the name of the opponent, whether the speaker is the moderator, and also who is speaking (3 features). We further use three binary features, indicating whether the target sentence is followed by a system message: applause, laugh, or cross-talk.

## Modeling Check-Worthiness ::: Mixed Features

The feature groups in this subsection contain a mixture of sentence- and of contextual-level features. For example, if we use a discourse parser to parse the target sentence only, any features we extract from the parse would be sentence-level. However, if we parse an entire segment, we would also have contextual features.

Topics (300+3 S+C features): Some topics are more likely to be associated with check-worthy claims, and thus we have features modeling the topics in the target sentence as well as in the surrounding context. We trained a Latent Dirichlet Allocation (LDA) topic model BIBREF22 on all political speeches and debates in The American Presidency Project using all US presidential debates in the 2007–2016 period. We had 300 topics, and we used the distribution over the topics as a representation for the target sentence. We further modeled the context using cosines with such representations for the previous, the current, and the next segment.

Embeddings (300+3 S+C features): We also modeled semantics using word embeddings. We used the pre-trained 300-dimensional Google News word embeddings by BIBREF23 to compute an average embedding vector for the target sentence, and we used the 300 coordinates of that vector. We also modeled the context as the cosine between that vector and the vectors for three segments: the previous, the current, and the following one.

Discourse (2+18 S+C features): We saw above that contradiction can signal the presence of check-worthy claims, and contradiction can be expressed by a discourse relation such as Contrast. As other discourse relations such as Background, Cause, and Elaboration can also be useful, we used a discourse parser BIBREF24 to parse the entire segment, and we focused on the relationship between the target sentence and the other sentences in its segment; this gave rise to 18 contextual indicator features. We further analyzed the internal structure of the target sentence —how many nuclei and how many satellites it contains—, which gave rise to two sentence-level features.

Contradictions (1+4 S+C features): Many claims selected for fact-checking contain contradictions to what has been said earlier, as in the example below (from the 3rd presidential debate):

We model this by counting the negations in the target sentence as found in a dictionary of negation cues such as not, didn't, and never. We further model the context as the number of such cues in the two neighboring sentences from the same segment and the two neighboring segments.

Similarity to known positive/negative examples (kNN) (2+1 S+C features): We used three more features inspired by $k$-nearest neighbor (kNN) classification. The first one (sentence-level) uses the maximum over the training sentences of the number of matching words between the testing and the training sentence, which is further multiplied by $-1$ if the latter was not check-worthy. We also used another version of the feature, where we multiplied it by 0 if the speakers were different (contextual). A third version took as a training set all claims checked by PolitiFact (excluding the target sentence).

## Experiments and Evaluation

In this section, we describe our evaluation setup and the obtained results.

## Experiments and Evaluation ::: Experimental Setting

We experimented with two learning algorithms. The first one is an SVM classifier with an RBF kernel. The second one is a deep feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification. We used ReLU BIBREF25 as the activation function and we trained the network with Stochastic Gradient Descent BIBREF26.

The models were trained to classify sentences as positive if one or more media had fact-checked a claim inside the target sentence, and negative otherwise. We then used the classifier scores to rank the sentences with respect to check-worthiness.

We tuned the parameters and we evaluated the performance using 4-fold cross-validation, using each of the four debates in turn for testing while training on the remaining three ones.

For evaluation, we used ranking measures such as Precision at $k$ ($P@k$) and Mean Average Precision (MAP). As Table TABREF7 shows, most media rarely check more than 50 claims per debate. NPR and PolitiFact are notable exceptions, the former going up to 99; yet, on average there are two claims per sentence, which means that there is no need to fact-check more than 50 sentences even for them. Thus, we report $P@k$ for $k \in \lbrace 5, 10, 20, 50\rbrace $.

MAP is the mean of the Average Precision across the four debates. The average precision for a debate is computed as follows:

where $n$ is the number of sentences to rank in the debate, $P(k)$ is the precision at $k$ and $rel(k)=1$ if the utterance at position $k$ is check-worthy, and it is 0 otherwise.

We also measure the recall at the $R$-th position of returned sentences for each debate. $R$ is the number of relevant documents for that debate and the metric is known as $R$-Precision ($R$-Pr).

## Experiments and Evaluation ::: Evaluation Results

Table TABREF21 shows the performance of our models when using all features described in Section SECREF4: see the SVM$_{All}$ and the FNN$_{All}$ rows. In order to put the numbers in perspective, we also show the results for five increasingly competitive baselines.

First, there is a random baseline, followed by an SVM classifier based on a bag-of-words representation with TF.IDF weights learned on the training data. Then come three versions of the ClaimBuster system: CB-Platform uses scores from the online demo, which we accessed on December 20, 2016, and SVM$_{CBfeat}$ and FNN$_{CBfeat}$ are our re-implementations, trained on our dataset.

We can see that all systems perform well above the random baseline. The three versions of ClaimBuster also outperform the TF.IDF baseline on most measures. Moreover, our reimplementation of ClaimBuster performs better than the online platform in terms of MAP. This is expected as their system is trained on a different dataset and it may suffer from testing on slightly out-of-domain data. At the same time, this is reassuring for our implementation of the features, and allows for a more realistic comparison to the ClaimBuster system.

More importantly, both the SVM and the FNN versions of our system consistently outperform all three versions of ClaimBuster on all measures. This means that the extra information coded in our model, mainly more linguistic, structural, and contextual features, has an important contribution to the overall performance.

We can further see that the neural network model, FNN$_{All}$, clearly outperforms the SVM model: consistently on all measures. As an example, with the precision values achieved by FNN$_{All}$, the system would rank on average 4 positive examples in the list of its top-5 choices, and also 14-15 in the top-20 list. Considering the recall at the first $R$ sentences, we will be able to encounter 43% of the total number of check-worthy sentences. This is quite remarkable given the difficulty of the task.

## Experiments and Evaluation ::: Individual Feature Types

Table TABREF24 shows the performance of the individual feature groups, which we have described in Section SECREF4 above, when training using our FNN model, ordered by their decreasing MAP score. We can see that embeddings perform best, with MAP of .357 and P@50 of .495. This shows that modeling semantics and the similarity of a sentence against its context is quite important.

Then come the kNN group with MAP of .313 and P@50 of .455. The high performance of this group of features reveals the frequent occurrence of statements that resemble already fact-checked claims. In the case of false claims, this can be seen as an illustration of the essence of our post-truth era, where lies are repeated continuously, in the hope to make them sound true BIBREF27.

Then follow two sentence-level features, linguistic features and sentiment, with MAP of .308 and .260, and P@50 of .430 and .315, respectively; this is on par with previous work, which has focused primarily on similar sentence-level features.

Then we see the group of contextual features Metadata with MAP=.256, and P@50=.370, followed by two sentence-level features: length and named entities, with MAP of .254 and .236, and P@50 of .340 and .280, respectively.

At the bottom of the table we find position, a general contextual feature with MAP of .212 and P@50 of .230, followed by discourse and topics.

## Discussion

In this section, we present some in-depth analysis and further discussion.

## Discussion ::: Error Analysis

We performed error analysis of the decisions made by the Neural Network that uses all available features. Below we present some examples of False Positives (FP) and False Negatives (FN):

The list of false negatives contains sentences that belong to a whole group of annotations and some of them are not check-worthy on their own, e.g., the eighth example. Some of the false negatives, though, need to be fact-checked and our model missed them, e.g., the sixth and the seventh examples. Note also that the fourth and the fifth sentences make the same statement, but they use different wording. On the one hand, the annotators should have labeled both sentences in the same way, and on the other hand, our model should have also labeled them consistently.

Regarding the false positive examples above, we can see that they could also be potentially interesting for fact-checking as they make some questionable statements. We can conclude that at least some of the false positives of our ranking system could make good candidates for credibility verification, and we demonstrate that the system has successfully extracted common patterns for check-worthiness. Thus, the top-$n$ list will contain mostly sentences that should be fact-checked. Given the discrepancies and the disagreement between the annotations, further cleaning of the dataset might be needed in order to double-check for potentially missing important check-worthy sentences.

## Discussion ::: Effect of Context Modeling

Table TABREF27 shows the results when using all features vs. excluding the contextual features vs. using the contextual features only. We can see that the contextual features have a major impact on performance: excluding them yields major drop for all measures, e.g., MAP drops from .427 to .385, and P@5 drops from .800 to .550. The last two rows in the table show that using contextual features only performs about the same as CB Platform (which uses no contextual features at all).

## Discussion ::: Mimicking Each Particular Source

In the experiments above, we have been trying to predict whether a sentence is check-worthy in general, i.e., with respect to at least one source; this is how we trained and this is how we evaluated our models. Here, we want to evaluate how well our models perform at finding sentences that contain claims that would be judged as worthy for fact-checking with respect to each of the individual sources. The purpose is to see to what extent we can make our system potentially useful for a particular medium.

Another interesting question is whether we should use our generic system or we should retrain with respect to the target medium. Table TABREF31 shows the results for such a comparison, and it further compares to CB Platform. We can see that for all nine media, our model outperforms CB Platform in terms of MAP and P@50; this is also true for the other measures in most cases.

Moreover, we can see that training on all media is generally preferable to training on the target medium only, which shows that they do follow some common principles for selecting what is check-worthy; this means that a general system could serve journalists in all nine, and possibly other, media. Overall, our model works best on PolitiFact, which is a reputable source for fact checking, as this is their primary expertise. We also do well on NPR, NYT, Guardian, and FactCheck, which is quite encouraging.

## Conclusions and Future Work

We have developed a novel approach for automatically finding check-worthy claims in political debates, which is an understudied problem, despite its importance. Unlike previous work, which has looked primarily at sentences in isolation, here we have focused on the context: relationship between the target statement and the larger context of the debate, interaction between the opponents, and reaction by the moderator and by the public.

Our models have achieved state-of-the-art results, outperforming a strong rivaling system by a margin, while also confirming the importance of the contextual information. We further compiled, and we are making freely available, a new dataset of manually-annotated claims, extracted from the 2016 US presidential and vice-presidential debates, which we gathered from nine reputable sources including FactCheck, PolitiFact, CNN, NYT, WP, and NPR.

In future work, we plan to extend our dataset with additional debates, e.g., from other elections, but also with interviews and general discussions. We would also like to experiment with distant supervision, which would allow us to gather more training data, thus facilitating deep learning. We further plan to extend our system with finding claims at the sub-sentence level, as well as with automatic fact-checking of the identified claims.

## Acknowledgments

This research was performed by the Arabic Language Technologies group at Qatar Computing Research Institute, HBKU, within the Interactive sYstems for Answer Search project (Iyas).
