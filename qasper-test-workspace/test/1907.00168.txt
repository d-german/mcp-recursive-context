# The CUED's Grammatical Error Correction Systems for BEA-2019

**Paper ID:** 1907.00168

## Abstract

We describe two entries from the Cambridge University Engineering Department to the BEA 2019 Shared Task on grammatical error correction. Our submission to the low-resource track is based on prior work on using finite state transducers together with strong neural language models. Our system for the restricted track is a purely neural system consisting of neural language models and neural machine translation models trained with back-translation and a combination of checkpoint averaging and fine-tuning -- without the help of any additional tools like spell checkers. The latter system has been used inside a separate system combination entry in cooperation with the Cambridge University Computer Lab.

## Introduction

The automatic correction of errors in text [In a such situaction INLINEFORM0 In such a situation] is receiving more and more attention from the natural language processing community. A series of competitions has been devoted to grammatical error correction (GEC): the CoNLL-2013 shared task BIBREF0 , the CoNLL-2014 shared task BIBREF1 , and finally the BEA 2019 shared task BIBREF2 . This paper presents the contributions from the Cambridge University Engineering Department to the latest GEC competition at the BEA 2019 workshop.

We submitted systems to two different tracks. The low-resource track did not permit the use of parallel training data except a small development set with around 4K sentence pairs. For our low-resource system we extended our prior work on finite state transducer based GEC BIBREF3 to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words. For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC – a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture BIBREF6 . Our purely neural system was also part of the joint submission with the Cambridge University Computer Lab described by BIBREF7 .

## FST-based Grammatical Error Correction

 BIBREF3 investigated the use of finite state transducers (FSTs) for neural grammatical error correction. They proposed a cascade of FST compositions to construct a hypothesis space which is then rescored with a neural language model. We will outline this approach and explain our modifications in this section. For more details we refer to BIBREF3 .

In a first step, the source sentence is converted to an FST INLINEFORM0 (Fig. FIGREF3 ). This initial FST is augmented by composition (denoted with the INLINEFORM1 -operator) with various other FSTs to cover different error types. Composition is a widely used standard operation on FSTs and supported efficiently by FST toolkits such as OpenFST BIBREF8 . We construct the hypothesis space as follows:

We compose the input INLINEFORM0 with the deletion transducer INLINEFORM1 in Fig. FIGREF5 . INLINEFORM2 allows to delete tokens on the short list shown in Tab. TABREF6 at a cost INLINEFORM3 . We selected INLINEFORM4 by looking up all tokens which have been deleted in the dev set more than five times and manually filtered that list slightly. We did not use the full list of dev set deletions to avoid under-estimating INLINEFORM5 in tuning.

In a next step, we compose the transducer from step 1 with the edit transducer INLINEFORM0 in Fig. FIGREF7 . This step addresses substitution errors such as spelling or morphology errors. Like BIBREF3 , we use the confusion sets of BIBREF9 based on CyHunspell for spell checking BIBREF10 , the AGID morphology database for morphology errors BIBREF11 , and manually defined corrections for determiner and preposition errors to construct INLINEFORM1 . Additionally, we extracted all substitution errors from the BEA-2019 dev set which occurred more than five times, and added a small number of manually defined rules that fix tokenization around punctuation symbols.

We found it challenging to allow insertions in LM-based GEC because the LM often prefers inserting words with high unigram probability such as articles and prepositions before less predictable words like proper names. We therefore restrict insertions to the three tokens “,”, “-”, and “'s” and allow only one insertion per sentence. We achieve this by adding the transducer INLINEFORM0 in Fig. FIGREF8 to our composition cascade.

Finally, we map the word-level FSTs to the subword-level by composition with a mapping transducer INLINEFORM0 that applies byte pair encoding BIBREF12 to the full words. Word-to-BPE mapping transducers have been used in prior work to combine word-level models with subword-level neural sequence models BIBREF3 , BIBREF13 , BIBREF14 , BIBREF15 .

In a more condensed form, we can describe the final transducer as: DISPLAYFORM0 

with INLINEFORM0 for deletions, INLINEFORM1 for substitutions, INLINEFORM2 for insertions, and INLINEFORM3 for converting words to BPE tokens. Path scores in the FST in Eq. EQREF14 are the accumulated penalties INLINEFORM4 , INLINEFORM5 , and INLINEFORM6 . The INLINEFORM7 -parameters are tuned on the dev set using a variant of Powell search BIBREF16 . We apply standard FST operations like output projection, INLINEFORM8 -removal, determinization, minimization, and weight pushing BIBREF17 , BIBREF18 to help downstream decoding. Following BIBREF3 we then use the resulting transducer to constrain a neural LM beam decoder.

## Experimental Setup

Our LMs are Transformer BIBREF6 decoders (transformer_big) trained using the Tensor2Tensor library BIBREF19 . We delay SGD updates BIBREF20 , BIBREF21 with factor 2 to simulate 500K training steps with 8 GPUs on 4 physical GPUs. Training batches contain about 4K source and target tokens. Our LM training set comprises the monolingual news2015-news2018 English training sets from the WMT evaluation campaigns BIBREF22 after language detection BIBREF23 (138M sentences) and subword segmentation using byte pair encoding BIBREF12 with 32K merge operations. For decoding, we use our SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8 .

We use neural LMs and neural machine translation (NMT) models in our restricted track entry. Our neural LM is as described in Sec. SECREF15 . Our LMs and NMT models share the same subword segmentation. We perform exploratory NMT experiments with the Base setup, but switch to the Big setup for our final models. Tab. TABREF21 shows the differences between both setups. Tab. TABREF22 lists some corpus statistics for the BEA-2019 training sets. In our experiments without fine-tuning we decode with the average of the 20 most recent checkpoints BIBREF26 . We use the SGNMT decoder BIBREF13 , BIBREF14 in all our experiments.

The BEA-2019 training corpora (Tab. TABREF22 ) differ significantly not only in size but also their closeness to the target domain. The W&I+LOCNESS corpus is most similar to the BEA-2019 dev and test sets in terms of domains and the distribution over English language proficiency, but only consists of 34K sentence pairs. To increase the importance of in-domain training samples we over-sampled the W&I+LOCNESS corpus with different rates. Tab. TABREF24 shows that over-sampling by factor 4 (i.e. adding the W&I+LOCNESS corpus four times to the training set) improves the ERRAMT INLINEFORM0 -score by 2.2 points on the BEA-2019 dev set and does not lead to substantial losses on the CoNLL-2014 test set. We will over-sample the W&I+LOCNESS corpus by four in all subsequent experiments.

Previous works often suggested to remove unchanged sentences (i.e. source and target sentences are equal) from the training corpora BIBREF3 , BIBREF27 , BIBREF28 . We note that removing these identity mappings can be seen as measure to control the balance between precision and recall. As shown in Tab. TABREF26 , removing identities encourages the model to make more corrections and thus leads to higher recall but lower precision. It depends on the test set whether this results in an improvement in INLINEFORM0 score. For the subsequent experiments we found that removing identities in the parallel training corpora but not in the back-translated synthetic data works well in practice.

Back-translation BIBREF5 has become the most widely used technique to use monolingual data in neural machine translation. Back-translation extends the existing parallel training set by additional training samples with real English target sentences but synthetic source sentences. Different methods have been proposed to synthesize the source sentence such as using dummy tokens BIBREF5 , copying the target sentence BIBREF29 , or sampling from or decoding with a reverse sequence-to-sequence model BIBREF5 , BIBREF30 , BIBREF4 . The most popular approach is to generate the synthetic source sentences with a reverse model that is trained to transform target to source sentences using beam search. In GEC, this means that the reverse model learns to introduce errors into a correct English sentence. Back-translation has been applied successfully to GEC by BIBREF4 . We confirm the effectiveness of back-translation in GEC and discuss some of the differences between applying this technique to grammatical error correction and machine translation.

Our experiments with back-translation are summarized in Tab. TABREF28 . Adding 1M synthetic sentences to the training data already yields very substantial gains on both test sets. We achieve our best results with 5M synthetic sentences (+8.44 on BEA-2019 Dev). In machine translation, it is important to maintain a balance between authentic and synthetic data BIBREF5 , BIBREF31 , BIBREF32 . Over-sampling the real data is a common practice to rectify that ratio if large amounts of synthetic data are available. Interestingly, over-sampling real data in GEC hurts performance (row 3 vs. 5 in Tab. TABREF28 ), and it is possible to mix real and synthetic sentences at a ratio of 1:7.9 (last three rows in Tab. TABREF28 ). We will proceed with the 5M setup for the remainder of this paper.

As explained previously, we over-sample the W&I+LOCNESS corpus by factor 4 to mitigate the domain gap between the training set and the BEA-2019 dev and test sets. To further adapt our system to the target domain, we fine-tune the NMT models on W&I+LOCNESS after convergence on the full training set. We do that by continuing training on W&I+LOCNESS from the last checkpoint of the first training pass. Fig. FIGREF30 plots the INLINEFORM0 score on the BEA-2019 dev set for two different setups. For the red curve, we average all checkpoints BIBREF26 (including the last unadapted checkpoint) up to a certain training iteration. Checkpoints are dumped every 500 steps. The green curve does not use any checkpoint averaging. Checkpoint averaging helps to smooth out fluctuations in INLINEFORM1 score, and also generalizes better to CoNLL-2014 (Tab. TABREF31 ).

Tab. TABREF33 contains our experiments with the Big configuration. In addition to W&I+LOCNESS over-sampling, back-translation with 5M sentences, and fine-tuning with checkpoint averaging, we report further gains by adding the language models from our low-resource system (Sec. SECREF15 ) and ensembling. Our best system (4 NMT models, 2 language models) achieves 58.9 M2 on CoNLL-2014, which is slightly (2.25 points) worse than the best published result on that test set BIBREF27 . However, we note that we have tailored our system towards the BEA-2019 dev set and not the CoNLL-2013 or CoNLL-2014 test sets. As we argued in Sec. SECREF18 , our results throughout this work suggest strongly that the optimal system parameters for these test sets are very different from each other, and that our final system settings are not optimal for CoNLL-2014. We also note that unlike the system of BIBREF27 , our system for the restricted track does not use spell checkers or other NLP tools but relies solely on neural sequence models.

## Results

We report M2 BIBREF24 scores on the CoNLL-2014 test set BIBREF1 and span-based ERRANT scores BIBREF25 on the BEA-2019 dev set BIBREF2 . On CoNLL-2014 we compare with the best published results with comparable amount of parallel training data. We refer to BIBREF2 for a full comparison of BEA-2019 systems. We tune our systems on BEA-2019 and only report the performance on CoNLL-2014 for comparison to prior work.

Tab. TABREF9 summarizes our low-resource experiments. Our substitution-only system already outperforms the prior work of BIBREF3 . Allowing for deletions and insertions improves the ERRANT score on BEA-2019 Dev by 2.57 points. We report further gains on both test sets by ensembling two language models and increasing the beam size.

## Differences Between CoNLL-2014 and BEA-2019 Dev

Our results in Tab. TABREF9 differ significantly between the CoNLL-2014 test set and the BEA-2019 dev set. Allowing insertions is beneficial on BEA-2019 Dev but decreases the M2 score on CoNLL-2014. Increasing the beam size improves our system by 3.28 points on CoNLL-2014 while the impact on BEA-2019 Dev is smaller (+0.85 points). These differences can be partially explained by comparing the error type frequencies in the reference annotations in both test sets (Tab. TABREF19 ). Samples in CoNLL-2014 generally need more corrections per sentence than in BEA-2019 Dev. More importantly, the CoNLL-2014 test set contains fewer missing words, but much more unnecessary words than BEA-2019 Dev. This mismatch tempers with tuning as we explicitly tune insertion and deletion penalties.

## Restricted Track Submission

In contrast to our low-resource submission, our restricted system entirely relies on neural models and does not use any external NLP tools, spell checkers, or hand-crafted confusion sets. For simplicity, we also chose to use standard implementations BIBREF19 of standard Transformer BIBREF6 models with standard hyper-parameters. This makes our final system easy to deploy as it is a simple ensemble of standard neural models with minimal preprocessing (subword segmentation). Our contributions on this track focus on NMT training techniques such as over-sampling, back-translation, and fine-tuning. We show that over-sampling effectively reduces domain mismatch. We found back-translation BIBREF5 to be a very effective technique to utilize unannotated training data. However, while over-sampling is commonly used in machine translation to balance the number of real and back-translated training sentences, we report that using over-sampling this way for GEC hurts performance. Finally, we propose a combination of checkpoint averaging BIBREF26 and continued training to adapt our NMT models to the target domain.

## Conclusion

We participated in the BEA 2019 Shared Task on grammatical error correction with submissions to the low-resource and the restricted track. Our low-resource system is an extension of prior work on FST-based GEC BIBREF3 to allow insertions and deletions. Our restricted track submission is a purely neural system based on standard NMT and LM architectures. We pointed out the similarity between GEC and machine translation, and demonstrated that several techniques which originate from MT research such as over-sampling, back-translation, and fine-tuning, are also useful for GEC. Our models have been used in a joint submission with the Cambridge University Computer Lab BIBREF7 .

## Acknowledgments

This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC) grant EP/L027623/1 and has been performed using resources provided by the Cambridge Tier-2 system operated by the University of Cambridge Research Computing Service funded by EPSRC Tier-2 capital grant EP/P020259/1.
