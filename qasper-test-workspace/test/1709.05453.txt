# Augmenting End-to-End Dialog Systems with Commonsense Knowledge

**Paper ID:** 1709.05453

## Abstract

Building dialog agents that can converse naturally with humans is a challenging yet intriguing problem of artificial intelligence. In open-domain human-computer conversation, where the conversational agent is expected to respond to human responses in an interesting and engaging way, commonsense knowledge has to be integrated into the model effectively. In this paper, we investigate the impact of providing commonsense knowledge about the concepts covered in the dialog. Our model represents the first attempt to integrating a large commonsense knowledge base into end-to-end conversational models. In the retrieval-based scenario, we propose the Tri-LSTM model to jointly take into account message and commonsense for selecting an appropriate response. Our experiments suggest that the knowledge-augmented models are superior to their knowledge-free counterparts in automatic evaluation.

## Introduction

In recent years, data-driven approaches to building conversation models have been made possible by the proliferation of social media conversation data and the increase of computing power. By relying on a large number of message-response pairs, the Seq2Seq framework BIBREF0 attempts to produce an appropriate response based solely on the message itself, without any memory module.

In human-to-human conversations, however, people respond to each other's utterances in a meaningful way not only by paying attention to the latest utterance of the conversational partner itself, but also by recalling relevant information about the concepts covered in the dialogue and integrating it into their responses. Such information may contain personal experience, recent events, commonsense knowledge and more (Figure 1 ). As a result, it is speculated that a conversational model with a “memory look-up” module can mimic human conversations more closely BIBREF1 , BIBREF2 . In open-domain human-computer conversation, where the model is expected to respond to human utterances in an interesting and engaging way, commonsense knowledge has to be integrated into the model effectively.

In the context of artificial intelligence (AI), commonsense knowledge is the set of background information that an individual is intended to know or assume and the ability to use it when appropriate BIBREF3 , BIBREF4 , BIBREF5 . Due to the vastness of such kind of knowledge, we speculate that this goal is better suited by employing an external memory module containing commonsense knowledge rather than forcing the system to encode it in model parameters as in traditional methods.

In this paper, we investigate how to improve end-to-end dialogue systems by augmenting them with commonsense knowledge, integrated in the form of external memory. The remainder of this paper is as follows: next section proposes related work in the context of conversational models and commonsense knowledge; following, a section describes the proposed model in detail; later, a section illustrates experimental results; finally, the last section proposes concluding remarks and future work.

## Conversational Models

Data-driven conversational models generally fall into two categories: retrieval-based methods BIBREF6 , BIBREF7 , BIBREF8 , which select a response from a predefined repository, and generation-based methods BIBREF9 , BIBREF10 , BIBREF11 , which employ an encoder-decoder framework where the message is encoded into a vector representation and, then, fed to the decoder to generate the response. The latter is more natural (as it does not require a response repository) yet suffers from generating dull or vague responses and generally needs a great amount of training data.

The use of an external memory module in natural language processing (NLP) tasks has received considerable attention recently, such as in question answering BIBREF12 and language modeling BIBREF13 . It has also been employed in dialogue modeling in several limited settings. With memory networks, BIBREF14 used a set of fact triples about movies as long-term memory when modeling reddit dialogues, movie recommendation and factoid question answering. Similarly in a restaurant reservation setting, BIBREF2 provided local restaurant information to the conversational model.

Researchers have also proposed several methods to incorporate knowledge as external memory into the Seq2Seq framework. BIBREF15 incorporated the topic words of the message obtained from a pre-trained latent Dirichlet allocation (LDA) model into the context vector through a joint attention mechanism. BIBREF1 mined FoodSquare tips to be searched by an input message in the food domain and encoded such tips into the context vector through one-turn hop. The model we propose in this work shares similarities with BIBREF16 , which encoded unstructured textual knowledge with a recurrent neural network (RNN). Our work distinguishes itself from previous research in that we consider a large heterogeneous commonsense knowledge base in an open-domain retrieval-based dialogue setting.

## Commonsense Knowledge

Several commonsense knowledge bases have been constructed during the past decade, such as ConceptNet BIBREF17 and SenticNet BIBREF18 . The aim of commonsense knowledge representation and reasoning is to give a foundation of real-world knowledge to a variety of AI applications, e.g., sentiment analysis BIBREF19 , handwriting recognition BIBREF20 , e-health BIBREF21 , aspect extraction BIBREF22 , and many more. Typically, a commonsense knowledge base can be seen as a semantic network where concepts are nodes in the graph and relations are edges (Figure 2 ). Each $<concept1, relation, concept2 >$ triple is termed an assertion.

Based on the Open Mind Common Sense project BIBREF23 , ConceptNet not only contains objective facts such as “Paris is the capital of France” that are constantly true, but also captures informal relations between common concepts that are part of everyday knowledge such as “A dog is a pet”. This feature of ConceptNet is desirable in our experiments, because the ability to recognize the informal relations between common concepts is necessary in the open-domain conversation setting we are considering in this paper.

## Task Definition

In this work, we concentrate on integrating commonsense knowledge into retrieval-based conversational models, because they are easier to evaluate BIBREF24 , BIBREF7 and generally take a lot less data to train. We leave the generation-based scenario to future work.

Message (context) $x$ and response $y$ are a sequence of tokens from vocabulary $V$ . Given $x$ and a set of response candidates $[y_1,y_2,y_3...,y_K]\in Y$ , the model chooses the most appropriate response $\hat{y}$ according to: 

$$\hat{y}=\mathop {\arg \max }_{y\in {Y}}f(x,y),$$   (Eq. 6) 

where $f(x,y)$ is a scoring function measuring the “compatibility” of $x$ and $y$ . The model is trained on $<message, response, label >$ triples with cross entropy loss, where $label$ is binary indicating whether the $<message, response >$ pair comes from real data or is randomly combined.

## Dual-LSTM Encoder

As a variation of vanilla RNN, a long short-term memory (LSTM) network BIBREF25 is good at handling long-term dependencies and can be used to map an utterance to its last hidden state as fixed-size embedding representation. The Dual-LSTM encoder BIBREF6 represents the message $x$ and response $y$ as fixed-size embeddings $\vec{x}$ and $\vec{y}$ with the last hidden states of the same LSTM. The compatibility function of the two is thus defined by: 

$$f(x,y) = \sigma (\vec{x}^{T}W\vec{y}),$$   (Eq. 8) 

where matrix $W \in \mathcal {R}^{D\times D}$ is learned during training.

## Commonsense Knowledge Retrieval

In this paper, we assume that a commonsense knowledge base is composed of assertions $A$ about concepts $C$ . Each assertion $a \in A$ takes the form of a triple $<c_1,r,c_2 >$ , where $r \in R$ is a relation between $c_1$ and $c_2$ , such as IsA, CapableOf, etc. $c_1,c_2$ are concepts in $C$ . The relation set $R$ is typically much smaller than $C$0 . $C$1 can either be a single word (e.g., “dog” and “book”) or a multi-word expression (e.g., “take_a_stand” and “go_shopping”). We build a dictionary $C$2 out of $C$3 where every concept $C$4 is a key and a list of all assertions in $C$5 concerning $C$6 , i.e., $C$7 or $C$8 , is the value. Our goal is to retrieve commonsense knowledge about every concept covered in the message.

We define $A_x$ as the set of commonsense assertions concerned with message $x$ . To recover concepts in message $x$ , we use simple $n$ -gram matching ( $n\le N$ ). Every $n$ -gram in $c$ is considered a potential concept. If the $n$ -gram is a key in $x$0 , the corresponding value, i.e., all assertions in $x$1 concerning the concept, is added to $x$2 (Figure 4 ).

## Tri-LSTM Encoder

Our main approach to integrating commonsense knowledge into the conversational model involves using another LSTM for encoding all assertions $a$ in $A_x$ , as illustrated in Figure 3 . Each $a$ , originally in the form of $<c_1,r,c_2 >$ , is transformed into a sequence of tokens by chunking $c_1$ , $c_2$ , concepts which are potentially multi-word phrases, into $[c_{11},c_{12},c_{13}...]$ and $[c_{21},c_{22},c_{23}...]$ . Thus, $a=[c_{11},c_{12},c_{13}...,r,c_{21},c_{22},c_{23}...]$ .

We add $R$ to vocabulary $V$ , that is, each $r$ in $R$ will be treated like any regular word in $V$ during encoding. We decide not to use each concept $c$ as a unit for encoding $a$ because $C$ is typically too large ( $>$ 1M). $a$ is encoded as embedding representation $V$0 using another LSTM. Note that this encoding scheme is suitable for any natural utterances containing commonsense knowledge in addition to well-structured assertions. We define the match score of assertion $V$1 and response $V$2 as: 

$$m(a,y) = \vec{a}^{T}W_a\vec{y},$$   (Eq. 16) 

where $W_a \in \mathcal {R}^{D\times D}$ is learned during training. Commonsense assertions $A_x$ associated with a message is usually large ( $>$ 100 in our experiment). We observe that in a lot of cases of open-domain conversation, response $y$ can be seen as triggered by certain perception of message $x$ defined by one or more assertions in $A_x$ , as illustrated in Figure 4 . We can see the difference between message and response pair when commonsense knowledge is used. For example, the word `Insomnia' in the message is mapped to the commonsense assertion `Insomnia, IsA, sleep $\_$ problem'. The appropriate response is then matched to `sleep $\_$ problem' that is `go to bed'. Similarly, the word `Hawaii' in the message is mapped to the commonsense assertion `Hawaii, UsedFor, tourism'. The appropriate response is then matched to `tourism' that is `enjoy vacation'. In this way, new words can be mapped to the commonly used vocabulary and improve response accuracy.

Our assumption is that $A_x$ is helpful in selecting an appropriate response $y$ . However, usually very few assertions in $A_x$ are related to a particular response $y$ in the open-domain setting. As a result, we define the match score of $A_x$ and $y$ as 

$$m(A_x,y)=\mathop {\max }_{a\in {A_x}} m(a,y),$$   (Eq. 17) 

that is, we only consider the commonsense assertion $a$ with the highest match score with $y$ , as most of $A_x$ are not relevant to $y$ . Incorporating $m(A_x,y)$ into the Dual-LSTM encoder, our Tri-LSTM encoder model is thus defined as: 

$$f(x,y) = \sigma (\vec{x}^{T}W\vec{y} + m(A_x,y)),$$   (Eq. 18) 

i.e., we use simple addition to supplement $x$ with $A_x$ , without introducing a mechanism for any further interaction between $x$ and $A_x$ . This simple approach is suitable for response selection and proves effective in practice.

The intuition we are trying to capture here is that an appropriate response $y$ should not only be compatible with $x$ , but also related to certain memory recall triggered by $x$ as captured by $m(A_x,y)$ . In our case, the memory is commonsense knowledge about the world. In cases where $A_x = \emptyset $ , i.e., no commonsense knowledge is recalled, $m(A_x,y)=0$ and the model degenerates to Dual-LSTM encoder.

## Comparison Approaches

We follow BIBREF2 , BIBREF14 and use supervised word embeddings as a baseline. Word embeddings are most well-known in the context of unsupervised training on raw text as in BIBREF27 , yet they can also be used to score message-response pairs. The embedding vectors are trained directly for this goal. In this setting, the “compatibility” function of $x$ and $y$ is defined as: 

$$f(x,y)=\vec{x}^T\vec{y}$$   (Eq. 21) 

In this setting, $\vec{x},\vec{y}$ are bag-of-words embeddings. With retrieved commonsense assertions $A_x$ , we embed each $a\in {A_x}$ to bag-of-words representation $\vec{a}$ and have: 

$$f(x,y)=\vec{x}^T\vec{y}+\mathop {\max }_{a\in {A_x}} \ \ \vec{a}^T\vec{y}.$$   (Eq. 22) 

This linear model differs from Tri-LSTM encoder in that it represents an utterance with its bag-of-words embedding instead of RNNs.

Memory networks BIBREF13 , BIBREF28 are a class of models that perform language understanding by incorporating a memory component. They perform attention over memory to retrieve all relevant information that may help with the task. In our dialogue modeling setting, we use $A_x$ as the memory component. Our implementation of memory networks, similar to BIBREF2 , BIBREF14 , differs from supervised word embeddings described above in only one aspect: how to treat multiple entries in memory. In memory networks, output memory representation $\vec{o}=\sum _{i}p_i\vec{a}_i$ , where $\vec{a}_i$ is the bag-of-words embedding of $a_i\in {A_x}$ and $p_i$ is the attention signal over memory $A_x$ calculated by $p_i=softmax(\vec{x}^T\vec{a_i})$ . The “compatibility” function of $x$ and $y$ is defined as: 

$$f(x,y)=(\vec{x}+\vec{o})^T\vec{y}=\vec{x}^T\vec{y}+(\sum _{i}p_i\vec{a}_i)^T\vec{y}$$   (Eq. 24) 

In contrast to supervised word embeddings described above, attention over memory is determined by message $x$ . This mechanism was originally designed to retrieve information from memory that is relevant to the context, which in our setting is already achieved during commonsense knowledge retrieval. As speculated, the attention over multiple memory entries is better determined by response $y$ in our setting. We empirically prove this point below.

## Twitter Dialogue Dataset

To the best of our knowledge, there is currently no well-established open-domain response selection benchmark dataset available, although certain Twitter datasets have been used in the response generation setting BIBREF29 , BIBREF30 . We thus evaluate our method against state-of-the-art approaches in the response selection task on Twitter dialogues.

1.4M Twitter <message, response $>$ pairs are used for our experiments. They were extracted over a 5-month period, from February through July in 2011. 1M Twitter <message, response $>$ pairs are used for training. With the original response as ground truth, we construct 1M <message, response, label=1 $>$ triples as positive instances. Another 1M negative instances <message, response, label=0 $>$ are constructed by replacing the ground truth response with a random response in the training set.

For tuning and evaluation, we use 20K <message, response $>$ pairs that constitute the validation set (10K) and test set (10K). They are selected by a criterion that encourages interestingness and relevance: both the message and response have to be at least 3 tokens long and contain at least one non-stopword. For every message, at least one concept has to be found in the commonsense knowledge base. For each instance, we collect another 9 random responses from elsewhere to constitute the response candidates.

Preprocessing of the dataset includes normalizing hashtags, “@User”, URLs, emoticons. Vocabulary $V$ is built out of the training set with 5 as minimum word frequency, containing 62535 words and an extra $<UNK >$ token representing all unknown words.

## ConceptNet

In our experiment, ConceptNet is used as the commonsense knowledge base. Preprocessing of this knowledge base involves removing assertions containing non-English characters or any word outside vocabulary $V$ . 1.4M concepts remain. 0.8M concepts are unigrams, 0.43M are bi-grams and the other 0.17M are tri-grams or more. Each concept is associated with an average of 4.3 assertions. More than half of the concepts are associated with only one assertion.

An average of 2.8 concepts can be found in ConceptNet for each message in our Twitter Dialogue Dataset, yielding an average of 150 commonsense assertions (the size of $A_x$ ). Unsurprisingly, common concepts with more assertions associated are favored in actual human conversations.

It is worth noting that ConceptNet is also noisy due to uncertainties in the constructing process, where 15.5% of all assertions are considered “false” or “vague” by human evaluators BIBREF17 . Our max-pooling strategy used in Tri-LSTM encoder and supervised word embeddings is partly designed to alleviate this weakness.

## Parameter Settings

In all our models excluding term frequency–inverse document frequency (TF-IDF) BIBREF31 , we initialize word embeddings with pretrained GloVe embedding vectors BIBREF32 . The size of hidden units in LSTM models is set to 256 and the word embedding dimension is 100. We use stochastic gradient descent (SGD) for optimizing with batch size of 64. We fixed training rate at 0.001.

## Results and Analysis

The main results for TF-IDF, word embeddings, memory networks and LSTM models are summarized in Table 1 . We observe that:

(1) LSTMs perform better at modeling dialogues than word embeddings on our dataset, as shown by the comparison between Tri-LSTM and word embeddings.

(2) Integrating commonsense knowledge into conversational models boosts model performance, as Tri-LSTM outperforms Dual-LSTM by a certain margin.

(3) Max-pooling over all commonsense assertions depending on response $y$ is a better method for utilizing commonsense knowledge than attention over memory in our setting, as demonstrated by the gain of performance of word embeddings over memory networks.

We also analyze samples from the test set to gain an insight on how commonsense knowledge supplements the message itself in response selection by comparing Tri-LSTM encoder and Dual-LSTM encoder.

As illustrated in Table 2 , instances 1,2 represent cases where commonsense assertions as an external memory module provide certain clues that the other model failed to capture. For example in instance 2, Tri-LSTM selects the response “...improve your french” to message “bonjour madame” based on a retrieved assertion “ $bonjour, IsA, hello\_in\_french$ ”, while Dual-LSTM selects an irrelevant response. Unsurprisingly, Dual-LSTM is also able to select the correct response in some cases where certain commonsense knowledge is necessary, as illustrated in instance 3. Both models select “... pink or black” in response to message “...what color shoes...”, even though Dual-LSTM does not have access to a helpful assertion “ $pink, RelatedTo,
color$ ”.

Informally speaking, such cases suggest that to some extent, Dual-LSTM (models with no memory) is able to encode certain commonsense knowledge in model parameters (e.g., word embeddings) in an implicit way. In other cases, e.g., instance 4, the message itself is enough for the selection of the correct response, where both models do equally well.

## Conclusion and Future Work

In this paper, we emphasized the role of memory in conversational models. In the open-domain chit-chat setting, we experimented with commonsense knowledge as external memory and proposed to exploit LSTM to encode commonsense assertions to enhance response selection.

In the other research line of response generation, such knowledge can potentially be used to condition the decoder in favor of more interesting and relevant responses. Although the gains presented by our new method is not spectacular according to Recall@ $k$ , our view represents a promising attempt at integrating a large heterogeneous knowledge base that potentially describes the world into conversational models as a memory component.

Our future work includes extending the commonsense knowledge with common (or factual) knowledge, e.g., to extend the knowledge base coverage by linking more named entities to commonsense knowledge concepts BIBREF34 , and developing a better mechanism for utilizing such knowledge instead of the simple max-pooling scheme used in this paper. We would also like to explore the memory of the model for multiple message response pairs in a long conversation.

Lastly, we plan to integrate affective knowledge from SenticNet in the dialogue system in order to enhance its emotional intelligence and, hence, achieve a more human-like interaction. The question, after all, is not whether intelligent machines can have any emotions, but whether machines can be intelligent without any emotions BIBREF35 .

## Acknowledgements

We gratefully acknowledge the help of Alan Ritter for sharing the twitter dialogue dataset and the NTU PDCC center for providing computing resources.
