# Arabic Offensive Language on Twitter: Analysis and Experiments

**Paper ID:** 2004.02192

## Abstract

Detecting offensive language on Twitter has many applications ranging from detecting/predicting bullying to measuring polarization. In this paper, we focus on building effective Arabic offensive tweet detection. We introduce a method for building an offensive dataset that is not biased by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. Next, we analyze the dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Arabic speakers use offensive language. Lastly, we conduct a large battery of experiments to produce strong results (F1 = 79.7) on the dataset using Support Vector Machine techniques.

## Introduction

Disclaimer: Due to the nature of the paper, some examples contain highly offensive language and hate speech. They don't reflect the views of the authors in any way, and the point of the paper is to help fight such speech. Much recent interest has focused on the detection of offensive language and hate speech in online social media. Such language is often associated with undesirable online behaviors such as trolling, cyberbullying, online extremism, political polarization, and propaganda. Thus, offensive language detection is instrumental for a variety of application such as: quantifying polarization BIBREF0, BIBREF1, trolls and propaganda account detection BIBREF2, detecting the likelihood of hate crimes BIBREF3; and predicting conflict BIBREF4. In this paper, we describe our methodology for building a large dataset of Arabic offensive tweets. Given that roughly 1-2% of all Arabic tweets are offensive BIBREF5, targeted annotation is essential for efficiently building a large dataset. Since our methodology does not use a seed list of offensive words, it is not biased by topic, target, or dialect. Using our methodology, we tagged 10,000 Arabic tweet dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled tweets as vulgar or hate speech. To date, this is the largest available dataset, which we plan to make publicly available along with annotation guidelines. We use this dataset to characterize Arabic offensive language to ascertain the topics, dialects, and users' gender that are most associated with the use of offensive language. Though we suspect that there are common features that span different languages and cultures, some characteristics of Arabic offensive language is language and culture specific. Thus, we conduct a thorough analysis of how Arabic users use offensive language. Next, we use the dataset to train strong Arabic offensive language classifiers using state-of-the-art representations and classification techniques. Specifically, we experiment with static and contextualized embeddings for representation along with a variety of classifiers such as a deep neural network classifier and Support Vector Machine (SVM).

The contributions of this paper are as follows:

We built the largest Arabic offensive language dataset to date that includes special tags for vulgar language and hate speech. We describe the methodology for building it along with annotation guidelines.

We performed thorough analysis of the dataset and described the peculiarities of Arabic offensive language.

We experimented with Support Vector Machine classifiers on character and word ngrams classification techniques to provide strong results on Arabic offensive language classification.

## Related Work

Many recent papers have focused on the detection of offensive language, including hate speech BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13. Offensive language can be categorized as: Vulgar, which include explicit and rude sexual references, Pornographic, and Hateful, which includes offensive remarks concerning people’s race, religion, country, etc. BIBREF14. Prior works have concentrated on building annotated corpora and training classification models. Concerning corpora, hatespeechdata.com attempts to maintain an updated list of hate speech corpora for multiple languages including Arabic and English. Further, SemEval 2019 ran an evaluation task targeted at detecting offensive language, which focused exclusively on English BIBREF15. As for classification models, most studies used supervised classification at either word level BIBREF10, character sequence level BIBREF11, and word embeddings BIBREF9. The studies used different classification techniques including using Naïve Bayes BIBREF10, SVM BIBREF11, and deep learning BIBREF6, BIBREF7, BIBREF12 classification. The accuracy of the aforementioned system ranged between 76% and 90%. Earlier work looked at the use of sentiment words as features as well as contextual features BIBREF13.

The work on Arabic offensive language detection is relatively nascent BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF5. Mubarak et al. mubarak2017abusive suggested that certain users are more likely to use offensive languages than others, and they used this insight to build a list of offensive Arabic words and they constructed a labeled set of 1,100 tweets. Abozinadah et al. abozinadah2017detecting used supervised classification based on a variety of features including user profile features, textual features, and network features. They reported an accuracy of nearly 90%. Alakrot et al. alakrot2018towards used supervised classification based on word unigrams and n-grams to detect offensive language in YouTube comments. They improved classification with stemming and achieved a precision of 88%. Albadi et al. albadi2018they focused on detecting religious hate speech using a recurrent neural network. Further; Schmidt and Wiegand schmidt-wiegand-2017-survey surveyed major works on hate speech detection; Fortuna and Nunes Fortuna2018Survey provided a comprehensive survey for techniques and works done in the area between 2004 and 2017.

Arabic is a morphologically rich language with a standard variety, namely Modern Standard Arabic (MSA) and is typically used in formal communication, and many dialectal varieties that differ from MSA in lexical selection, morphology, and syntactic structures. For MSA, words are typically derived from a set of thousands of roots by fitting a root into a stem template and the resulting stem may accept a variety of prefixes and suffixes such as coordinating conjunctions and pronouns. Though word segmentation (or stemming) is quite accurate for MSA BIBREF20, with accuracy approaching 99%, dialectal segmentation is not sufficiently reliable, with accuracy ranging between 91-95% for different dialects BIBREF21. Since dialectal Arabic is ubiquitous in Arabic tweets and many tweets have creative spellings of words, recent work on Arabic offensive language detection used character-level models BIBREF5.

## Data Collection ::: Collecting Arabic Offensive Tweets

Our target was to build a large Arabic offensive language dataset that is representative of their appearance on Twitter and is hopefully not biased to specific dialects, topics, or targets. One of the main challenges is that offensive tweets constitute a very small portion of overall tweets. To quantify their proportion, we took 3 random samples of tweets from different days, with each sample composed of 1,000 tweets, and we found that between 1% and 2% of them were in fact offensive (including pornographic advertisement). This percentage is consistent with previously reported percentages BIBREF19. Thus, annotating random tweets is grossly inefficient. One way to overcome this problem is to use a seed list of offensive words to filter tweets. However, doing so is problematic as it would skew the dataset to particular types of offensive language or to specific dialects. Offensiveness is often dialect and country specific.

After inspecting many tweets, we observed that many offensive tweets have the vocative particle يا> (“yA” – meaning “O”), which is mainly used in directing the speech to a specific person or group. The ratio of offensive tweets increases to 5% if each tweet contains one vocative particle and to 19% if has at least two vocative particles. Users often repeat this particle for emphasis, as in: يا أمي يا حنونة> (“yA Amy yA Hnwnp” – O my mother, O kind one), which is endearing and non-offensive, and يا كلب يا قذر> (“yA klb yA q*r” – “O dog, O dirty one”), which is offensive. We decided to use this pattern to increase our chances of finding offensive tweets. One of the main advantages of the pattern يا ... يا> (“yA ... yA”) is that it is not associated with any specific topic or genre, and it appears in all Arabic dialects. Though the use of offensive language does not necessitate the appearance of the vocative particle, the particle does not favor any specific offensive expressions and greatly improves our chances of finding offensive tweets. It is clear, the dataset is more biased toward positive class. Using the dataset for real-life application may require de-biasing it by boosting negative class or random sampling additional data from Twitter BIBREF22.Using the Twitter API, we collected 660k Arabic tweets having this pattern between April 15, 2019 and May 6, 2019. To increase diversity, we sorted the word sequences between the vocative particles and took the most frequent 10,000 unique sequences. For each word sequence, we took a random tweet containing each sequence. Then we annotated those tweets, ending up with 1,915 offensive tweets which represent roughly 19% of all tweets. Each tweet was labeled as: offensive, which could additionally be labeled as vulgar and/or hate speech, or Clean. We describe in greater detail our annotation guidelines, which we made sure that they are compatible with the OffensEval2019 annotation guidelines BIBREF15. For example, if a tweet has insults or threats targeting a group based on their nationality, ethnicity, gender, political affiliation, religious belief, or other common characteristics, this is considered as hate speech BIBREF15. It is worth mentioning that we also considered insulting groups based on their sport affiliation as a form of hate speech. In most Arab countries, being a fan of a particularly sporting club is considered as part of the personality and ideology which rarely changes over time (similar to political affiliation). Many incidents of violence have occurred among fans of rival clubs.

## Data Collection ::: Annotating Tweets

We developed the annotation guidelines jointly with an experienced annotator, who is a native Arabic speaker with a good knowledge of various Arabic dialects. We made sure that our guidelines were compatible with those of OffensEval2019. The annotator carried out all annotation. Tweets were given one or more of the following four labels: offensive, vulgar, hate speech, or clean. Since the offensive label covers both vulgar and hate speech and vulgarity and hate speech are not mutually exclusive, a tweet can be just offensive or offensive and vulgar and/or hate speech. The annotation adhered to the following guidelines:

## Data Collection ::: Annotating Tweets ::: OFFENSIVE (OFF):

Offensive tweets contain explicit or implicit insults or attacks against other people, or inappropriate language, such as:

Direct threats or incitement, ex: احرقوا> مقرات المعارضة> (“AHrqwA mqrAt AlmEArDp” – “burn the headquarters of the opposition”) and هذا المنافق يجب قتله> (“h*A AlmnAfq yjb qtlh” – “this hypocrite needs to be killed”).

Insults and expressions of contempt, which include: Animal analogy, ex: يا كلب> (“yA klb” – “O dog”) and كل تبن> (“kl tbn” – “eat hay”).; Insult to family members, ex: يا روح أمك> (“yA rwH Amk” – “O mother's soul”); Sexually-related insults, ex: يا ديوث> (“yA dywv” – “O person without envy”); Damnation, ex: الله يلعنك> (“Allh ylEnk” – “may Allah/God curse you”); and Attacks on morals and ethics, ex: يا كاذب> (“yA kA*b” – “O liar”)

## Data Collection ::: Annotating Tweets ::: VULGAR (VLG):

Vulgar tweets are a subset of offensive tweets and contain profanity, such as mentions of private parts or sexual-related acts or references.

## Data Collection ::: Annotating Tweets ::: HATE SPEECH (HS):

Hate speech tweets, a subset of offensive tweets containing offensive language targeting group based on common characteristics such as: Race, ex: يا زنجي> (“yA znjy” – “O negro”); Ethnicity, ex. الفرس الأنجاس> (“Alfrs AlAnjAs” – “Impure Persians”); Group or party, ex: أبوك شيوعي> (“Abwk $ywEy” – “your father is communist”); and Religion, ex: دينك القذر> (“dynk Alq*r” – “your filthy religion”).

## Data Collection ::: Annotating Tweets ::: CLEAN (CLN):

Clean tweets do not contain vulgar or offensive language. We noticed that some tweets have some offensive words, but the whole tweet should not be considered as offensive due to the intention of users. This suggests that normal string match without considering contexts will fail in some cases. Examples of such ambiguous cases include: Humor, ex: يا عدوة الفرحة ههه> (“yA Edwp AlfrHp hhh” – “O enemy of happiness hahaha”); Advice, ex: لا تقل لصاحبك يا خنزير> (“lA tql lSAHbk yA xnzyr” – “don't say to your friend: You are a pig”); Condition, ex: إذا عارضتهم يقولون يا عميل> (“A*A EArDthm yqwlwn yA Emyl” – “if you disagree with them they will say: You are an agent”); Condemnation, ex: لماذا نسب بقول: يا بقرة؟> (“lmA*A nsb bqwl: yA bqrp?” – “Why do we insult others by saying: O cow?”); Self offense, ex: تعبت من لساني القذر> (“tEbt mn lsAny Alq*r” – “I am tired of my dirty tongue”); Non-human target, ex: يا بنت المجنونة يا كورة> (“yA bnt Almjnwnp yA kwrp” – “O daughter of the crazy one O football”); and Quotation from a movies or a story, ex: تاني يا زكي! تاني يا فاشل> (“tAny yA zky! tAny yA fA$l” – “again O Zaky! again O loser”). For other ambiguous cases, the annotator searched Twitter to find how actual users used expressions.

Table TABREF11 shows the distribution of the annotated tweets. There are 1,915 offensive tweets, including 225 vulgar tweet and 506 hate speech tweets, and 8,085 clean tweets. To validate the quality of annotation, a random sample of 100 tweets from the data, containing 50 offensive and 50 clean tweets, was given to additional three annotators. We calculated the Inter-Annotator Agreement between the annotators using Fleiss’s Kappa coefficient BIBREF23. The Kappa score was 0.92 indicating high quality annotation and agreement.

## Data Collection ::: Statistics and User Demographics

Given the annotated tweets, we wanted to ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language.

Figure FIGREF13 shows the distribution of topics associated with offensive tweets. As the figure shows, sports and politics are most dominant for offensive language including vulgar and hate speech. As for dialect, we looked at MSA and four major dialects, namely Egyptian (EGY), Leventine (LEV), Maghrebi (MGR), and Gulf (GLF). Figure FIGREF14 shows that 71% of vulgar tweets were written in EGY followed by GLF, which accounted for 13% of vulgar tweets. MSA was not used in any of the vulgar tweets. As for offensive tweets in general, EGY and GLF were used in 36% and 35% of the offensive tweets respectively. Unlike the case of vulgar language where MSA was non-existent, 15% of the offensive tweets were in fact written in MSA. For hate speech, GLF and EGY were again dominant and MSA consistuted 21% of the tweets. This is consistent with findings for other languages such as English and Italian where vulgar language was more frequently associated with colloquial language BIBREF24, BIBREF25. Regarding the gender, Figure FIGREF15 shows that the vast majority of offensive tweets, including vulgar and hate speech, were authored by males. Female Twitter users accounted for 14% of offensive tweets in general and 6% and 9% of vulgar and hate speech respectively. Figure FIGREF16 shows a detailed categorization of hate speech types, where the top three include insulting groups based on their political ideology, origin, and sport affiliation. Religious hate speech appeared in only 15% of all hate speech tweets.

Next, we analyzed all tweets labeled as offensive to better understand how Arabic speakers use offensive language. Here is a breakdown of usage:

Direct name calling: The most frequent attack is to call a person an animal name, and the most used animals were كلب> (“klb” – “dog”), حمار> (“HmAr” – “donkey”), and بهيم> (“bhym” – “beast”). The second most common was insulting mental abilities using words such as غبي> (“gby” – “stupid”) and عبيط> (“EbyT” –“idiot”). Some culture-specific differences should be considered. Not all animal names are used as insults. For example, animals such as أسد> (“Asd” – “lion”), صقر> (“Sqr” – “falcon”), and غزال> (“gzAl” – “gazelle”) are typically used for praise. For other insults, people use: some bird names such as دجاجة> (“djAjp” – “chicken”), بومة> (“bwmp” – “owl”), and غراب> (“grAb” – “crow”); insects such as ذبابة> (“*bAbp” – “fly”), صرصور> (“SrSwr” – “cockroach”), and حشرة> (“H$rp” – “insect”); microorganisms such as جرثومة> (“jrvwmp” – “microbe”) and طحالب> (“THAlb” – “algae”); inanimate objects such as جزمة> (“jzmp” – “shoes”) and سطل> (“sTl” – “bucket”) among other usages.

Simile and metaphor: Users use simile and metaphor were they would compare a person to: an animal as in زي الثور> (“zy Alvwr” – “like a bull”), سمعني نهيقك> (“smEny nhyqk” – “let me hear your braying”), and هز ديلك> (“hz dylk” – “wag your tail”); a person with mental or physical disability such as منغولي> (“mngwly” – “Mongolian (down-syndrome)”), معوق> (“mEwq” – “disabled”), and قزم> (“qzm” – “dwarf”); and to the opposite gender such as جيش نوال> (“jy$ nwAl” – “Nawal's army (Nawal is female name)”) and نادي زيزي> (“nAdy zyzy” – “Zizi's club (Zizi is a female pet name)”).

Indirect speech: This type of offensive language includes: sarcasm such as أذكى إخواتك> (“A*kY AxwAtk” – “smartest one of your siblings”) and فيلسوف الحمير> (“fylswf AlHmyr” – “the donkeys' philosopher”); questions such as ايه كل الغباء ده> (“Ayh kl AlgbA dh” – “what is all this stupidity”); and indirect speech such as النقاش مع البهايم غير مثمر> (“AlnqA$ mE AlbhAym gyr mvmr” – “no use talking to cattle”).

Wishing Evil: This entails wishing death or major harm to befall someone such as ربنا ياخدك> (“rbnA yAxdk” – “May God take (kill) you”), الله يلعنك> (“Allh ylEnk” – “may Allah/God curse you”), and روح في داهية> (“rwH fy dAhyp” – equivalent to “go to hell”).

Name alteration: One common way to insult others is to change a letter or two in their names to produce new offensive words that rhyme with the original names. Some examples of such include changing الجزيرة> (“Aljzyrp” – “Aljazeera (channel)”) to الخنزيرة> (“Alxnzyrp” – “the pig”) and خلفان> (“xlfAn” – “Khalfan (person name)”) to خرفان> (“xrfAn” – “crazed”).

Societal stratification: Some insults are associated with: certain jobs such as بواب> (“bwAb” – “doorman”) or خادم> (“xAdm” – “servant”); and specific societal components such بدوي> (“bdwy” – “bedouin”) and فلاح> (“flAH” – “farmer”).

Immoral behavior: These insults are associated with negative moral traits or behaviors such as حقير> (“Hqyr” – “vile”), خاين> (“xAyn” – “traitor”), and منافق> (“mnAfq” – “hypocrite”).

Sexually related: They include expressions such as خول> (“xwl” – “gay”), وسخة> (“wsxp” – “prostitute”), and عرص> (“ErS” – “pimp”).

Figure FIGREF17 shows top words with the highest valance score for individual words in the offensive tweets. Larger fonts are used to highlight words with highest score and align as well with the categories mentioned ahead in the breakdown for the offensive languages. We modified the valence score described by BIBREF1 conover2011political to magnify its value based on frequency of occurrence. The score is computed as follows:

$V(I) = 2 \frac{ \frac{tf(I, C_off)}{total(C_off)}}{\frac{tf(I, C_off)}{total(C_off)} + \frac{tf(I, C_cln)}{total(C_cln)} } - 1$

where

$tf(I, C_i) = \sum _{a \in I \bigcap C_i} [ln(Cnt(a, C_i)) + 1]$

$total(C_i) = \sum _{I} tf(I, C_i)$

$Cnt(a, C_i)$ is the number of times word $a$ was used in offensive or clean tweets tweets $C_i$. In essence, we are replacing term frequencies with the natural log of the term frequencies.

## Experiments

We conducted an extensive battery of experiments on the dataset to establish strong Arabic offensive language classification results. Though the offensive tweets have finer-grained labels where offensive tweet could also be vulgar or constitute hate speech, we conducted coarser-grained classification to determine if a tweet was offensive or not. For classification, we experimented with several tweet representation and classification models. For tweet representations, we used: the count of positive and negative terms, based on a polarity lexicon; static embeddings, namely fastText and Skip-Gram; and deep contextual embeddings, namely BERTbase-multilingual.

## Experiments ::: Data Pre-processing

We performed several text pre-processing steps. First, we tokenized the text using the Farasa Arabic NLP toolkit BIBREF20. Second, we removed URLs, numbers, and all tweet specific tokens, namely mentions, retweets, and hashtags as they are not part of the language semantic structure, and therefore, not usable in pre-trained embeddings. Third, we performed basic Arabic letter normalization, namely variants of the letter alef to bare alef, ta marbouta to ha, and alef maqsoura to ya. We also separated words that are commonly incorrectly attached such as ياكلب> (“yAklb” – “O dog”), is split to يا كلب> (“yA klb”). Lastly, we normalized letter repetitions to allow for a maximum of 2 repeated letters. For example, the token ههههه> (“hhhhh” – “ha ha ha ..”) is normalized to هه> (“hh”). We also removed the Arabic short-diacritics (tashkeel) and word elongation (kashida).

## Experiments ::: Representations ::: Lexical Features

Since offensive words typically have a negative polarity, we wanted to test the effectiveness of using a polarity lexicon in detecting offensive tweets. For the lexicon, we used NileULex BIBREF26, which is an Arabic polarity lexicon containing 3,279 MSA and 2,674 Egyptian terms, out of which 4,256 are negative and 1,697 are positive. We used the counts of terms with positive polarity and terms with negative polarity in tweets as features.

## Experiments ::: Representations ::: Static Embeddings

We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings BIBREF27 with vector dimensionality of 300; AraVec skip-gram embeddings BIBREF28, trained on 66.9M Arabic tweets with 100-dimensional vectors; and Mazajak skip-gram embeddings BIBREF29, trained on 250M Arabic tweets with 300-dimensional vectors.

Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model BIBREF30 on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2$-$10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search.

## Experiments ::: Representations ::: Deep Contextualized Embeddings

We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) BIBREF31, UMLFIT BIBREF32, and OpenAI GPT BIBREF33, to name but a few, have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we used BERTbase-multilingual (hereafter as simply BERT) fine-tuning method to classify Arabic offensive language on Twitter as it eliminates the need of heavily engineered task-specific architectures. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge) on GLUE BIBREF34, RACE BIBREF35, and SQuAD BIBREF36 tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wikipedia text from 104 languages and comes with hundreds of millions of parameters. It contains an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. Though the training data for the BERT embeddings don't match our genre, these embedding use BP sub-word segments. Following devlin-2019-bert, the classification consists of introducing a dense layer over the final hidden state $h$ corresponding to first token of the sequence, [CLS], adding a softmax activation on the top of BERT to predict the probability of the $l$ label:

where $W$ is the task-specific weight matrix. During fine-tuning, all BERT parameters together with $W$ are optimized end-to-end to maximize the log-probability of the correct labels.

## Experiments ::: Classification Models

We explored different classifiers. When using lexical features and pre-trained static embeddings, we primarily used an SVM classifier with a radial basis function kernel. Only when using the Mazajak embeddings, we experimented with other classifiers such as AdaBoost and Logistic regression. We did so to show that the SVM classifier was indeed the best of the bunch, and we picked the Mazajak embeddings because they yielded the best results among all static embeddings. We used the Scikit Learn implementations of all the classifiers such as libsvm for the SVM classifier. We also experimented with fastText, which trained embeddings on our data. When using contextualized embeddings, we fine-tuned BERT by adding a fully-connected dense layer followed by a softmax classifier, minimizing the binary cross-entropy loss function for the training data. For all experiments, we used the PyTorch implementation by HuggingFace as it provides pre-trained weights and vocabulary.

## Experiments ::: Evaluation

For all of our experiments, we used 5-fold cross validation with identical folds for all experiments. Table TABREF27 reports on the results of using lexical features, static pre-trained embeddings with an SVM classifier, embeddings trained on our data with fastText classifier, and BERT over a dense layer with softmax activation. As the results show, using Mazajak/SVM yielded the best results overall with large improvements in precision over using BERT. We suspect that the Mazajak/SVM combination performed better than the BERT setup due to the fact that the Mazajak embeddings, though static, were trained on in-domain data, as opposed to BERT. Perhaps if BERT embeddings were trained on tweets, they might have outperformed all other setups. For completeness, we compared 7 other classifiers with SVM using Mazajak embeddings. As results in Table TABREF28 show, using SVM yielded the best results.

## Experiments ::: Error Analysis

We inspected the tweets of one fold that were misclassified by the Mazajak/SVM model (36 false positives/121 false negatives) to determine the most common errors.

## Experiments ::: Error Analysis ::: False Positives

had four main types:

[leftmargin=*]

Gloating: ex. يا هبيده> (“yA hbydp” - “O you delusional”) referring to fans of rival sports team for thinking they could win.

Quoting: ex. لما حد يسب ويقول يا كلب> (“lmA Hd ysb wyqwl yA klb” – “when someone swears and says: O dog”).

Idioms: ex. يا فاطر رمضان يا خاسر دينك> (“yA fATr rmDAn yA xAsr dynk” – “o you who does not fast Ramadan, you who have lost your religion”), which is a colloquial idiom.

Implicit Sarcasm: ex. يا خاين انت عايز تشكك>

في حب الشعب للريس> (“yA KAyn Ant EAwz t$kk fy Hb Al$Eb llrys” – “O traitor, (you) want to question the love of the people to the president ”) where the author is mocking the president's popularity.

## Experiments ::: Error Analysis ::: False Negatives

had two types:

[leftmargin=*]

Mixture of offensiveness and admiration: ex. calling a girl a puppy يا كلبوبة> (“yA klbwbp” – “O puppy”) in a flirtatious manner.

Implicit offensiveness: ex. calling for cure while implying sanity in: وتشفي حكام قطر من المرض> (“wt$fy HkAm qTr mn AlmrD” – “and cure Qatar rulers from illness”).

Many errors stem from heavy use of dialectal Arabic as well as ambiguity. Since BERT was trained on Wikipedia (MSA) and Google books, the model failed to classify tweets with dialectal cues. Conversely, Mazajak/SVM is more biased towards dialects, often failing to classify MSA tweets.

## Conclusion and Future Work

In this paper we presented a systematic method for building an Arabic offensive language tweet dataset that does not favor specific dialects, topics, or genres. We developed detailed guidelines for tagging the tweets as clean or offensive, including special tags for vulgar tweets and hate speech. We tagged 10,000 tweets, which we plan to release publicly and would constitute the largest available Arabic offensive language dataset. We characterized the offensive tweets in the dataset to determine the topics that illicit such language, the dialects that are most often used, the common modes of offensiveness, and the gender distribution of their authors. We performed this breakdown for offensive tweets in general and for vulgar and hate speech tweets separately. We believe that this is the first detailed analysis of its kind. Lastly, we conducted a large battery of experiments on the dataset, using cross-validation, to establish a strong system for Arabic offensive language detection. We showed that using static embeddings produced a competitive results on the dataset.

For future work, we plan to pursue several directions. First, we want explore target specific offensive language, where attacks against an entity or a group may employ certain expressions that are only offensive within the context of that target and completely innocuous otherwise. Second, we plan to examine the effectiveness of cross dialectal and cross lingual learning of offensive language.
