# Opinion Recommendation using Neural Memory Model

**Paper ID:** 1702.01517

## Abstract

We present opinion recommendation, a novel task of jointly predicting a custom review with a rating score that a certain user would give to a certain product or service, given existing reviews and rating scores to the product or service by other users, and the reviews that the user has given to other products and services. A characteristic of opinion recommendation is the reliance of multiple data sources for multi-task joint learning, which is the strength of neural models. We use a single neural network to model users and products, capturing their correlation and generating customised product representations using a deep memory network, from which customised ratings and reviews are constructed jointly. Results show that our opinion recommendation system gives ratings that are closer to real user ratings on Yelp.com data compared with Yelp's own ratings, and our methods give better results compared to several pipelines baselines using state-of-the-art sentiment rating and summarization systems.

## Introduction

Offering a channel for customers to share opinions and give scores to products and services, review websites have become a highly influential information source that customers refer to for making purchase decisions. Popular examples include IMDB on the movie domain, Epinions on the product domain, and Yelp on the service domain. Figure FIGREF4 shows a screenshot of a restaurant review page on Yelp.com, which offers two main types of information. First, an overall rating score is given under the restaurant name; second, detailed user reviews are listed below the rating.

Though offering a useful overview and details about a product or service, such information has several limitations. First, the overall rating is general and not necessarily agreeable to the taste of individual customers. Being a simple reflection of all customer scores, it serves an average customer well, but can be rather inaccurate for individuals. For example, the authors themselves often find highly rated movies being tedious. Second, there can be hundreds of reviews for a product or service, which makes it infeasible for exhaustive reading. It would be useful to have a brief summary of all reviews, which ideally should be customized to the reader.

We investigate the feasibility of a model that addresses the limitations above. There are two sources of information that the model should collect to achieve its goal, namely information on the target product, and information about the user. The former can be obtained from reviews written by other customers about the target product, and the latter can be obtained from the reviews that the user has written for other products and services. Given the above two sources of information, the model should generate a customized score of the product that the user is likely to give after trying, as well as a customized review that the user would have written for the target product.

We refer to the task above using the term opinion recommendation, which is a new task, yet closely related to several existing lines of work in NLP. The first is sentiment analysis BIBREF0 , BIBREF1 , which is to give a rating score based on a customer review. Our task is different in that we aim to predict user rating scores of new product, instead of predicting the opinion score of existing reviews. The second is opinion summarization BIBREF2 , BIBREF3 , which is to generate a summary based on reviews of a product. A major difference between our task and this task is that the summary must be customized to a certain user, and a rating score must additionally be given. The third is recommendation BIBREF4 , BIBREF5 , which is to give a ranking score for a certain product or service based on the purchase history of the user and other customers who have purchased the target product. Our task is different in the source of input, which is textual customer reviews and ratings rather than numerical purchase history.

There are three types of inputs for our task, namely the reviews of the target product, the reviews of the user on other products, and other users reviews on other products, and two types of outputs, namely a customized rating score and a customized review. The ideal solution should consider the interaction between all given types of information, jointly predicting the two types of outputs. This poses significant challenges to statistical models, which require manually defined features to capture relevant patterns from training data. Deep learning is a relatively more feasible choice, offering viabilities of information fusion by fully connected hidden layers BIBREF6 , BIBREF7 . We leverage this advantage in building our model.

In particular, we use a recurrent neural network to model the semantic content of each review. A neural network is used to consolidate existing reviews for the target product, serving the role of a product model. In addition, a user model is built by consolidating the reviews of the given user into a single vector form. Third, to address potential sparsity of a user's history reviews, neighbor users are identified by collaborative filtering BIBREF8 , and a vector representation is learned by using a neural neighborhood model, which consolidates their history reviews. Finally, a deep memory network is utilized to find the association between the user and target product, jointly yielding the rating score and customised review.

Experiments on a Yelp dataset show that the model outperforms several pipelined baselines using state-of-the-art techniques. In particular, review scores given by the opinion recordation system are closer to real user review scores compared to the review scores which Yelp assigns to target products. Our code is released at http://github.com/anonymous.

## Related Work

Sentiment Analysis. Our task is related to document-level sentiment classification BIBREF1 , which is to infer the sentiment polarity of a given document. Recently, various neural network models are used to capture the sentimental information automatically, including convolutional neural networks BIBREF9 , recursive neural network BIBREF10 and recurrent neural network BIBREF11 , BIBREF12 , which have been shown to achieve competitive results across different benchmarks. Different from binary classification, review rating prediction aims to predict the numeric rating of a given review. PangL05 pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews BIBREF13 , BIBREF14 , BIBREF15 . Recently, TangQLY15 proposed a neural network model to predict the rating score by using both lexical semantic and user model.

Beyond textural features, user information is also investigated in the literature of sentiment analysis. For example, gao2013modeling developed user-specific features to capture user leniency, and li2014suit incorporated textual topic and user-word factors through topic modeling. For integrating user information into neural network models, TangQLY15 predicted the rating score given a review by using both lexical semantic information and a user embedding model. ChenSTLL16 proposed a neural network to incorporate global user and product information for sentiment classification via an attention mechanism.

Different from the above research on sentiment analysis, which focuses on predicting the opinion on existing reviews. Our task is to recommend the score that a user would give to a new product without knowing his review text. The difference originates from the object, previous research aims to predict opinions on reviewed products, while our task is to recommend opinion on new products, which the user has not reviewed.

Opinion Summarization. Our work also overlaps with to the area of opinion summarization, which constructs natural language summaries for multiple product reviews BIBREF0 . Most previous work extracts opinion words and aspect terms. Typical approaches include association mining of frequent candidate aspects BIBREF0 , BIBREF16 , sequence labeling based methods BIBREF17 , BIBREF18 , as well as topic modeling techniques BIBREF19 . Recently, word embeddings and recurrent neural networks are also used to extract aspect terms BIBREF20 , BIBREF21 .

Aspect term extraction approaches lack critical information for a user to understand how an aspect receives a particular rating. To address this, NishikawaHMK10 generated summaries by selecting and ordering sentences taken from multiple review texts according to affirmativeness and readability of the sentence order. WangL11 adopted both sentence-ranking and graph-based methods to extract summaries on an opinion conversation dataset. While all the methods above are extractive, ganesan2010opinosis presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and WangL16 used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews.

Different from the above research on opinion summarization, we generate a customized review to a certain user, and a rating score must be additionally given.

Recommendation. Recommendation systems suggest to a user new products and services that might be of their interest. There are two main approaches, which are content-based and collaborative-filtering (CF) based BIBREF22 , BIBREF5 , respectively. Most existing social recommendation systems are CF-based, and can be further grouped into model-based CF and neighborhood-based CF BIBREF23 , BIBREF4 . Matrix Factorization (MF) is one of the most popular models for CF. In recent MF-based social recommendation works, user-user social trust information is integrated with user-item feedback history (e.g., ratings, clicks, purchases) to improve the accuracy of traditional recommendation systems, which only factorize user-item feedback data BIBREF8 , BIBREF24 , BIBREF25 .

There has been work integrating sentiment analysis and recommendation systems, which use recommendation strategies such as matrix factorization to improve the performance of sentiment analysis BIBREF26 , BIBREF27 . These methods typically use ensemble learning BIBREF27 or probabilistic graph models BIBREF28 . For example, ZhangL0ZLM14 who proposed a factor graph model to recommend opinion rating scores by using explicit product features as hidden variables.

Different from the above research on recommendation systems, which utilize numerical purchase history between users and products, we work with textual information. In addition, recommendation systems only predict a rating score, while our system generates also a customized review, which is more informative.

Neural Network Models. Multi-task learning has been recognised as a strength of neural network models for natural language processing BIBREF6 , BIBREF7 , BIBREF29 , BIBREF30 , where hidden feature layers are shared between different tasks that have common basis. Our work can be regarded as an instance of such multi-tasks learning via shared parameters, which has been widely used in the research community recently.

Dynamic memory network models are inspired by neural turing machines BIBREF31 , and have been applied for NLP tasks such as question answering BIBREF32 , BIBREF33 , language modeling BIBREF34 and machine translation BIBREF35 . It is typically used to find abstract semantic representations of texts towards certain tasks, which are consistent with our main need, namely abstracting the representation of a product that is biased towards the taste of a certain user.

## Model

Formally, the input to our model is a tuple INLINEFORM0 , where INLINEFORM1 is the set of existing reviews of a target product, INLINEFORM2 is the set of user's history reviews, and INLINEFORM3 is the set of the user's neighborhood reviews. All the reviews are sorted with temporal order. The output is a pair INLINEFORM4 , where INLINEFORM5 is a real number between 0 and 5 representing the rating score of the target product, and INLINEFORM6 is a customised review.

For capturing both general and personalized information, we first build a product model, a user model, and a neighborhood model, respectively, and then use a memory network model to integrate these three types of information, constructing a customized product model. Finally, we predict a customized rating score and a review collectively using neural stacking. The overall architecture of the model is shown in Figure FIGREF5 .

## Review Model

A customer review is the foundation of our model, based on which we derive representations of both a user and a target product. In particular, a user profile can be achieved by modeling all the reviews of the user INLINEFORM0 , and a target product profile can be obtained by using all existing reviews of the product INLINEFORM1 . We use the average of word embeddings to model a review. Formally, given a review INLINEFORM2 , where INLINEFORM3 is the length of the review, each word INLINEFORM4 is represented with a K-dimensional embedding INLINEFORM5 BIBREF36 . We use the INLINEFORM6 for the representation of the review INLINEFORM7 .

## User Model

A standard LSTM BIBREF37 without coupled input and forget gates or peephole connections is used to learn the hidden states of the reviews. Denoting the recurrent function at step INLINEFORM0 as INLINEFORM1 , we obtain a sequence of hidden state vectors INLINEFORM2 recurrently by feeding INLINEFORM3 as inputs, where INLINEFORM4 . The initial state and all stand LSTM parameters are randomly initialized and tuned during training.

Not all reviews contribute equally to the representation of a user. We introduce an attention mechanism BIBREF38 , BIBREF39 to extract the reviews that are relatively more important, and aggregate the representation of reviews to form a vector. Taking the hidden state INLINEFORM0 of user model as input, the attention model outputs, a continuous vector INLINEFORM1 , which is computed as a weighted sum of each hidden state INLINEFORM2 , namely DISPLAYFORM0 

where INLINEFORM0 is the hidden variable size, INLINEFORM1 is the weight of INLINEFORM2 , and INLINEFORM3 .

For each piece of hidden state INLINEFORM0 , the scoring function is calculated by DISPLAYFORM0 DISPLAYFORM1 

where INLINEFORM0 and INLINEFORM1 are model parameters. The attention vector INLINEFORM2 is used to represent the User Model.

## Finding Neighbor Users

We use neighborhood reviews to improve the user model, since a user may not have sufficient reviews to construct a reliable model. Here a neighbor refers to a user that has similar tastes to the target user BIBREF24 , BIBREF40 . The same as the user model, we construct the neighborhood model INLINEFORM0 using the neighborhood reviews INLINEFORM1 with an attention recurrent network.

A key issue in building the neighborhood model is how to find neighbors of a certain user. In this study, we use matrix factorization BIBREF24 to detect neighbors, which is a standard approach for recommendation BIBREF8 , BIBREF41 , BIBREF25 . In particular, users' rating scores of products are used to build a product-users matrix INLINEFORM0 with INLINEFORM1 products and INLINEFORM2 users. We approximate it using three factors, which specify soft membership of products and users BIBREF8 by finding: DISPLAYFORM0 

where INLINEFORM0 represents the posterior probability of INLINEFORM1 topic clusters for each product; INLINEFORM2 encodes the distribution of each topic INLINEFORM3 ; and INLINEFORM4 indicates the posterior probability of INLINEFORM5 topic clusters for each user.

As a result of matrix factorization, we directly obtain the probability of each user on each topic from the person-topic matrix INLINEFORM0 . To infer INLINEFORM1 , the optimization problem in Eq. EQREF12 can be solved using the following updating rule: DISPLAYFORM0 

Obtaining the user-topic matrix INLINEFORM0 , we measure the implicit connection between two users using: DISPLAYFORM0 

where INLINEFORM0 measure the implicit connection degree between users INLINEFORM1 and INLINEFORM2 . If INLINEFORM3 is higher than a threshold INLINEFORM4 , we consider user INLINEFORM5 as the neighbor of user INLINEFORM6 .

## Product Model

Given the representations of existing reviews INLINEFORM0 of the product, we use a LSTM to model their temporal orders, obtaining a sequence of hidden state vectors INLINEFORM1 by recurrently feeding INLINEFORM2 as inputs. The hidden state vectors INLINEFORM3 are used to represent the product.

## Customized Product Model

We use the user representation INLINEFORM0 and the neighbour representation INLINEFORM1 to transform the target product representation INLINEFORM2 into a customised product representation INLINEFORM3 , which is tailored to the taste of the user. In particular, a dynamic memory network BIBREF32 , BIBREF42 is utilized to iteratively find increasingly abstract representations of INLINEFORM4 , by injecting INLINEFORM5 and INLINEFORM6 information.

The memory model consists of multiple dynamic computational layers (hops), each of which contains an attention layer and a linear layer. In the first computational layer (hop 1), we take the hidden variables INLINEFORM0 ( INLINEFORM1 ) of product model as input, adaptively selecting important evidences through one attention layer using INLINEFORM2 and INLINEFORM3 . The output of the attention layer gives a linear interpolation of INLINEFORM4 , and the result is considered as input to the next layer (hop 2). In the same way, we stack multiple hops and run the steps multiple times, so that more abstract representations of the target product can be derived.

The attention model outputs a continuous vector INLINEFORM0 , which is computed as a weighted sum of INLINEFORM1 ( INLINEFORM2 ), namely DISPLAYFORM0 

where INLINEFORM0 is the hidden variable size, INLINEFORM1 is the weight of INLINEFORM2 , and INLINEFORM3 . For each piece of hidden state INLINEFORM4 , we use a feed forward neural network to compute its semantic relatedness with the abstract representation INLINEFORM5 . The scoring function is calculated as follows at hop INLINEFORM6 : DISPLAYFORM0 DISPLAYFORM1 

The vector INLINEFORM0 is used to represent the customized product model. At the first hop, we define INLINEFORM1 .

The product model INLINEFORM0 ( INLINEFORM1 ) represents salient information of existing reviews in their temporal order, they do not reflect the taste of a particular user. We use the customised product model to integrate user information and product information (as reflected by the product model), resulting in a single vector that represents a customised product. From this vector we are able to synthesis both a customised review and a customised rating score.

## Customized Review Generation

The goal of customized review generation is to generate a review INLINEFORM0 from the customized product representation INLINEFORM1 , composed by a sequence of words INLINEFORM2 . We decompose the prediction of INLINEFORM3 into a sequence of word-level predictions: DISPLAYFORM0 

where each word INLINEFORM0 is predicted conditional on the previously generated INLINEFORM1 and the input INLINEFORM2 . The probability is estimated by using standard word softmax: DISPLAYFORM0 

where INLINEFORM0 is the hidden state variable at timestamp INLINEFORM1 , which is modeled as INLINEFORM2 . Here a LSTM is used to generate a new state INLINEFORM3 from the representation of the previous state INLINEFORM4 and INLINEFORM5 . INLINEFORM6 is the concatenation of previously generated word INLINEFORM7 and the input representation of customized model INLINEFORM8 .

## Customized Opinion Rating Prediction

We consider two factors for customised opinion rating, namely existing review scores and the customised product representation INLINEFORM0 . A baseline rating system such as Yelp.com uses only the former information, typically by taking the average of existing review scores. Such a baseline gives an empirical square error of 1.28 (out of 5) in our experiments, when compared with a test set of individual user ratings, which reflects the variance in user tastes. In order to integrate user preferences into the rating, we instead take a weighted average of existing ratings cores, so that the scores of reviews that are closer to the user preference are given higher weights.

As a second factor, we calculate a review score independently according to the customised representation INLINEFORM0 of existing reviews, without considering review scores. The motivation is two fold. First, existing reviews can be relatively few, and hence using their scores alone might not be sufficient for a confident score. Second, existing ratings can be all different from a user¡¯s personal rating, if the existing reviews do not come from the user's neighbours. As a result, using the average or weighted average of existing reviews, the personalised user rating might not be reached.

Formally, given the rating scores INLINEFORM0 of existing reviews, and the the customized product representation INLINEFORM1 , we calculate: DISPLAYFORM0 

In the left term INLINEFORM0 , we use attention weights INLINEFORM1 to measure the important of each rating score INLINEFORM2 . The right term INLINEFORM3 is a review-based shift, weighted by INLINEFORM4 .

Since the result of customized review generation can be helpful for rating score prediction, we use neural stacking additionally feeding the last hidden state INLINEFORM0 of review generation model as input for INLINEFORM1 prediction, resulting in DISPLAYFORM0 

where INLINEFORM0 denotes vector concatenation.

## Training

For our task, there are two joint training objectives, for review scoring and review summarisation, respectively. The loss function for the former is defined as: DISPLAYFORM0 

where INLINEFORM0 is the predicted rating score, INLINEFORM1 is the rating score in the training data, INLINEFORM2 is the set of model parameters and INLINEFORM3 is a parameter for L2 regularization.

We train the customized review generation model by maximizing the log probability of Eq. EQREF21 BIBREF43 , BIBREF44 . Standard back propagation is performed to optimize parameters, where gradients also propagate from the scoring objective to the review generation objective due to neural stacking (Eq. EQREF25 ). We apply online training, where model parameters are optimized by using Adagrad BIBREF45 . For all LSTM models, we empirically set the size of the hidden layers to 128. We train word embeddings using the Skip-gram algorithm BIBREF36 , using a window size of 5 and vector size of 128. In order to avoid over-fitting, dropout BIBREF46 is used for word embedding with a ratio of 0.2. The neighbor similarity threshold INLINEFORM0 is set to 0.25.

## Experimental Settings

Our data are collected from the yelp academic dataset, provided by Yelp.com, a popular restaurant review website. The data set contains three types of objects: business, user, and review, where business objects contain basic information about local businesses (i.e. restaurants), review objects contain review texts and star rating, and user objects contain aggregate information about a single user across all of Yelp. Table TABREF31 illustrates the general statistics of the dataset.

For evaluating our model, we choose 4,755 user-product pairs from the dataset. For each pair, the existing reviews of the target service (restaurant) are used for the product model. The rating score given by each user to the target service is considered as the gold customized rating score, and the review of the target service given by each user is used as the gold-standard customized review for the user. The remaining reviews of each user are used for training the user model. We use 3,000 user-product pairs to train the model, 1,000 pairs as testing data, and remaining data for development.

We use the ROUGE-1.5.5 BIBREF47 toolkit for evaluating the performance of customized review generation, and report unigram overlap (ROUGE-1) as a means of assessing informativeness. We use Mean Square Error (MSE) BIBREF15 , BIBREF48 is used as the evaluation metric for measuring the performance of customized rating score prediction. MSE penalizes more severe errors more heavily.

## Development Experiments

Effects of various configurations of our model, are shown on Table TABREF34 , where Joint is the full model of this paper, -user ablates the user model, -neighbor ablates the neighbor model, -rating is a single-task model that generates a review without the rating score, and -generation generates only the rating score.

By comparing “Joint” and “-user,-neighbor”, we can find that customized information have significant influence on both the rating and review generation results ( INLINEFORM0 using INLINEFORM1 -test). In addition, comparison between “-Joint” and “-user”, and between “-user” and “-user, -neighbor” shows that both the user information and the neighbour user information of the user are effective for improving the results. A user¡¯s neighbours can indeed alleviate scarcity of user reviews.

Finally, comparison between “Joint” and “-generation”, and between “Joint” and “-rating” shows that multi-task learning by parameter sharing is highly useful.

We show the influence of hops of memory network for rating prediction on Figure FIGREF36 . Note that, the model would only consider the general product reviews ( INLINEFORM0 ), when INLINEFORM1 . From the figure we can find that, when INLINEFORM2 , the performance is the best. It indicates that multiple hops can capture more abstract evidences from external memory to improve the performance. However, too many hops leads to over-fitting, thereby harms the performance. As a result, we choose 3 as the number of hops in our final test.

We show the influence of the bias weight parameter INLINEFORM0 for rating prediction in Figure FIGREF38 . With INLINEFORM1 being 0, the model uses the weighted sum of existing reviews to score the product. When INLINEFORM2 is very large, the system tends to use only the customized product representation INLINEFORM3 to score the product, hence ignoring existing review scores, which are a useful source of information. Our results show that when INLINEFORM4 is 1, the performance is optimal, thus indicating both existing review scores and review contents are equally useful.

## Final Results

We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems:

RS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score.

RS-Linear estimates the rating score that a user would give by INLINEFORM0 BIBREF49 , where INLINEFORM1 and INLINEFORM2 are the the training deviations of the user INLINEFORM3 and the product INLINEFORM4 , respectively.

RS-Item applies INLINEFORM0 NN to estimate the rating score BIBREF50 . We choose the cosine similarity between INLINEFORM1 to measure the distance between product.

RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score BIBREF8 , BIBREF41 , BIBREF25 .

Sum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions BIBREF51 .

Sum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding BIBREF44 , BIBREF3 .

All the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table TABREF46 . Our model (“ Joint”) significantly outperforms both “RS-Average” and “RS-Linear” ( INLINEFORM0 using INLINEFORM1 -test), which demonstrates the strength of opinion recommendation, which leverages user characteristics for calculating a rating score for the user.

Our proposed model also significantly outperforms state-of-the-art recommendation systems (RS-Item and RS-MF) ( INLINEFORM0 using INLINEFORM1 -test), indicating that textual information are a useful addition to the rating scores themselves for recommending a product.

Finally, comparison between our proposed model and state-of-the-art summarisation techniques (Sum-Opinosis and Sum-LSTM-Att) shows the advantage of leveraging user information to enhance customised review generation, and also the strength of joint learning.

## Conclusion

We presented a dynamic memory model for opinion recommendation, a novel task of jointly predicting the review and rating score that a certain user would give to a certain product or service. In particular, a deep memory network was utilized to find the association between the user and the product, jointly yielding the rating score and customised review. Results show that our methods are better results compared to several pipelines baselines using state-of-the-art sentiment rating and summarisation systems.
