# Neural Word Segmentation with Rich Pretraining

**Paper ID:** 1704.08960

## Abstract

Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.

## Introduction

There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Neural network models have been exploited due to their strength in non-sparse representation learning and non-linear power in feature combination, which have led to advances in many NLP tasks. So far, neural word segmentors have given comparable accuracies to the best statictical models.

With respect to non-sparse representation, character embeddings have been exploited as a foundation of neural word segmentors. They serve to reduce sparsity of character ngrams, allowing, for example, “猫(cat) 躺(lie) 在(in) 墙角(corner)” to be connected with “狗(dog) 蹲(sit) 在(in) 墙角(corner)” BIBREF0 , which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams BIBREF6 , BIBREF1 and words BIBREF2 , BIBREF5 have also been shown to improve segmentation accuracies.

With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on five-character windows BIBREF0 , BIBREF6 , BIBREF1 , BIBREF7 , as well as LSTMs on characters BIBREF3 , BIBREF8 and words BIBREF2 , BIBREF4 , BIBREF5 . For structured learning and inference, CRF has been used for character sequence labelling models BIBREF1 , BIBREF3 and structural beam search has been used for word-based segmentors BIBREF4 , BIBREF5 .

Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing BIBREF9 . Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation BIBREF10 , BIBREF11 , and making use of self-predictions BIBREF12 , BIBREF13 . It has also utilised heterogenous annotations such as POS BIBREF14 , BIBREF15 and segmentation under different standards BIBREF16 . To our knowledge, such rich external information has not been systematically investigated for neural segmentation.

We fill this gap by investigating rich external pretraining for neural segmentation. Following BIBREF4 and BIBREF5 , we adopt a globally optimised beam-search framework for neural structured prediction BIBREF9 , BIBREF17 , BIBREF18 , which allows word information to be modelled explicitly. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data. We adopt a multi-task learning strategy BIBREF19 , casting each external source of information as a auxiliary classification task, sharing a five-character window network. After pretraining, the character window network is used to initialize the corresponding module in our segmentor.

Results on 6 different benchmarks show that our method outperforms the best statistical and neural segmentation models consistently, giving the best reported results on 5 datasets in different domains and genres. Our implementation is based on LibN3L BIBREF20 . Code and models can be downloaded from http://gitHub.com/jiesutd/RichWordSegmentor

## Related Work

Work on statistical word segmentation dates back to the 1990s BIBREF21 . State-of-the-art approaches include character sequence labeling models BIBREF22 using CRFs BIBREF23 , BIBREF24 and max-margin structured models leveraging word features BIBREF25 , BIBREF26 , BIBREF27 . Semi-supervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation BIBREF11 , BIBREF12 , BIBREF13 , BIBREF28 . Our work belongs to recent neural word segmentation.

To our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, BIBREF11 empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information. Their baseline is similar to ours in the sense that both character and word contexts are considered. On the other hand, their model is statistical while ours is neural. Consequently, they integrate external knowledge as features, while we integrate it by shared network parameters. Our results show a similar degree of error reduction compared to theirs by using external data.

Our model inherits from previous findings on context representations, such as character windows BIBREF6 , BIBREF1 , BIBREF7 and LSTMs BIBREF3 , BIBREF8 . Similar to BIBREF5 and BIBREF4 , we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with BIBREF5 and BIBREF4 , allowing a central sub module, namely a five-character context window, to be pretrained.

## Model

Our segmentor works incrementally from left to right, as the example shown in Table TABREF1 . At each step, the state consists of a sequence of words that have been fully recognized, denoted as INLINEFORM0 , a current partially recognized word INLINEFORM1 , and a sequence of next incoming characters, denoted as INLINEFORM2 , as shown in Figure FIGREF4 . Given an input sentence, INLINEFORM3 and INLINEFORM4 are initialized to INLINEFORM5 and INLINEFORM6 , respectively, and INLINEFORM7 contains all the input characters. At each step, a decision is made on INLINEFORM8 , either appending it as a part of INLINEFORM9 , or seperating it as the beginning of a new word. The incremental process repeats until INLINEFORM10 is empty and INLINEFORM11 is null again ( INLINEFORM12 , INLINEFORM13 ). Formally, the process can be regarded as a state-transition process, where a state is a tuple INLINEFORM14 , and the transition actions include Sep (seperate) and App (append), as shown by the deduction system in Figure FIGREF7 .

In the figure, INLINEFORM0 denotes the score of a state, given by a neural network model. The score of the initial state (i.e. axiom) is 0, and the score of a non-axiom state is the sum of scores of all incremental decisions resulting in the state. Similar to BIBREF5 and BIBREF4 , our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions.

Different from previous work, the structure of our scoring network is shown in Figure FIGREF4 . It consists of three main layers. On the bottom is a representation layer, which derives dense representations INLINEFORM0 and INLINEFORM1 for INLINEFORM2 and INLINEFORM3 , respectively. We compare various distributed representations and neural network structures for learning INLINEFORM4 and INLINEFORM5 , detailed in Section SECREF8 . On top of the representation layer, we use a hidden layer to merge INLINEFORM6 and INLINEFORM7 into a single vector DISPLAYFORM0 

The hidden feature vector INLINEFORM0 is used to represent the state INLINEFORM1 , for calculating the scores of the next action. In particular, a linear output layer with two nodes is employed: DISPLAYFORM0 

The first and second node of INLINEFORM0 represent the scores of Sep and App given INLINEFORM1 , namely INLINEFORM2 , INLINEFORM3 respectively.

## Representation Learning

Characters. We investigate two different approaches to encode incoming characters, namely a window approach and an LSTM approach. For the former, we follow prior methods BIBREF22 , BIBREF1 , using five-character window INLINEFORM0 to represent incoming characters. Shown in Figure FIGREF13 , a multi-layer perceptron (MLP) is employed to derive a five-character window vector INLINEFORM1 from single-character vector representations INLINEFORM2 . DISPLAYFORM0 

For the latter, we follow recent work BIBREF3 , BIBREF5 , using a bi-directional LSTM to encode input character sequence. In particular, the bi-directional LSTM hidden vector INLINEFORM0 of the next incoming character INLINEFORM1 is used to represent the coming characters INLINEFORM2 given a state. Intuitively, a five-character window provides a local context from which the meaning of the middle character can be better disambiguated. LSTM, on the other hand, captures larger contexts, which can contain more useful clues for dismbiguation but also irrelevant information. It is therefore interesting to investigate a combination of their strengths, by first deriving a locally-disambiguated version of INLINEFORM3 , and then feed it to LSTM for a globally disambiguated representation.

Now with regard to the single-character vector representation INLINEFORM0 , we follow previous work and consider both character embedding INLINEFORM1 and character-bigram embedding INLINEFORM2 , investigating the effect of each on the accuracies. When both INLINEFORM3 and INLINEFORM4 are utilized, the concatenated vector is taken as INLINEFORM5 .

Partial Word. We take a very simple approach to representing the partial word INLINEFORM0 , by using the embedding vectors of its first and last characters, as well as the embedding of its length. Length embeddings are randomly initialized and then tuned in model training. INLINEFORM1 has relatively less influence on the empirical segmentation accuracies. DISPLAYFORM0 

Word. Similar to the character case, we investigate two different approaches to encoding incoming characters, namely a window approach and an LSTM approach. For the former, we follow prior methods BIBREF25 , BIBREF27 , using the two-word window INLINEFORM0 to represent recognized words. A hidden layer is employed to derive a two-word vector INLINEFORM1 from single word embeddings INLINEFORM2 and INLINEFORM3 . DISPLAYFORM0 

For the latter, we follow BIBREF5 and BIBREF4 , using an uni-directional LSTM on words that have been recognized.

## Pretraining

Neural network models for NLP benefit from pretraining of word/character embeddings, learning distributed sementic information from large raw texts for reducing sparsity. The three basic elements in our neural segmentor, namely characters, character bigrams and words, can all be pretrained over large unsegmented data. We pretrain the five-character window network in Figure FIGREF13 as an unit, learning the MLP parameter together with character and bigram embeddings. We consider four types of commonly explored external data to this end, all of which have been studied for statistical word segmentation, but not for neural network segmentors.

Raw Text. Although raw texts do not contain explicit word boundary information, statistics such as mutual information between consecutive characters can be useful features for guiding segmentation BIBREF11 . For neural segmentation, these distributional statistics can be implicitly learned by pretraining character embeddings. We therefore consider a more explicit clue for pretraining our character window network, namely punctuations BIBREF10 .

Punctuation can serve as a type of explicit mark-up BIBREF30 , indicating that the two characters on its left and right belong to two different words. We leverage this source of information by extracting character five-grams excluding punctuation from raw sentences, using them as inputs to classify whether there is punctuation before middle character. Denoting the resulting five character window as INLINEFORM0 , the MLP in Figure FIGREF13 is used to derive its representation INLINEFORM1 , which is then fed to a softmax layer for binary classification: DISPLAYFORM0 

Here INLINEFORM0 indicates the probability of a punctuation mark existing before INLINEFORM1 . Standard backpropagation training of the MLP in Figure FIGREF13 can be done jointly with the training of INLINEFORM2 and INLINEFORM3 . After such training, the embedding INLINEFORM4 and MLP values can be used to initialize the corresponding parameters for INLINEFORM5 in the main segmentor, before its training.

Automatically Segmented Text. Large texts automatically segmented by a baseline segmentor can be used for self-training BIBREF13 or deriving statistical features BIBREF12 . We adopt a simple strategy, taking automatically segmented text as silver data to pretrain the five-character window network. Given INLINEFORM0 , INLINEFORM1 is derived using the MLP in Figure FIGREF13 , and then used to classify the segmentation of INLINEFORM2 into B(begining)/M(middle)/E(end)/S(single character word) labels. DISPLAYFORM0 

Here INLINEFORM0 and INLINEFORM1 are model parameters. Training can be done in the same way as training with punctuation.

Heterogenous Training Data. Multiple segmentation corpora exist for Chinese, with different segmentation granularities. There has been investigation on leveraging two corpora under different annotation standards to improve statistical segmentation BIBREF16 . We try to utilize heterogenous treebanks by taking an external treebank as labeled data, training a B/M/E/S classifier for the character windows network. DISPLAYFORM0 

POS Data. Previous research has shown that POS information is closely related to segmentation BIBREF14 , BIBREF15 . We verify the utility of POS information for our segmentor by pretraining a classifier that predicts the POS on each character, according to the character window representation INLINEFORM0 . In particular, given INLINEFORM1 , the POS of the word that INLINEFORM2 belongs to is used as the output. DISPLAYFORM0 

Multitask Learning. While each type of external training data can offer one source of segmentation information, different external data can be complimentary to each other. We aim to inject all sources of information into the character window representation INLINEFORM0 by using it as a shared representation for different classification tasks. Neural model have been shown capable of doing multi-task learning via parameter sharing BIBREF19 . Shown in Figure FIGREF13 , in our case, the output layer for each task is independent, but the hidden layer INLINEFORM1 and all layers below INLINEFORM2 are shared.

For training with all sources above, we randomly sample sentences from the Punc./Auto-seg/Heter./POS sources with the ratio of 10/1/1/1, for each sentence in punctuation corpus we take only 2 characters (character before and after the punctuation) as input instances.

[t] InputInput OutputOutput Parameters: INLINEFORM0 

Process:

agenda INLINEFORM0 INLINEFORM1 

j in [0:Len( INLINEFORM0 )] beam = []

 INLINEFORM0 in agenda INLINEFORM1 = Action( INLINEFORM2 , Sep)

Add( INLINEFORM0 , beam)

 INLINEFORM0 = Action( INLINEFORM1 , App)

Add( INLINEFORM0 , beam)

 agenda INLINEFORM0 Top(beam, B)

 INLINEFORM0 agenda INLINEFORM1 = BestIn(agenda)

Update( INLINEFORM0 , INLINEFORM1 , INLINEFORM2 )

return

 INLINEFORM0 = BestIn(agenda)

Update( INLINEFORM0 , INLINEFORM1 , INLINEFORM2 )

return

Training

## Decoding and Training

To train the main segmentor, we adopt the global transition-based learning and beam-search strategy of BIBREF31 . For decoding, standard beam search is used, where the B best partial output hypotheses at each step are maintained in an agenda. Initially, the agenda contains only the start state. At each step, all hypotheses in the agenda are expanded, by applying all possible actions and B highest scored resulting hypotheses are used as the agenda for the next step.

For training, the same decoding process is applied to each training example INLINEFORM0 . At step INLINEFORM1 , if the gold-standard sequence of transition actions INLINEFORM2 falls out of the agenda, max-margin update is performed by taking the current best hypothesis INLINEFORM3 in the beam as a negative example, and INLINEFORM4 as a positive example. The loss function is DISPLAYFORM0 

where INLINEFORM0 is the number of incorrect local decisions in INLINEFORM1 , and INLINEFORM2 controls the score margin.

The strategy above is early-update BIBREF32 . On the other hand, if the gold-standard hypothesis does not fall out of the agenda until the full sentence has been segmented, a final update is made between the highest scored hypothesis INLINEFORM0 (non-gold standard) in the agenda and the gold-standard INLINEFORM1 , using exactly the same loss function. Pseudocode for the online learning algorithm is shown in Algorithm SECREF14 .

We use Adagrad BIBREF33 to optimize model parameters, with an initial learning rate INLINEFORM0 . INLINEFORM1 regularization and dropout BIBREF34 on input are used to reduce overfitting, with a INLINEFORM2 weight INLINEFORM3 and a dropout rate INLINEFORM4 . All the parameters in our model are randomly initialized to a value INLINEFORM5 , where INLINEFORM6 BIBREF35 . We fine-tune character and character bigram embeddings, but not word embeddings, acccording to BIBREF5 .

## Experimental Settings

Data. We use Chinese Treebank 6.0 (CTB6) BIBREF36 as our main dataset. Training, development and test set splits follow previous work BIBREF37 . In order to verify the robustness of our model, we additionally use SIGHAN 2005 bake-off BIBREF38 and NLPCC 2016 shared task for Weibo segmentation BIBREF39 as test datasets, where the standard splits are used. For pretraining embedding of words, characters and character bigrams, we use Chinese Gigaword (simplified Chinese sections), automatically segmented using ZPar 0.6 off-the-shelf BIBREF25 , the statictics of which are shown in Table TABREF24 .

For pretraining character representations, we extract punctuation classification data from the Gigaword corpus, and use the word-based ZPar and a standard character-based CRF model BIBREF40 to obtain automatic segmentation results. We compare pretraining using ZPar results only and using results that both segmentors agree on. For heterogenous segmentation corpus and POS data, we use a People's Daily corpus of 5 months. Statistics are listed in Table TABREF24 .

Evaluation. The standard word precision, recall and F1 measure BIBREF38 are used to evaluate segmentation performances.

Hyper-parameter Values. We adopt commonly used values for most hyperparameters, but tuned the sizes of hidden layers on the development set. The values are summarized in Table TABREF20 .

## Development Experiments

We perform development experiments to verify the usefulness of various context representations, network configurations and different pretraining methods, respectively.

The influence of character and word context representations are empirically studied by varying the network structures for INLINEFORM0 and INLINEFORM1 in Figure FIGREF4 , respectively. All the experiments in this section are performed using a beam size of 8.

Character Context. We fix the word representation INLINEFORM0 to a 2-word window and compare different character context representations. The results are shown in Table TABREF27 , where “no char” represents our model without INLINEFORM1 , “5-char window” represents a five-character window context, “char LSTM” represents character LSTM context and “5-char window + LSTM” represents a combination, detailed in Section SECREF8 . “-char emb” and “-bichar emb” represent the combined window and LSTM context without character and character-bigram information, respectively.

As can be seen from the table, without character information, the F-score is 84.62%, demonstrating the necessity of character contexts. Using window and LSTM representations, the F-scores increase to 95.41% and 95.51%, respectively. A combination of the two lead to further improvement, showing that local and global character contexts are indeed complementary, as hypothesized in Section SECREF8 . Finally, by removing character and character-bigram embeddings, the F-score decreases to 95.20% and 94.27%, respectively, which suggests that character bigrams are more useful compared to character unigrams. This is likely because they contain more distinct tokens and hence offer a larger parameter space.

Word Context. The influence of various word contexts are shown in Table TABREF28 . Without using word information, our segmentor gives an F-score of 95.66% on the development data. Using a context of only INLINEFORM0 (1-word window), the F-measure increases to 95.78%. This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors BIBREF5 , BIBREF4 . This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with BIBREF5 . The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins. Given that character context is what we pretrain, our model relies more heavily on them.

With both INLINEFORM0 and INLINEFORM1 being used for the context, the F-score further increases to 95.86%, showing that a 2-word window is useful by offering more contextual information. On the other hand, when INLINEFORM2 is also considered, the F-score does not improve further. This is consistent with previous findings of statistical word segmentation BIBREF25 , which adopt a 2-word context. Interestingly, using a word LSTM does not bring further improvements, even when it is combined with a window context. This suggests that global word contexts may not offer crucial additional information compared with local word contexts. Intuitively, words are significantly less polysemous compared with characters, and hence can serve as effective contexts even if used locally, to supplement a more crucial character context.

We verify the effectiveness of structured learning and inference by measuring the influence of beam size on the baseline segmentor. Figure FIGREF30 shows the F-scores against different numbers of training iterations with beam size 1,2,4,8 and 16, respectively. When the beam size is 1, the inference is local and greedy. As the size of the beam increases, more global structural ambiguities can be resolved since learning is designed to guide search. A contrast between beam sizes 1 and 2 demonstrates the usefulness of structured learning and inference. As the beam size increases, the gain by doubling the beam size decreases. We choose a beam size of 8 for the remaining experiments for a tradeoff between speed and accuracy.

Table TABREF31 shows the effectiveness of rich pretraining of INLINEFORM0 on the development set. In particular, by using punctuation information, the F-score increases from 95.86% to 96.25%, with a relative error reduction of 9.4%. This is consistent with the observation of BIBREF11 , who show that punctuation is more effective compared with mutual information and access variety as semi-supervised data for a statistical word segmentation model. With automatically-segmented data, heterogenous segmentation and POS information, the F-score increases to 96.26%, 96.27% and 96.22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation BIBREF16 , BIBREF12 , BIBREF28 . Finally, by integrating all above information via multi-task learning, the F-score is further improved to 96.48%, with a 15.0% relative error reduction.

Both our model and BIBREF5 use global learning and beam search, but our network is different. BIBREF5 utilizes the action history with LSTM encoder, while we use partial word rather than action information. Besides, the character and character bigram embeddings are fine-tuned in our model while BIBREF5 set the embeddings fixed during training. We study the F-measure distribution with respect to sentence length on our baseline model, multitask pretraining model and BIBREF5 . In particular, we cluster the sentences in the development dataset into 6 categories based on their length and evaluate their F1-values, respectively. As shown in Figure FIGREF35 , the models give different error distributions, with our models being more robust to the sentence length compared with BIBREF5 . Their model is better on very short sentences, but worse on all other cases. This shows the relative advantages of our model.

## Final Results

Our final results on CTB6 are shown in Table TABREF38 , which lists the results of several current state-of-the-art methods. Without multitask pretraining, our model gives an F-score of 95.44%, which is higher than the neural segmentor of BIBREF5 , which gives the best accuracies among pure neural segments on this dataset. By using multitask pretraining, the result increases to 96.21%, with a relative error reduction of 16.9%. In comparison, BIBREF11 investigated heterogenous semi-supervised learning on a state-of-the-art statistical model, obtaining a relative error reduction of 13.8%. Our findings show that external data can be as useful for neural segmentation as for statistical segmentation.

Our final results compare favourably to the best statistical models, including those using semi-supervised learning BIBREF11 , BIBREF12 , and those leveraging joint POS and syntactic information BIBREF37 . In addition, it also outperforms the best neural models, in particular BIBREF5 *, which is a hybrid neural and statistical model, integrating manual discrete features into their word-based neural model. We achieve the best reported F-score on this dataset. To our knowledge, this is the first time a pure neural network model outperforms all existing methods on this dataset, allowing the use of external data . We also evaluate our model pretrained only on punctuation and auto-segmented data, which do not include additional manual labels. The results on CTB test data show the accuracy of 95.8% and 95.7%, respectivley, which are comparable with those statistical semi-supervised methods BIBREF11 , BIBREF12 . They are also among the top performance methods in Table TABREF38 . Compared with discrete semi-supervised methods BIBREF11 , BIBREF12 , our semi-supervised model is free from hand-crafted features.

In addition to CTB6, which has been the most commonly adopted by recent segmentation research, we additionally evaluate our results on the SIGHAN 2005 bakeoff and Weibo datasets, to examine cross domain robustness. Different state-of-the-art methods for which results are recorded on these datasets are listed in Table TABREF40 . Most neural models reported results only on the PKU and MSR datasets of the bakeoff test sets, which are in simplified Chinese. The AS and CityU corpora are in traditional Chinese, sourced from Taiwan and Hong Kong corpora, respectively. We map them into simplified Chinese before segmentation. The Weibo corpus is in a yet different genre, being social media text. BIBREF41 achieved the best results on this dataset by using a statistical model with features learned using external lexicons, the CTB7 corpus and the People Daily corpus. Similar to Table TABREF38 , our method gives the best accuracies on all corpora except for MSR, where it underperforms the hybrid model of BIBREF5 by 0.2%. To our knowledge, we are the first to report results for a neural segmentor on more than 3 datasets, with competitive results consistently. It verifies that knowledge learned from a certain set of resources can be used to enhance cross-domain robustness in training a neural segmentor for different datasets, which is of practical importance.

## Conclusion

We investigated rich external resources for enhancing neural word segmentation, by building a globally optimised beam-search model that leverages both character and word contexts. Taking each type of external resource as an auxiliary classification task, we use neural multi-task learning to pre-train a set of shared parameters for character contexts. Results show that rich pretraining leads to 15.4% relative error reduction, and our model gives results highly competitive to the best systems on six different benchmarks.

## Acknowledgments

We thank the anonymous reviewers for their insightful comments and the support of NSFC 61572245. We would like to thank Meishan Zhang for his insightful discussion and assisting coding. Yue Zhang is the corresponding author.
