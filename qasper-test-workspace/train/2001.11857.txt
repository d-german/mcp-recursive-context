# Hybrid Tiled Convolutional Neural Networks for Text Sentiment Classification

**Paper ID:** 2001.11857

## Abstract

The tiled convolutional neural network (tiled CNN) has been applied only to computer vision for learning invariances. We adjust its architecture to NLP to improve the extraction of the most salient features for sentiment analysis. Knowing that the major drawback of the tiled CNN in the NLP field is its inflexible filter structure, we propose a novel architecture called hybrid tiled CNN that applies a filter only on the words that appear in the similar contexts and on their neighbor words (a necessary step for preventing the loss of some n-grams). The experiments on the datasets of IMDB movie reviews and SemEval 2017 demonstrate the efficiency of the hybrid tiled CNN that performs better than both CNN and tiled CNN.

## INTRODUCTION

Sentiment analysis or opinion mining is a sort of text classification that assigns a sentiment orientation to documents based on the detected contextual polarity BIBREF0. In the past, research work has focused only on the overall sentiment of a document, trying to determine if the entire text is positive, neutral or negative BIBREF1. However, besides predicting the general sentiment, a better understanding of the reviews could be undertaken using the more in-depth aspect based sentiment analysis (ABSA) BIBREF2, BIBREF1. Specifically, ABSA’s goal is to predict the sentiment polarity of the corpus’ target entities and aspects (e.g. the possible aspects of the entity “movie” could be “the plot”, “the actors’ acting” or “the special effects”). While it is easy to establish the text entities based on the a priori information about text corpus, the aspect terms are difficult to infer and usually the training process requires to use a predefined set of term categories BIBREF3, BIBREF4, BIBREF5.

In this paper, we propose a framework that applies the idea of ABSA only conceptually and tries to enhance the performance of one of the most popular baseline models for sentiment classification. More specifically, we adjust a convolutional neural network (CNN) to the ABSA requirements forcing it to capture more diverse features (both sentiment information and aspects). The CNN models were proposed for object recognition BIBREF6 and initially were used within computer vision. Soon they were adapted or integrated with other deep learning architectures to be compatible with a broad array of tasks, including NLP: classification BIBREF7, recognition of entailment and contraction between sentences BIBREF8 or question answering BIBREF9.

Even if the CNN model already captures the most salient features by employing global max-pooling operations on multiple feature maps, the effectiveness of extraction depends not only on the number of feature maps but also on the initialisation of their filter weights. Usually, all the filter weights of a convolutional layer are initialised with small values, following a uniform distribution. The max-pooling operations can generate a smaller number of different features than the given number of feature maps as a result of using the same input and of applying filters with weights of the same distribution. Our approach to control better features extraction and to force the network to look for features in different regions of a text consists of applying the tiled CNN (TCNN), a variant of the CNN architecture which considers multiple filters per feature map.

The TCNN is an improved CNN that was proposed for image processing to capture a wide range of picture invariances BIBREF10. Traditionally, the CNN architecture already learns the translational invariance due to its convolving operation but the TCNN goes forward and handles more complex cases such as viewpoint/rotation and size invariances. Since its development, the TCNN model has been mainly used for computer vision in tasks like image classification BIBREF11, object and emotion recognition BIBREF12, BIBREF13.

The reason behind the TCNN’s restricted applicability is its nature of multiple invariances learner. In this paper we adjust the model to meet the NLP requirements and set each filter of the feature map to cover only a set of $n$-grams (not all $n$-grams as in the case of pure CNN models). Following a precise filter structure per feature map, the extraction of the most relevant features is more effective and depends less on the initial values of the weights.

In addition to the TCNN, a new network called hybrid tiled CNN (HTCNN) is introduced to outweigh the disadvantage of the TCNN model's inflexible filter structure. Using the idea of more diverse feature extraction with multiple filters, we create word clusters and allow a filter of a feature map to convolve only on the words that appear in the same context (assigned to the same cluster) and on their $n – 1$ neighbouring words (where $n$ is the size of the $n$-gram or the filter size).

In this paper, word clusters are computed using the expectation-maximization (EM) algorithm using Gaussian distribution. Since we do not include information about aspects, this approach allows us to identify only the document-level sentiment polarities. However, HTCNN’s structure with multiple filters per feature map can be easily adjusted to ABSA sentiment classification task. An interesting idea could be to replace general word clusters defined a priori with sentence-level word clusters defined with respect to each aspect based on attention scores. We let this extension for future work.

Our contributions are summarized as follows:

We adjust the TCNN (which so far has been used only for image processing) to the NLP field.

We design a novel hybrid tiled CNN model with multiple filters per feature map. The filter structure is flexible and each filter of the feature map can convolve only on the similar words of a given word cluster and on its $n-1$ neighbouring words, where $n$ is the window size.

Experimental results prove the effectiveness of the introduced model over the simple architecture of the CNN.

The remainder of the paper is organized as follows. The next section discusses the related literature. The third section depicts the TCNN in NLP and its hybrid variant. The fourth section presents the details of the experiments and the results and the last one concludes the paper. The source code used to implement the proposed models can be found at https://github.com/mtrusca/HTCNN

## RELATED WORK

In text classification and sentiment analysis, CNN and RNN are the two main deep neural network architectures that produce similar results. While the CNN extracts the most relevant $n$-grams, the RNN finds context dependencies within an entire sentence. Still, for sentence classification, especially sentiment analysis it is tempting to use CNN models since polarities are usually determined only by some keywords BIBREF15. Regarding the sentence’s length, both neural networks have similar performances, especially for the case of short text. While for long sentences, the RNN could be considered more adequate than the CNN, some studies prove the contrary BIBREF16, BIBREF17, arguing that the RNN is a biased model where the later words of a sequence are more important than the others.

Currently, the CNN is a popular method for text classification BIBREF18, BIBREF19 and even models with little tuning of the hyperparameters provide competitive results BIBREF14, BIBREF20. The variant of the CNN applied for sentiment analysis task could vary from simple approaches BIBREF21 to more complex ones. For example, in BIBREF22, the idea of linguistic homophily is exploited and sentiment polarities are found by combining a single-layer CNN model with information about users’ social proximity. In BIBREF23, the results of a CNN sentiment model are improved by employing lexicon embeddings and attention mechanism.

Regarding the ABSA task, CNNs are not as popular as memory networks like LSTM and GRU even if they have shown potentiality to address this problem recently. The CNN model was introduced to tackle an ABSA problem in BIBREF24 by using parameterized gates and parameterized filters to include aspect information. Similar, in BIBREF25, it was proved that a simple CNN model with Gated Tanh-ReLU units is more accurate than the traditional approach with LSTM and attention mechanism.

Since not all datasets provide information about aspects, we try to increase the sensitivity of convolutional neural networks to the aspects and their related keywords by using a novel hybrid tiled CNN model. Our approach is similar to the one proposed in BIBREF26 in terms of multiple inputs for simultaneous convolutional layers but instead of using different word embeddings we create word clusters and define sentence representation for each one.

## MODELS

The standard one-dimensional CNN architecture employed in BIBREF27 and since then widely used in many papers, assumes that the feed-forward neural network is a composition of convolutional layers with the max-pooling operation and one or more fully connected layers with non-linear activation functions. An input text of length $m$ is represented as a matrix obtained by concatenating word embedding vectors BIBREF14:

This matrix is used as an input for a convolution operation that applies a filter $w$ on all possible windows of $n$ consecutive words at a given stride value. A new feature map is computed by applying a non-linear function $f$ with a bias term $b$ ($b$ $\in \mathbb {R}$) on the convolving results. Given the window of n words $X_{i:i+n-1}$, the generic object $C_i$ of the feature map is generated by:

To capture the most important feature $C_{i}$, we apply the max-pooling operation.

Usually, a single feature is not enough for depicting the entire sentence and therefore it is a common practice to generate multiple feature maps for each convolutional layer. All features, with the highest value for each feature map, input the following convolutional or fully connected layers. In terms of sentiment analysis, we are interested to extract not only sentiment information but also information about text corpus’ entities and their aspects (attributes) BIBREF0. Based on this need, the use of multiple feature maps per convolutional layer turns out to be a compulsory step.

## MODELS ::: TCNN

In this paper, we attempt to introduce the TCNN in the field of textual classification. The idea behind the TCNN is simple and assumes that each feature map is computed using at least two filters BIBREF10. The map of filters applied on the input is not randomly chosen but it has to follow a structure that imposes for each filter to have different nearby filters, while the next and previous $k$-th filters have to be the same. Term $k$ is known as the tile size and it is a hyperparameter that indicates the number of filters corresponding to each feature map. The generic object $C_i$ of a feature map is defined as:

If $k$ is 1 then the tiled convolutional layer turns into a simple convolutional layer and if $k$ is greater or equal to $m-n+1$ then we have a fully connected layer. While one of CNN's rules is parameter sharing, the tiled CNN follows it only partially so that weights are not entirely tied.

To better understand the way the tiled CNN works with textual data we consider an example sentence (“We choose to go to the moon”) in Figure FIGREF4. We set the number of feature maps to three and the filter size and the tile size to two. The first filter convolves over the first two words ("we" and "choose"), then the second one covers the second and the third words ("choose" and "to"), then the first filter goes over the third and the fourth words ("to" and "go") and so on. This rule is applied for each feature map, which means that six weight matrices have to be initialised. Then a global max-pooling operation covers each of the three feature maps and the most representative $n$-grams are obtained (no matter the initial filter that identified them). The CNN's representation is similar to the one presented in Figure FIGREF4 but, unlike its tiled variant, there is only a filter per feature map (the tile size is equal to 1).

The TCNN implementation can be seen as a neural network with multiple simultaneous convolutional layers. Each layer corresponds to a filter and uses a stride value equal to the tile size. An important note to this process is related to the shape of the input that feeds each convolutional layer. Generally, filter $i$ ($i \le k$) can slide from the $i$-th word to the word with the index equal to:

where $\lfloor x \rfloor $ and $\lbrace x \rbrace $ represent the integer and decimal part of a number $x$, respectively. According to this rule, we have to adjust the input of each convolutional layer. This rule together with the constraint of the stride value enforces the filters to convolve on different $n$-grams and to cover together the entire input. Before moving further, we have to add two sets of pooling layers. The layers of the first set correspond to the $k$ convolutional layers. The second set has just one layer covering the previous concatenated results. By this means, it is assured that the multiple simultaneous CNN behaves just like a TCNN model with a single pooling operation.

The reasoning behind using the TCNN model for text classification is the need to capture more diverse features (like aspects and sentiment words) that could take our results closer to the sentences’ labels. Even if this task is theoretically accomplished by the simple CNN, the risk of getting a smaller number of different features than the number of feature maps is high. On the other side, TCNN with its integrated multiple filters per feature map forces the model to look for features in diverse regions of a sequence enhancing the quality of extraction. By choosing different values of $k$, we get a palette of models that are a trade-off between estimating a small number of parameters and having more different features.

## MODELS ::: HTCNN

The disadvantage of TCNN is the inflexible structure of filters sliding on the input that requires to have $k$ repetitive filters with no possibility to change the ordering. Knowing that words with semantic similarity are likely to appear in a similar context BIBREF28, one reasonable filter map could be the one that applies the same filter on all these words and other filters on the remaining words. For example in a sentence like “The lens of my digital camera have a powerful/weak optical zoom” it could be useful to apply the same filter on the “powerful” and “weak” words and different ones on the other words.

Knowing that word embeddings are built to capture not only words’ context but also their syntactic and semantic similarity, words like “powerful” and “weak” have similar word embeddings and could be assigned to the same cluster. Using an appropriate clustering model, we can group the word vectors in $k$ clusters. In the experiments, the clustering solution is provided by the EM algorithm using Gaussian distribution. The reason behind this choice is that Gaussian mixture models support mixed membership and are robust against errors and flexible regarding cluster covariance BIBREF29. The EM algorithm has two steps that are repeatedly applied till the convergence is noticed. First, we compute the probability of a word to belong to a cluster, knowing that the words of clusters follow normal distributions (the expectation step). Second, we update the cluster means accordingly with the previously computed probabilities (the maximization step).

For each cluster, we code the outside cluster words of a sentence with “PAD” and leave unchanged its associated words. Sequences generated based on indices of dictionary words, get 0 for “PAD” word and a value greater than 0 otherwise. In this way, we define $k$ inputs (one for each cluster) and use them to feed $k$ multiple simultaneous convolutional layers. The stride value constraint is used no more because simultaneous layers already convolve on different words.

The problem of the last depicted neural network is the loss of the $n$-grams concept caused by the replacement of the some words’ indexes with 0 (words assigned to other clusters than the one associated with the given convolutional layer). This idea is supported by the assumption that words are dependent on their neighbouring words. To overcome this problem, for each convolutional layer we have to modify the input by adding in the left and in the right side (if it is possible) of each word (whose index is different than 0) its $n-1$ neighbouring words. The added words could be in the same cluster or not. We call this model, the hybrid tiled CNN (HTCNN).

Considering the above sentence and setting the filter size and the number of clusters to two (the same as setting the tile size value for the TCNN), we assign the words “choose” and “moon” to the first cluster and the other words to the second one. Firstly we assign to the each cluster the same sentence and replace the words of the other cluster with “PAD”. The modified input sentences are: “PAD choose PAD PAD PAD PAD moon” and “we PAD to go to the PAD”. If we add the neighbors (a single word to the left and the right for bigram cases) of each word, the input sentences will change into “we choose to PAD PAD the moon” and “we choose to go to the moon”. The sentence of the first cluster gets the words “we” and “to” as the neighbors of the “choose” word and the word “the” as the neighbor of the “moon” word. The words of the second cluster are: “we”, “to”, “go”, “to”, “the” and the sentence gets the word “choose” as the neighbor of the words “we” and “to” and the word “moon” as the neighbor of the word “the”. This process is described in Figure FIGREF8, for representative reasons we use sentences instead of sequences. The colour of the cluster words assigned to each convolutional layer is red. The colour of the neighbouring words is blue. The remaining words are coded with the word "PAD". If the blue words are replaced with "PAD" words then we get a new model that does not consider the $n$-gram concept and it is called simple HTCNN (SHTCNN).

## EXPERIMENTAL RESULTS

We test the TCNN and the HTCNN models on the IMDB movie reviews dataset BIBREF30. The collection has 50000 documents, evenly separated between negative and positive reviews.

Our models based on convolution operation are compared to a one-layer one-dimensional CNN. The configuration of the CNN baseline model and of the proposed models is presented in Table TABREF13. Regarding word vectors initialisation, we use word embeddings trained on the current dataset with word2vec models BIBREF31. Each word is represented by a 200 dimension vector computed by concatenating a continuous bag-of-words (CBOW) 100 dimension vector with a skip-gram (SG) vector of the same size. Sentences are represented using zero-padding strategy which means we are applying the wide convolutions BIBREF18.

Knowing that the IMBD dataset is balanced, the evaluation of models is done in terms of test accuracy. We use the binary cross entropy loss function and unfreeze the word embeddings during the training phase. Using 4-fold cross-validation, 10,000 reviews are allocated to the testing set and the rest of them are divided between training and validation sets. The cross-validation process has four iterations and at each time step other 10,000 reviews are allocated to the validation set. In addition to the TCNN and its hybrid variant, we present the results SHTCNN model to see how the lack of the neighbouring words affects the performance of the HTCNN. The results of our comparison between the baseline network and our models are listed in Table TABREF15.

The baseline CNN has a volatile performance, unlike all other models. While for a filter size equal to two the model has one of the best test accuracies, for bigger sizes (trigram and fourgram cases) the performance decreases quickly and reaches the lowest threshold. All proposed networks have a natural behaviour and their performance grows proportionally with the filter size. Besides the $n$-gram dimension, the tile size or the number of word clusters has also impact on the results, generally leading to a gradual improvement.

Results of the TCNN and of the SHTCNN are complementary and while the first one performs better when we set the tile size to three, the second one has better results for two word clusters. However, we note that the HTCNN achieves the best performance without having the drawbacks of the other two models: the inflexible filter structure of the the TCNN and the SHTCNN’s lack of $n$-grams.

Natural, more filters per feature map makes the model more sensitive to different features but in the same way, increases the number of estimated parameters or the model complexity. Further on, the HTCNN is tested for larger tile sizes, precisely for four and five word clusters. The results are listed in Table TABREF16. Only the pair (k = 4, n = 2) has a slightly better result than the previous model variants with the same filter size (the pairs (k = 3, n = 2) and (k = 2, n = 2)). All other results are worse than the ones of HTCNN with the tile size equal to three and the difference is more significant for the case of five word clusters. We conclude that the optimal tile size value corresponding to the IMBD dataset that balances the model capacity to extract different features with its complexity is equal to three.

To confirm the performance of HTCNN over single-layer CNN, we undertake a second test on the SemEval-2017 English dataset for task 4: Sentiment analysis in Twitter, subtask A: Message Polarity Classification BIBREF32. The dataset has 62,617 tweets with the following statistics: 22,277 positive tweets, 28,528 neutral tweets and 11,812 negative tweets. The approach with the highest rank on this subtask was proposed by Cliche BIBREF33 and ensembles a LSTM and a neural network with multiple simultaneous convolutional layers. Since the Cliche’s CNN structure is similar to the structure of HTCNN, we use some of his default settings: the length of the word embeddings, the activation functions, the number of filters per convolutional layer, the dropout layers (with the same dropout probabilities) and the number of epochs. The number of simultaneous convolutional layers utilizes in the Cliche’s structure is equal to three which means that we have to run our HTCNN model for the same number of filters. Table TABREF13 shows an overview of the models' architecture for the SemEval dataset.

Word embeddings are trained using CBOW and SG models in a similar way as the one depicted above for the IMDB dataset and remain frozen over the entire training phase. 10% of the corpus tweets are allocated to the testing set while the remaining tweets are separated between validation and training sets using 9-fold cross-validation. Because the dataset is imbalanced, the evaluation is done using macro-average testing recall. To counteract the imbalance, the cross-entropy loss function uses class weights adjusted inversely proportional to the class frequencies in the training phase. Both the HTCNN and the CNN share the above hyperparameters. Similar to the IMDB experiments, we set the window size to two, three or four. The results of our comparison on the SemEval dataset are shown in Table TABREF17 and confirm the performance of HTCNN over simple CNN.

## CONCLUSIONS

In this paper, we present the TCNN model in the field of sentiment classification to improve the process of feature extraction. Due to the constraint of partial untying of filter weights that forces the network to apply the same filter at each k step, we introduced the HTCNN model. The new architecture combines the benefits of word embeddings clustering with the idea of tiling and if the tile size is chosen properly, the model could achieve competitive performance with the TCNN and better results than the reference CNN model. Setting the tile size or the number of word clusters could be difficult. Because the HTCNN works like a simultaneous CNN structure, too many parallel convolutional layers could lead to both higher feature sensitivity and higher complexity. CNN does not implicitly fit the fine-grained aspect based sentiment analysis, but using the right network configuration we can adjust it to be more sensitive to the corpus’ features and to increase the overall performance. Even if the results of our experiments are modest compared with the state-of-art sentiment analysis models for IMDB and SemEval-2017 datasets, we prove that the HTCNN is a good substitute for CNN in the NLP field and the replacement of the CNN with the HTCNN in more laborious architectures could lead to higher rates of performance than CNN.

As our future work, it would be interesting to see how word networks based on word embeddings improve the clustering solution incorporated in the HTCNN, perhaps in an approach similar to the one proposed in BIBREF34. Moreover, a promising direction would be to have meaningful clusters that denote specific functions (e.g. aspect) or other roles (e.g. syntactic). This would give a boost to the transparency of deep models in natural language processing, since the CNN filters (and consequently, the weights) would be directly interpretable.
