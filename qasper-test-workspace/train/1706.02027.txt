# Question Answering and Question Generation as Dual Tasks

**Paper ID:** 1706.02027

## Abstract

We study the problem of joint question answering (QA) and question generation (QG) in this paper. Our intuition is that QA and QG have intrinsic connections and these two tasks could improve each other. On one side, the QA model judges whether the generated question of a QG model is relevant to the answer. On the other side, the QG model provides the probability of generating a question given the answer, which is a useful evidence that in turn facilitates QA. In this paper we regard QA and QG as dual tasks. We propose a training framework that trains the models of QA and QG simultaneously, and explicitly leverages their probabilistic correlation to guide the training process of both models. We implement a QG model based on sequence-to-sequence learning, and a QA model based on recurrent neural network. As all the components of the QA and QG models are differentiable, all the parameters involved in these two models could be conventionally learned with back propagation. We conduct experiments on three datasets. Empirical results show that our training framework improves both QA and QG tasks. The improved QA model performs comparably with strong baseline approaches on all three datasets.

## Introduction

Question answering (QA) and question generation (QG) are two fundamental tasks in natural language processing BIBREF0 , BIBREF1 . Both tasks involve reasoning between a question sequence $q$ and an answer sentence $a$ . In this work, we take answer sentence selection BIBREF2 as the QA task, which is a fundamental QA task and is very important for many applications such as search engine and conversational bots. The task of QA takes a question sentence $q$ and a list of candidate answer sentences as the input, and finds the top relevant answer sentence from the candidate list. The task of QG takes a sentence $a$ as input, and generates a question sentence $q$ which could be answered by $a$ .

It is obvious that the input and the output of these two tasks are (almost) reverse, which is referred to as “duality” in this paper. This duality connects QA and QG, and potentially could help these two tasks to improve each other. Intuitively, QA could improve QG through measuring the relevance between the generated question and the answer. This QA-specific signal could enhance the QG model to generate not only literally similar question string, but also the questions that could be answered by the answer. In turn, QG could improve QA by providing additional signal which stands for the probability of generating a question given the answer.

Moreover, QA and QG have probabilistic correlation as both tasks relate to the joint probability between $q$ and $a$ . Given a question-answer pair $\langle q, a \rangle $ , the joint probability $P(q, a)$ can be computed in two equivalent ways. 

$$P(q, a) = P(a) P(q|a) = P(q)P(a|q)$$   (Eq. 1) 

The conditional distribution $P(q|a)$ is exactly the QG model, and the conditional distribution $P(a|q)$ is closely related to the QA model. Existing studies typically learn the QA model and the QG model separately by minimizing their own loss functions, while ignoring the probabilistic correlation between them.

Based on these considerations, we introduce a training framework that exploits the duality of QA and QG to improve both tasks. There might be different ways of exploiting the duality of QA and QG. In this work, we leverage the probabilistic correlation between QA and QG as the regularization term to influence the training process of both tasks. Specifically, the training objective of our framework is to jointly learn the QA model parameterized by $\theta _{qa}$ and the QG model parameterized by $\theta _{qg}$ by minimizing their loss functions subject to the following constraint. 

$$P_a(a) P(q|a;\theta _{qg}) = P_q(q)P(a|q;\theta _{qa})$$   (Eq. 3) 

 $P_a(a)$ and $P_q(q)$ are the language models for answer sentences and question sentences, respectively.

We examine the effectiveness of our training criterion by applying it to strong neural network based QA and QG models. Specifically, we implement a generative QG model based on sequence-sequence learning, which takes an answer sentence as input and generates a question sentence in an end-to-end fashion. We implement a discriminative QA model based on recurrent neural network, where both question and answer are represented as continuous vector in a sequential way. As every component in the entire framework is differentiable, all the parameters could be conventionally trained through back propagation. We conduct experiments on three datasets BIBREF2 , BIBREF3 , BIBREF4 . Empirical results show that our training framework improves both QA and QG tasks. The improved QA model performs comparably with strong baseline approaches on all three datasets.

## The Proposed Framework

In this section, we first formulate the task of QA and QG, and then present the proposed algorithm for jointly training the QA and QG models. We also describe the connections and differences between this work and existing studies.

## Task Definition and Notations

This work involves two tasks, namely question answering (QA) and question generation (QG). There are different kinds of QA tasks in natural language processing community. In this work, we take answer sentence selection BIBREF2 as the QA task, which takes a question $q$ and a list of candidate answer sentences $A = \lbrace a_1, a_2, ... , a_{|A|}\rbrace $ as input, and outputs one answer sentence $a_i$ from the candidate list which has the largest probability to be the answer. This QA task is typically viewed as a ranking problem. Our QA model is abbreviated as $f_{qa}(a,q;\theta _{qa})$ , which is parameterized by $\theta _{qa}$ and the output is a real-valued scalar.

The task of QG takes a sentence $a$ as input, and outputs a question $q$ which could be answered by $a$ . In this work, we regard QG as a generation problem and develop a generative model based on sequence-to-sequence learning. Our QG model is abbreviated as $P_{qg}(q|a;\theta _{qg})$ , which is parameterized by $\theta _{qg}$ and the output is the probability of generating a natural language question $q$ .

## Algorithm Description

We describe the proposed algorithm in this subsection. Overall, the framework includes three components, namely a QA model, a QG model and a regularization term that reflects the duality of QA and QG. Accordingly, the training objective of our framework includes three parts, which is described in Algorithm 1.

The QA specific objective aims to minimize the loss function $l_{qa}(f_{qa}(a,q;\theta _{qa}), label)$ , where $label$ is 0 or 1 that indicates whether $a$ is the correct answer of $q$ or not. Since the goal of a QA model is to predict whether a question-answer pair is correct or not, it is necessary to use negative QA pairs whose labels are zero. The details about the QA model will be presented in the next section.

For each correct question-answer pair, the QG specific objective is to minimize the following loss function, 

$$l_{qg}(q, a) = -log P_{qg}(q|a;\theta _{qg})$$   (Eq. 6) 

where $a$ is the correct answer of $q$ . The negative QA pairs are not necessary because the goal of a QG model is to generate the correct question for an answer. The QG model will be described in the following section.

[tb] Algorithm Description Input: Language models $P_a(a)$ and $P_q(q)$ for answer and question, respectively; hyper parameters $\lambda _q$ and $\lambda _a$ ; optimizer $opt$ Output: QA model $f_{qa}(a,q)$ parameterized by $\theta _{qa}$ ; QG model $P_{qg}(q|a)$ parameterized by $\theta _{qg}$ Randomly initialize $\theta _{qa}$ and $P_q(q)$0 Get a minibatch of positive QA pairs $P_q(q)$1 , where $P_q(q)$2 is the answer of $P_q(q)$3 ; Get a minibatch of negative QA pairs $P_q(q)$4 , where $P_q(q)$5 is not the answer of $P_q(q)$6 ;

Calculate the gradients for $\theta _{qa}$ and $\theta _{qg}$ .

$$\nonumber G_{qa} = \triangledown _{\theta _{qa}} &\frac{1}{m}\sum _{i = 1}^{m}[l_{qa}(f_{qa}(a^p_i,q^p_i;\theta _{qa}), 1) \\
&\nonumber + l_{qa}(f_{qa}(a^n_i,q^n_i;\theta _{qa}),0) \\
& +\lambda _al_{dual}(a^p_i,q^p_i;\theta _{qa}, \theta _{qg})]$$   (Eq. 7) 

$$\nonumber G_{qg} = \triangledown _{\theta _{qg}} &\frac{1}{m}\sum _{i = 1}^{m}[\ l_{qg}(q^p_i,a^p_i) \\& + \lambda _ql_{dual}(q^p_i,a^p_i;\theta _{qa}, \theta _{qg})]$$   (Eq. 8) 

 Update $\theta _{qa}$ and $\theta _{qg}$ $\theta _{qa} \leftarrow opt(\theta _{qa}, G_{qa})$ , $\theta _{qg} \leftarrow opt(\theta _{qg}, G_{qg})$ models converged

The third objective is the regularization term which satisfies the probabilistic duality constrains as given in Equation 3 . Specifically, given a correct $\langle q, a \rangle $ pair, we would like to minimize the following loss function, 

$$ \nonumber l_{dual}(a,q;\theta _{qa}, \theta _{qg}) &= [logP_a(a) + log P(q|a;\theta _{qg}) \\
& - logP_q(q) - logP(a|q;\theta _{qa})]^2$$   (Eq. 9) 

 where $P_a(a)$ and $P_q(q)$ are marginal distributions, which could be easily obtained through language model. $P(a|q;\theta _{qg})$ could also be easily calculated with the markov chain rule: $P(q|a;\theta _{qg}) = \prod _{t=1}^{|q|} P(q_t|q_{<t}, a;\theta _{qg})$ , where the function $P(q_t|q_{<t}, a;\theta _{qg})$ is the same with the decoder of the QG model (detailed in the following section).

However, the conditional probability $P(a|q;\theta _{qa})$ is different from the output of the QA model $f_{qa}(a,q;\theta _{qa})$ . To address this, given a question $q$ , we sample a set of answer sentences $A^{\prime }$ , and derive the conditional probability $P(a|q;\theta _{qa})$ based on our QA model with the following equation. 

$$\nonumber &P(a|q;\theta _{qa}) = \\
&\dfrac{exp(f_{qa}(a,q;\theta _{qa}))}{exp(f_{qa}(a,q;\theta _{qa})) + \sum _{a^{\prime } \in A^{\prime }} exp(f_{qa}(a^{\prime },q;\theta _{qa}))}$$   (Eq. 10) 

In this way, we learn the models of QA and QG by minimizing the weighted combination between the original loss functions and the regularization term.

## Relationships with Existing Studies

Our work differs from BIBREF5 in that they regard reading comprehension (RC) as the main task, and regard question generation as the auxiliary task to boost the main task RC. In our work, the roles of QA and QG are the same, and our algorithm enables QA and QG to improve the performance of each other simultaneously. Our approach differs from Generative Domain-Adaptive Nets BIBREF5 in that we do not pretrain the QA model. Our QA and QG models are jointly learned from random initialization. Moreover, our QA task differs from RC in that the answer in our task is a sentence rather than a text span from a sentence.

Our approach is inspired by dual learning BIBREF6 , BIBREF7 , which leverages the duality between two tasks to improve each other. Different from the dual learning BIBREF6 paradigm, our framework learns both models from scratch and does not need task-specific pretraining. The recently introduced supervised dual learning BIBREF7 has been successfully applied to image recognition, machine translation and sentiment analysis. Our work could be viewed as the first work that leveraging the idea of supervised dual learning for question answering. Our approach differs from Generative Adversarial Nets (GAN) BIBREF8 in two respects. On one hand, the goal of original GAN is to learn a powerful generator, while the discriminative task is regarded as the auxiliary task. The roles of the two tasks in our framework are the same. On the other hand, the discriminative task of GAN aims to distinguish between the real data and the artificially generated data, while we focus on the real QA task.

## The Question Answering Model

We describe the details of the question answer (QA) model in this section. Overall, a QA model could be formulated as a function $f_{qa}(q, a;\theta _{qa})$ parameterized by $\theta _{qa}$ that maps a question-answer pair to a scalar. In the inference process, given a $q$ and a list of candidate answer sentences, $f_{qa}(q, a;\theta _{qa})$ is used to calculate the relevance between $q$ and every candidate $a$ . The top ranked answer sentence is regarded as the output.

We develop a neural network based QA model. Specifically, we first represent each word as a low dimensional and real-valued vector, also known as word embedding BIBREF9 , BIBREF10 , BIBREF11 . Afterwards, we use recurrent neural network (RNN) to map a question of variable length to a fixed-length vector. To avoid the problem of gradient vanishing, we use gated recurrent unit (GRU) BIBREF12 as the basic computation unit. The approach recursively calculates the hidden vector $h_{t}$ based on the current word vector $e^q_t$ and the output vector $h_{t-1}$ in the last time step, 

$$&z_i = \sigma (W_{z}e^q_{i} + U_{z}{h}_{i-1}) \\
&r_i = \sigma (W_{r}e^q_{i} + U_{r}{h}_{i-1}) \\
&\widetilde{h}_i = \tanh (W_{h}e^q_{i} + U_{h}(r_i \odot {h}_{i-1})) \\
&{h}_{i} = z_i \odot \widetilde{h}_i + (1-z_i) \odot {h}_{i-1}$$   (Eq. 12) 

 where $z_i$ and $r_i$ are update and reset gates of s, $\odot $ stands for element-wise multiplication, $\sigma $ is sigmoid function. We use a bi-directional RNN to get the meaning of a question from both directions, and use the concatenation of two last hidden states as the final question vector $v_q$ . We compute the answer sentence vector $v_a$ in the same way.

After obtaining $v_q$ and $v_a$ , we implement a simple yet effective way to calculate the relevance between question-sentence pair. Specifically, we represent a question-answer pair as the concatenation of four vectors, namely $v(q, a) = [v_q; v_a; v_q \odot v_a ; e_{c(q,a)}]$ , where $\odot $ means element-wise multiplication, $c(q,a)$ is the number of co-occurred words in $q$ and $a$ . We observe that incorporating the embedding of the word co-occurrence $e^c_{c(q,a)}$ could empirically improve the QA performance. We use an additional embedding matrix $L_c \in \mathbb {R}^{d_c \times |V_c|}$ , where $d_c$ is the dimension of word co-occurrence vector and $v_a$0 is vocabulary size. The values of $v_a$1 are jointly learned during training. The output scalar $v_a$2 is calculated by feeding $v_a$3 to a linear layer followed by $v_a$4 . We feed $v_a$5 to a $v_a$6 layer and use negative log-likelihood as the QA specific loss function. The basic idea of this objective is to classify whether a given question-answer is correct or not. We also implemented a ranking based loss function $v_a$7 , whose basic idea is to assign the correct QA pair a higher score than a randomly select QA pair. However, our empirical results showed that the ranking loss performed worse than the negative log-likelihood loss function. We use log-likelihood as the QA loss function in the experiment.

## The Question Generation Model

We describe the question generation (QG) model in this section. The model is inspired by the recent success of sequence-to-sequence learning in neural machine translation. Specifically, the QG model first calculates the representation of the answer sentence with an encoder, and then takes the answer vector to generate a question in a sequential way with a decoder. We will present the details of the encoder and the decoder, respectively.

The goal of the encoder is to represent a variable-length answer sentence ${a}$ as a fixed-length continuous vector. The encoder could be implemented with different neural network architectures such as convolutional neural network BIBREF13 , BIBREF14 and recurrent neural network (RNN) BIBREF15 , BIBREF16 . In this work, we use bidirectional RNN based on GRU unit, which is consistent with our QA model as described in Section 3. The concatenation of the last hidden vectors from both directions is used as the output of the encoder, which is also used as the initial hidden state of the decoder.

The decoder takes the output of the encoder and generates the question sentence. We implement a RNN based decoder, which works in a sequential way and generates one question word at each time step. The decoder generates a word $q_{t}$ at each time step $t$ based on the representation of $a$ and the previously predicted question words $q_{<t}=\lbrace q_1,q_2,...,q_{t-1}\rbrace $ . This process is formulated as follows. 

$$p(q|a)=\prod ^{|q|}_{t=1}p(q_{t}|q_{<t},a)$$   (Eq. 14) 

Specifically, we use an attention-based architecture BIBREF17 , which selectively finds relevant information from the answer sentence when generating the question word. Therefore, the conditional probability is calculated as follows. 

$$p(q_{t}|q_{<t},a)=f_{dec}(q_{t-1},s_{t}, c_t)$$   (Eq. 15) 

where $s_{t}$ is the hidden state of GRU based RNN at time step $t$ , and $c_t$ is the attention state at time step $t$ . The attention mechanism assigns a probability/weight to each hidden state in the encoder at one time step, and calculates the attention state $c_t$ through weighted averaging the hidden states of the encoder: $c_{t}=\sum ^{|a|}_{i=1}\alpha _{\langle t,i\rangle }h_i$ . When calculating the attention weight of $h_i$ at time step $t$ , we also take into account of the attention distribution in the last time step. Potentially, the model could remember which contexts from answer sentence have been used before, and does not repeatedly use these words to generate the question words. 

$$\alpha _{\langle t,i\rangle }=\frac{\exp {[z(s_{t},h_i,\sum ^{N}_{j=1}\alpha _{\langle t-1,j\rangle }h_j)]}}{\sum ^{H}_{i^{\prime }=1}\exp {[z(s_{t},h_{i^{\prime }},\sum ^{N}_{j=1}\alpha _{\langle t-1,j\rangle }h_{j})]}}$$   (Eq. 16) 

Afterwards, we feed the concatenation of $s_t$ and $c_t$ to a linear layer followed by a $softmax$ function. The output dimension of the $softmax$ layer is equal to the number of top frequent question words (e.g. 30K or 50K) in the training data. The output values of the $softmax$ layer form the probability distribution of the question words to be generated. Furthermore, we observe that question sentences typically include informative but low-frequency words such as named entities or numbers. These low-frequency words are closely related to the answer sentence but could not be well covered in the target vocabulary. To address this, we add a simple yet effective post-processing step which replaces each “unknown word” with the most relevant word from the answer sentence. Following BIBREF18 , we use the attention probability as the relevance score of each word from the answer sentence. Copying mechanism BIBREF19 , BIBREF20 is an alternative solution that adaptively determines whether the generated word comes from the target vocabulary or from the answer sentence.

Since every component of the QG model is differentiable, all the parameters could be learned in an end-to-end way with back propagation. Given a question-answer pair $\langle q,a\rangle $ , where $a$ is the correct answer of the question $q$ , the training objective is to minimize the following negative log-likelihood. 

$$l_{qg}(q,a)=-\sum ^{|q|}_{t=1}\log [p(y_t|y_{<t},a)]$$   (Eq. 17) 

In the inference process, we use beam search to get the top- $K$ confident results, where $K$ is the beam size. The inference process stops when the model generates the symbol $\langle eos \rangle $ which stands for the end of sentence.

## Experiment

We describe the experimental setting and report empirical results in this section.

## Experimental Setting

We conduct experiments on three datasets, including MARCO BIBREF4 , SQUAD BIBREF3 , and WikiQA BIBREF2 .

The MARCO and SQUAD datasets are originally developed for the reading comprehension (RC) task, the goal of which is to answer a question with a text span from a document. Despite our QA task (answer sentence selection) is different from RC, we use these two datasets because of two reasons. The first reason is that to our knowledge they are the QA datasets that contains largest manually labeled question-answer pairs. The second reason is that, we could derive two QA datasets for answer sentence selection from the original MARCO and SQUAD datasets, with an assumption that the answer sentences containing the correct answer span are correct, and vice versa. We believe that our training framework could be easily applied to RC task, but we that is out of the focus of this work.

We also conduct experiments on WikiQA BIBREF2 , which is a benchmark dataset for answer sentence selection. Despite its data size is relatively smaller compared with MARCO and SQUAD, we still apply our algorithm on this data and report empirical results to further compare with existing algorithms.

It is worth to note that a common characteristic of MARCO and SQUAD is that the ground truth of the test is invisible to the public. Therefore, we randomly split the original validation set into the dev set and the test set. The statistics of SQUAD and MARCO datasets are given in Table 1 . We use the official split of the WikiQA dataset. We apply exactly the same model to these three datasets.

We evaluate our QA system with three standard evaluation metrics: Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision@1 (P@1) BIBREF23 . It is hard to find a perfect way to automatically evaluate the performance of a QG system. In this work, we use BLEU-4 BIBREF24 score as the evaluation metric, which measures the overlap between the generated question and the ground truth.

## Implementation Details

We train the parameters of the QA model and the QG model simultaneously. We randomly initialize the parameters in both models with a combination of the fan-in and fan-out BIBREF25 . The parameters of word embedding matrices are shared in the QA model and the QG model. In order to learn question and answer specific word meanings, we use two different embedding matrices for question words and answer words. The vocabularies are the most frequent 30K words from the questions and answers in the training data. We set the dimension of word embedding as 300, the hidden length of encoder and decoder in the QG model as 512, the hidden length of GRU in the QA model as 100, the dimension of word co-occurrence embedding as 10, the vocabulary size of the word co-occurrence embedding as 10, the hidden length of the attention layer as 30. We initialize the learning rate as 2.0, and use AdaDelta BIBREF26 to adaptively decrease the learning rate. We use mini-batch training, and empirically set the batch size as 64. The sampled answer sentences do not come from the same passage. We get 10 batches (640 instances) and sort them by answer length for accelerating the training process. The negative samples come from these 640 instances, which are from different passages.

In this work, we use smoothed bigram language models as $p_a(a)$ and $p_q(q)$ . We also tried trigram language model but did not get improved performance. Alternatively, one could also implement neural language model and jointly learn the parameters in the training process.

## Results and Analysis

We first report results on the MARCO and SQUAD datasets. As the dataset is splitted by ourselves, we do not have previously reported results for comparison. We compare with the following four baseline methods. It has been proven that word co-occurrence is a very simple yet effective feature for this task BIBREF2 , BIBREF22 , so the first two baselines are based on the word co-occurrence between a question sentence and the candidate answer sentence. WordCnt and WgtWordCnt use unnormalized and normalized word co-occurrence. The ranker in these two baselines are trained with with FastTree, which performs better than SVMRank and linear regression in our experiments. We also compare with CDSSM BIBREF21 , which is a very strong neural network approach to model the semantic relatedness of a sentence pair. We further compare with ABCNN BIBREF22 , which has been proven very powerful in various sentence matching tasks. Basic QA is our QA model which does not use the duality between QA and QG. Our ultimate model is abbreviated as Dual QA.

The QA performance on MARCO and SQUAD datasets are given in Table 2 . We can find that CDSSM performs better than the word co-occurrence based method on MARCO dataset. On the SQUAD dataset, Dual QA achieves the best performance among all these methods. On the MARCO dataset, Dual QA performs comparably with ABCNN.

We can find that Dual QA still yields better accuracy than Basic QA, which shows the effectiveness of the joint training algorithm. It is interesting that word co-occurrence based method (WgtWordCnt) is very strong and hard to beat on the MARCO dataset. Incorporating sophisticated features might obtain improved performance on both datasets, however, this is not the focus of this work and we leave it to future work.

Results on the WikiQA dataset is given in Table 3 . On this dataset, previous studies typically report results based on their deep features plus the number of words that occur both in the question and in the answer BIBREF2 , BIBREF22 . We also follow this experimental protocol. We can find that our basic QA model is simple yet effective. The Dual QA model achieves comparably to strong baseline methods.

To give a quantitative evaluation of our training framework on the QG model, we report BLEU-4 scores on MARCO and SQUAD datasets. The results of our QG model with or without using joint training are given in Table 5 . We can find that, despite the overall BLEU-4 scores are relatively low, using our training algorithm could improve the performance of the QG model.

We would like to investigate how the joint training process improves the QA and QG models. To this end, we analyze the results of development set on the SQUAD dataset. We randomly sample several cases that the Basic QA model gets the wrong answers while the Dual QA model obtains the correct results. Examples are given in Table 4 . From these examples, we can find that the questions generated by Dual QG tend to have more word overlap with the correct question, despite sometimes the point of the question is not correct. For example, compared with the Basic QG model, the Dual QG model generates more informative words, such as “green” in the first example, “purpose” in the second example, and “how much” in the third example. We believe this helps QA because the QA model is trained to assign a higher score to the question which looks similar with the generated question. It also helps QG because the QA model is trained to give a higher score to the real question-answer pair, so that generating more answer-alike words gives the generated question a higher QA score.

Despite the proposed training framework obtains some improvements on QA and QG, we believe the work could be further improved from several directions. We find that our QG model not always finds the point of the reference question. This is not surprising because the questions from these two reading comprehension datasets only focus on some spans of a sentence, rather than the entire sentence. Therefore, the source side (answer sentence) carries more information than the target side (question sentence). Moreover, we do not use the answer position information in our QG model. Accordingly, the model may pay attention to the point which is different from the annotator's direction, and generates totally different questions. We are aware of incorporating the position of the answer span could get improved performance BIBREF29 , however, the focus of this work is a sentence level QA task rather than reading comprehension. Therefore, despite MARCO and SQUAD are of large scale, they are not the desirable datasets for investigating the duality of our QA and QG tasks. Pushing forward this area also requires large scale sentence level QA datasets.

## Discussion

We would like to discuss our understanding about the duality of QA and QG, and also present our observations based on the experiments.

In this work, “duality” means that the QA task and the QG task are equally important. This characteristic makes our work different from Generative Domain-Adaptive Nets BIBREF5 and Generative Adversarial Nets (GAN) BIBREF8 , both of which have a main task and regard another task as the auxiliary one. There are different ways to leverage the “duality” of QA and QG to improve both tasks. We categorize them into two groups. The first group is about the training process and the second group is about the inference process. From this perspective, dual learning BIBREF6 is a solution that leverages the duality in the training process. In particular, dual learning first pretrains the models for two tasks separately, and then iteratively fine-tunes the models. Our work also belongs to the first group. Our approach uses the duality as a regularization item to guide the learning of QA and QG models simultaneously from scratch. After the QA and QG models are trained, we could also use the duality to improve the inference process, which falls into the second group. The process could be conducted on separately trained models or the models that jointly trained with our approach. This is reasonable because the QA model could directly add one feature to consider $q$ and $q^{\prime }$ , where $q^{\prime }$ is the question generated by the QG model. The first example in Table 4 also motivates this direction. Similarly, the QA model could give each $\langle q^{\prime }, a \rangle $ a score which could be assigned to each generated question $q^{\prime }$ . In this work we do not apply the duality in the inference process. We leave it as a future plan.

This work could be improved by refining every component involved in our framework. For example, we use a simple yet effective QA model, which could be improved by using more complex neural network architectures BIBREF30 , BIBREF22 or more external resources. We use a smoothed language model for both question and answer sentences, which could be replaced by designed neural language models whose parameters are jointly learned together with the parameters in QA and QG models. The QG model could be improved as well, for example, by developing more complex neural network architectures to take into account of more information about the answer sentence in the generation process.

In addition, it is also very important to investigate an automatic evaluation metric to effectively measure the performance of a QG system. BLEU score only measures the literal similarity between the generated question and the ground truth. However, it does not measure whether the question really looks like a question or not. A desirable evaluation system should also have the ability to judge whether the generated question could be answered by input sentence, even if the generated question use totally different words to express the meaning.

## Related Work

Our work relates to existing studies on question answering (QA) and question generation (QG).

There are different types of QA tasks including text-level QA BIBREF31 , knowledge based QA BIBREF32 , community based QA BIBREF33 and the reading comprehension BIBREF3 , BIBREF4 . Our work belongs to text based QA where the answer is a sentence. In recent years, neural network approaches BIBREF30 , BIBREF31 , BIBREF22 show promising ability in modeling the semantic relation between sentences and achieve strong performances on QA tasks.

Question generation also draws a lot of attentions in recent years. QG is very necessary in real application as it is always time consuming to create large-scale QA datasets. In literature, BIBREF34 use Minimal Recursion Semantics (MRS) to represent the meaning of a sentence, and then realize the MSR structure into a natural language question. BIBREF35 present a overgenerate-and-rank framework consisting of three stages. They first transform a sentence into a simpler declarative statement, and then transform the statement to candidate questions by executing well-defined syntactic transformations. Finally, a ranker is used to select the questions of high-quality. BIBREF36 focus on generating questions from a topic. They first get a list of texts related to the topic, and then generate questions by exploiting the named entity information and the predicate argument structures of the texts. BIBREF37 propose an ontology-crowd-relevance approach to generate questions from novel text. They encode the original text in a low-dimensional ontology, and then align the question templates obtained via crowd-sourcing to that space. A final ranker is used to select the top relevant templates. There also exists some studies on generating questions from knowledge base BIBREF38 , BIBREF39 . For example, BIBREF39 develop a neural network approach which takes a knowledge fact (including a subject, an object, and a predicate) as input, and generates the question with a recurrent neural network. Recent studies also investigate question generation for the reading comprehension task BIBREF40 , BIBREF29 . The approaches are typically based on the encoder-decoder framework, which could be conventionally learned in an end-to-end way. As the answer is a text span from the sentence/passage, it is helpful to incorporate the position of the answer span BIBREF29 . In addition, the computer vision community also pays attention to generating natural language questions about an image BIBREF41 .

## Conclusion

We focus on jointly training the question answering (QA) model and the question generation (QG) model in this paper. We exploit the “duality” of QA and QG tasks, and introduce a training framework to leverage the probabilistic correlation between the two tasks. In our approach, the “duality” is used as a regularization term to influence the learning of QA and QG models. We implement simple yet effective QA and QG models, both of which are neural network based approaches. Experimental results show that the proposed training framework improves both QA and QG on three datasets.
