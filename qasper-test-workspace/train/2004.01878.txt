# News-Driven Stock Prediction With Attention-Based Noisy Recurrent State Transition

**Paper ID:** 2004.01878

## Abstract

We consider direct modeling of underlying stock value movement sequences over time in the news-driven stock movement prediction. A recurrent state transition model is constructed, which better captures a gradual process of stock movement continuously by modeling the correlation between past and future price movements. By separating the effects of news and noise, a noisy random factor is also explicitly fitted based on the recurrent states. Results show that the proposed model outperforms strong baselines. Thanks to the use of attention over news events, our model is also more explainable. To our knowledge, we are the first to explicitly model both events and noise over a fundamental stock value state for news-driven stock movement prediction.

## Introduction

Stock movement prediction is a central task in computational and quantitative finance. With recent advances in deep learning and natural language processing technology, event-driven stock prediction has received increasing research attention BIBREF0, BIBREF1. The goal is to predict the movement of stock prices according to financial news. Existing work has investigated news representation using bag-of-words BIBREF2, named entities BIBREF3, event structures BIBREF4 or deep learning BIBREF1, BIBREF5.

Most previous work focuses on enhancing news representations, while adopting a relatively simple model on the stock movement process, casting it as a simple response to a set of historical news. The prediction model can therefore be viewed as variations of a classifier that takes news as input and yields stock movement predictions. In contrast, work on time-series based stock prediction BIBREF6, BIBREF7, BIBREF5, BIBREF8, aims to capture continuous movements of prices themselves.

We aim to introduce underlying price movement trends into news-driven stock movement prediction by casting the underlaying stock value as a recurrent state, integrating the influence of news events and random noise simultaneously into the recurrent state transitions. In particular, we take a LSTM with peephole connections BIBREF9 for modeling a stock value state over time, which can reflect the fundamentals of a stock. The influence of news over a time window is captured in each recurrent state transition by using neural attention to aggregate representations of individual news. In addition, all other factors to the stock price are modeled using a random factor component, so that sentiments, expectations and noise can be dealt with explicitly.

Compared with existing work, our method has three salient advantages. First, the process in which the influence of news events are absorbed into stock price changes is explicitly modeled. Though previous work has attempted towards this goal BIBREF1, existing models predict each stock movement independently, only modeling the correlation between news in historical news sequences. As shown in Figure FIGREF1, our method can better capture a continuous process of stock movement by modeling the correlation between past and future stock values directly. In addition, non-linear compositional effects of multiple events in a time window can be captured.

Second, to our knowledge, our method allows noise to be explicitly addressed in a model, therefore separating the effects of news and other factors. In contrast, existing work trains a stock prediction model by fitting stock movements to events, and therefore can suffer from overfitting due to external factors and noise.

Third, our model is also more explainable thanks to the use of attention over news events, which is similar to the work of BIBREF10 and BIBREF11. Due to the use of recurrent states, we can visualize past events over a large time window. In addition, we propose a novel future event prediction module to factor in likely next events according to natural events consequences. The future event module is trained over gold “future” data over historical events. Therefore, it can also deal with insider trading factors to some extent.

Experiments over the benchmark of BIBREF1 show that our method outperforms strong baselines, giving the best reported results in the literature. To our knowledge, we are the first to explicitly model both events and noise over a fundamental stock value state for news-driven stock movement prediction. Note that unlike time-series stock prediction models BIBREF12, BIBREF5, we do not take explicit historical prices as part of model inputs, and therefore our research still focuses on the influence of news information alone, and are directly comparable to existing work on news-driven stock prediction.

## Related Work

There has been a line of work predicting stock markets using text information from daily news. We compare this paper with previous work from the following two perspectives.

Modeling Price Movements Correlation

Most existing work treats the modeling of each stock movement independently using bag-of-words BIBREF2, named entities BIBREF3, semantic frames BIBREF0, event structures BIBREF4, event embeddings BIBREF1 or knowledge bases BIBREF13. Differently, we study modeling the correlation between past and future stock value movements.

There are also some work modeling the correlations between samples by sparse matrix factorization BIBREF14, hidden Markov model BIBREF8 and Bi-RNNs BIBREF5, BIBREF11 using both news and historical price data. Some work models the correlations among different stocks by pre-defined correlation graph BIBREF15 and tensor factorization BIBREF12. Our work is different from this line of work in that we use only news events as inputs, and our recurrent states are combined with impact-related noises.

Explainable Prediction

Rationalization is an important problem for news-driven stock price movement prediction, which is to find the most important news event along with the model's prediction. Factorization, such as sparse matrix factorization BIBREF14 and tensor factorization BIBREF12, is a popular method where results can be traced back upon the input features. While this type of method are limited because of the dimension of input feature, our attention-based module has linear time complexity on feature size.

BIBREF11 apply dual-layer attention to predict the stock movement by using news published in the previous six days. Each day's news embeddings and seven days' embeddings are summed by the layer. Our work is different from BIBREF11 in that our news events attention is query-based, which is more strongly related to the noisy recurrent states. In contrast, their attention is not query-based and tends to output the same result for each day even if the previous day's decision is changed.

## Task Definition

Following previous work BIBREF4, BIBREF1, the task is formalized as a binary classification task for each trading day. Formally, given a history news set about a targeted stock or index, the input of the task is a trading day $x$ and the output is a label $y \in \lbrace +1, -1\rbrace $ indicating whether the adjusted closing price $p_x$ will be greater than $p_{x-1}$ ($y=+1$) or not ($y=-1$).

## Method

The framework of our model is shown in Figure FIGREF2. We explicitly model both events and noise over a recurrent stock value state, which is modeled using LSTM. For each trading day, we consider the news events happened in that day as well as the past news events using neural attention BIBREF16. Considering the impacts of insider trading, we also involve future news in the training procedure. To model the high stochasticity of stock market, we sample an additive noise using a neural module. Our model is named attention-based noisy recurrent states transition (ANRES).

Considering the general principle of sample independence, building temporal connections between individual trading days in training set is not suitable for training BIBREF5 and we find it easy to overfit. We notice that a LSTM usually takes several steps to generate a more stable hidden state. As an alternative method, we extended the time span of one sample to $T$ previous continuous trading days (${t-T+1, t-T+2, ..., t-1, t}$), which we call a trading sequence, is used as the basic training element in this paper.

## Method ::: LSTM-based Recurrent State Transition

ANRES uses LSTM with peephole connections BIBREF9. The underlying stock value trends are represented as a recurrent state $z$ transited over time, which can reflect the fundamentals of a stock. In each trading day, we consider the impact of corresponding news events and a random noise as:

where $v_t$ is the news events impact vector on the trading day $t$ and $f$ is a function in which random noise will be integrated.

By using this basic framework, the non-linear compositional effects of multiple events can also be captured in a time window. Then we use the sequential state $z_t$ to make binary classification as:

where $\hat{p}_t$ is the estimated probabilities, $\hat{y}_t$ is the predicted label and $x_t$ is the input trading day.

## Method ::: Modeling News Events

For a trading day $t$ in a trading sequence, we model both long-term and short-term impact of news events. For short-term impact, we use the news published after the previous trading day $t-1$ and before the trading day $t$ as the present news set. Similarly, for long-term impact, we use the news published no more than thirty calendar days ago as the past news set.

For each news event, we extract its headline and use ELMo BIBREF17 to transform it to $V$-dim hidden state by concatenating the output bidirectional hidden states of the last words as the basic representation of a news event. By stacking those vectors accordingly, we obtain two embedding matrices $C^{\prime }_t$ and $B^{\prime }_t$ for the present and past news events as:

where ${hc}^i_t$ is one of the news event headline in the present news set, ${ec}^i_t$ is the headline representation of ${hc}^i_t$, $L_c$ is the size of present news set; while ${hb}^j_t$, ${eb}^j_t$ and $L_b$ are for the past news set.

To make the model more numerically stable and avoiding overfitting, we apply the over-parameterized component of BIBREF18 to the news events embedding matrices, where

$\odot $ is element-wise multiplication and $\sigma (\cdot )$ is the sigmoid function.

Due to the unequal importance news events contribute to the stock price movement in $t$, we use scaled dot-product attention BIBREF16 to capture the influence of news over a period for the recurrent state transition. In practical, we first transform the last trading day's stock value $z_{t-1}$ to a query vector $q_t$, and then calculate two attention score vectors $\gamma _t$ and $\beta _t$ for the present and past news events as:

We sum the news events embedding matrices to obtain news events impact vectors $c_t$ and $b_t$ on the trading day $t$ according to the weights $\gamma _t$ and $\beta _t$, respectively:

## Method ::: Modeling Future News

In spite of the long-term and short-term impact, we find that some short-term future news events will exert an influence on the stock price movement before the news release, which can be attributed to news delay or insider trading BIBREF19 factors to some extent.

We propose a novel future event prediction module to consider likely next events according to natural consequences. In this paper, we define future news events as those that are published within seven calendar days after the trading day $t$.

Similarly to the past and present news events, we stack the headline ELMo embeddings of future news events to an embedding matrix $A^{\prime }_t$. Then adapting the over-parameterized component and summing the stacked embedding vectors by scaled dot-product attention. We calculate the future news events impact vector $a_t$ on the trading day $t$ as:

Although the above steps can work in the training procedure, where the future event module is trained over gold “future” data over historical events, at test time, future news events are not accessible. To address this issue, we use a non-linear transformation to estimate a future news events impact vector $\hat{a}_t$ with the past and present news events impact vectors $b_t$ and $c_t$ as:

where $[,]$ is the vector concatenation operation.

We concatenate the above-mentioned three types of news events impact vectors to obtain the input $v_t$ for LSTM-based recurrent state transition on trading day $t$ as:

where $[,]$ is the vector concatenation operation.

## Method ::: Modeling Noise

In this model, all other factors to the stock price such as sentiments, expectations and noise are explicitly modeled as noise using a random factor. We sample a random factor from a normal distribution $\mathcal {N}(\textbf {0}, \sigma _t)$ parameterized by $z^{\prime }_t$ as:

However, in practice, the model can face difficulty of back propagating gradients if we directly sample a random factor from $\mathcal {N}(\textbf {0}, \sigma _t)$. We use re-parameterization BIBREF20 for normal distributions to address the problem and enhance the transition result $z^{\prime }_t$ with sample random factor to obtain the noisy recurrent state $z_t$ as:

## Method ::: Training Objective

For training, there are two main terms in our loss function. The first term is a cross entropy loss for the predicted probabilities $\hat{p}_t$ and gold labels $y_t$, and the second term is the mean squared error between the estimated future impact vector $\hat{a}_t$ and the true future impact vector $a_t$.

The total loss for a trading sequence containing $T$ trading days with standard $L_2$ regularization is calculated as:

where $\theta $ is a hyper-parameter which indicates how much important $L_{mse}$ is comparing to $L_{ce}$, $\Phi $ is the set of trainable parameters in the entire ANRES model and $\lambda $ is the regularization weight.

## Experiments

We use the public financial news dataset released by BIBREF4, which is crawled from Reuters and Bloomberg over the period from October 2006 to November 2013. We conduct our experiments on predicting the Standard & Poor’s 500 stock (S&P 500) index and its selected individual stocks, obtaining indices and prices from Yahoo Finance. Detailed statistics of the training, development and test sets are shown in Table TABREF8. We report the final results on test set after using development set to tune some hyper-parameters.

## Experiments ::: Settings

The hyper-parameters of our ANRES model are shown in Table TABREF11. We use mini-batches and stochastic gradient descent (SGD) with momentum to update the parameters. Most of the hyper-parameters are chosen according to development experiments, while others like dropout rate $r$ and SGD momentum $\mu $ are set according to common values.

Following previous work BIBREF0, BIBREF4, BIBREF5, we adopt the standard measure of accuracy and Matthews Correlation Coefficient (MCC) to evaluate S&P 500 index prediction and selected individual stock prediction. MCC is applied because it avoids bias due to data skew. Given the confusion matrix which contains true positive, false positive, true negative and false negative values, MCC is calculated as:

## Experiments ::: Initializing Noisy Recurrent States

As the first set of development experiments, we try different ways to initialize the noisy recurrent states of our ANRES model to find a suitable approach. For each trading day, we compare the results whether states transitions are modeled or not. Besides, we also compare the methods of random initialization and zero initialization. Note that the random initialization method we use here returns a tensor filled with random numbers from the standard normal distribution $\mathcal {N}(0, 1)$. In summary, the following four baselines are designed:

ANRES_Sing_R: randomly initializing the states for each single trading day.

ANRES_Sing_Z: initializing the states as zeros for each single trading day.

ANRES_Seq_R: randomly initializing the first states for each trading sequence only.

ANRES_Seq_Z: initializing the first states as zeros for each trading sequence only.

Development set results on predicting S&P 500 index are shown in Table TABREF13. We can see that modeling recurrent value sequences performs better than treating each trading day separately, which shows that modeling trading sequences can capture the correlations between trading days and the non-linear compositional effects of multiple events. From another perspective, the models ANRES_Sing_R and ANRES_Sing_Z also represent the strengths of our basic representations of news events in isolation. Therefore, we can also see that using only the basic news events representations is not sufficient for index prediction, while combining with our states transition module can achieve strong results.

By comparing the results of ANRES_Seq_R and ANRES_Seq_Z, we decide to use zero initialization for our ANRES models, including the noisy recurrent states also in the remaining experiments.

## Experiments ::: Study on Trading Sequence Length

We use the development set to find a suitable length $T$ for trading sequence, which is searched from $\lbrace 1, 3, 5, 7, 9, 11, 13, 15\rbrace $. The S&P 500 index prediction results of accuracy, MCC and consumed minutes per training epoch on the development set are shown in Figure FIGREF19.

We can see that the accuracy and MCC are positively correlated with the growth of $T$, while the change of accuracy is smaller than MCC. When $T \ge 7$, the growth of MCC becomes slower than that when $T < 7$. Also considering the running time per training epoch, which is nearly linear w.r.t. $T$, we choose the hyper-parameter $T=7$ and use it in the remaining experiments.

## Experiments ::: Predicting S&P 500 Index

We compare our approach with the following strong baselines on predicting the S&P 500 index, which also only use financial news:

BIBREF21 uses bags-of-words to represent news documents, and constructs the prediction model by using Support Vector Machines (SVMs).

BIBREF1 uses event embeddings as input and convolutional neural network prediction model.

BIBREF13 empowers event embeddings with knowledge bases like YAGO and also adopts convolutional neural networks as the basic prediction framework.

BIBREF22 uses fully connected model and character-level embedding input with LSTM to encode news texts.

BIBREF23 uses recurrent neural networks with skip-thought vectors to represent news text.

Table TABREF26 shows the test set results on predicting the S&P 500 index. From the table we can see that our ANRES model achieves the best results on the test sets. By comparing with BIBREF21, we can find that using news event embeddings and deep learning modules can be better representative and also flexible when dealing with high-dimension features.

When comparing with BIBREF1 and the knowledge-enhanced BIBREF13, we find that extracting structured events may suffer from error propagation. And more importantly, modeling the correlations between trading days can better capture the compositional effects of multiple news events.

By comparing with BIBREF22 and BIBREF23, despite that modeling the correlations between trading days can bring better results, we also find that modeling the noise by using a state-related random factor may be effective because of the high market stochasticity.

## Experiments ::: Ablation Study on News and Noise

We explore the effects of different types of news events and the introduced random noise factor with ablation on the test set. More specifically, we disable the past news, the present news, future news and the noise factor, respectively. The S&P 500 index prediction results of the ablated models are shown in Table TABREF28. First, without using the past news events, the result becomes the lowest. The reason may be that history news contains the biggest amount of news events. In addition, considering the trading sequence length and the time windows of future news, if we disable the past news, most of them will not be involved in our model at any chance, while the present or the past news will be input on adjacent trading days.

Second, it is worth noticing that using the future news events is more effective than using the present news events. On the one hand, it confirms the importances to involve the future news in our ANRES model, which can deal with insider trading factors to some extent. On the other hand, the reason may be the news impact redundancy in sequence, as the future news impact on the $t-1$-th day should be transited to the $t$-th day to compensate the absent loss of the present news events.

The effect of modeling the noise factor is lower only to modeling the past news events, but higher than the other ablated models, which demonstrates the effectiveness of the noise factor module. We think the reason may because that modeling such an additive noise can separate the effects of news event impacts from other factors, which makes modeling the stock price movement trends more clearly.

## Experiments ::: Predicting Individual Stock Movements

Other than predicting the S&P 500 index, we also investigate the effectiveness of our approach on the problem of individual stock prediction using the test set. We count the amounts of individual company related news events for each company by name matching, and select five well known companies with sufficient news, Apple, Citigroup, Boeing Company, Google and Wells Fargo from four different sectors, which is classified by the Global Industry Classification Standard. For each company, we prepare not only news events about itself, but also news events about the whole companies in the sector. We use company news, sector news and all financial news to predict individual stock price movements, respectively. The experimental results and news statistics are listed in Table TABREF30.

The result of individual stock prediction by only using company news dramatically outperforms that of sector news and all news, which presents a negative correlation between total used amounts of news events and model performance. The main reason maybe that company-related news events can more directly affect the volatility of company shares, while sector news and all news contain many irrelevant news events, which would obstruct our ANRES model's learning the underlaying stock price movement trends.

Note that BIBREF1, BIBREF13 and BIBREF11 also reported results on individual stocks. But we cannot directly compare our results with them because the existing methods used different individual stocks on different data split to report results, and BIBREF1, BIBREF13 reported only development set results. This is reasonable since the performance of each model can vary from stock to stock over the S&P 500 chart and comparison over the whole index is more indicative.

## Experiments ::: Case Study

To look into what news event contributes the most to our prediction result, we further analyze the test set results of predicting Apple Inc.'s stock price movements only using company news, which achieves the best results among the five selected companies mentioned before.

As shown in Figure FIGREF31, we take the example trading sequence from 07/15/2013 to 07/23/2013 for illustration. The table on the left shows the selected top-ten news events, while attention visualization and results are shown on the right chart. Note that there are almost fifty different past news events in total for the trading sequence, and the news events listed on the left table are selected by ranking attention scores from the past news events, which are the most effective news according to the ablation study. There are some zeros in the attention heat map because these news do not belong to the corresponding trading days.

We can find that the news event No. 1 has been correlated with the stock price rises on 07/15/2013, but for the next two trading days, its impact fades out. On 07/18/2013, the news event No. 7 begins to show its impact. However, our ANRES model pays too much attention in it and makes the incorrect prediction that the stock price decreases. On the next trading day, our model infers that the impact of the news event No. 2 is bigger than that of the news event No. 7, which makes an incorrect prediction again. From these findings, we can see that our ANRES model tends to pay more attention to a new event when it first occurs, which offers us a potential improving direction in the future.

## Conclusion

We investigated explicit modeling of stock value sequences in news-driven stock prediction by suing an LSTM state to model the fundamentals, adding news impact and noise impact by using attention and noise sampling, respectively. Results show that our method is highly effective, giving the best performance on a standard benchmark. To our knowledge, we are the first to explicitly model both events and noise over a fundamental stock value state for news-driven stock movement prediction.
