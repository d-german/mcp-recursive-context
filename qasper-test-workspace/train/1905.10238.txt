# Incorporating Context and External Knowledge for Pronoun Coreference Resolution

**Paper ID:** 1905.10238

## Abstract

Linking pronominal expressions to the correct references requires, in many cases, better analysis of the contextual information and external knowledge. In this paper, we propose a two-layer model for pronoun coreference resolution that leverages both context and external knowledge, where a knowledge attention mechanism is designed to ensure the model leveraging the appropriate source of external knowledge based on different context. Experimental results demonstrate the validity and effectiveness of our model, where it outperforms state-of-the-art models by a large margin.

## Introduction

The question of how human beings resolve pronouns has long been of interest to both linguistics and natural language processing (NLP) communities, for the reason that pronoun itself has weak semantic meaning BIBREF0 and brings challenges in natural language understanding. To explore solutions for that question, pronoun coreference resolution BIBREF1 was proposed. As an important yet vital sub-task of the general coreference resolution task, pronoun coreference resolution is to find the correct reference for a given pronominal anaphor in the context and has been shown to be crucial for a series of downstream tasks BIBREF2 , including machine translation BIBREF3 , summarization BIBREF4 , information extraction BIBREF5 , and dialog systems BIBREF6 .

Conventionally, people design rules BIBREF1 , BIBREF7 , BIBREF8 or use features BIBREF9 , BIBREF10 , BIBREF11 to resolve the pronoun coreferences. These methods heavily rely on the coverage and quality of the manually defined rules and features. Until recently, end-to-end solution BIBREF12 was proposed towards solving the general coreference problem, where deep learning models were used to better capture contextual information. However, training such models on annotated corpora can be biased and normally does not consider external knowledge.

Despite the great efforts made in this area in the past few decades BIBREF1 , BIBREF8 , BIBREF9 , BIBREF13 , pronoun coreference resolution remains challenging. The reason behind is that the correct resolution of pronouns can be influenced by many factors BIBREF0 ; many resolution decisions require reasoning upon different contextual and external knowledge BIBREF14 , which is also proved in other NLP tasks BIBREF15 , BIBREF16 , BIBREF17 . Figure 1 demonstrates such requirement with three examples, where Example A depends on the plurality knowledge that `them' refers to plural noun phrases; Example B illustrates the gender requirement of pronouns where `she' can only refer to a female person (girl); Example C requires a more general type of knowledge that `cats can climb trees but a dog normally does not'. All of these knowledge are difficult to be learned from training data. Considering the importance of both contextual information and external human knowledge, how to jointly leverage them becomes an important question for pronoun coreference resolution.

In this paper, we propose a two-layer model to address the question while solving two challenges of incorporating external knowledge into deep models for pronoun coreference resolution, where the challenges include: first, different cases have their knowledge preference, i.e., some knowledge is exclusively important for certain cases, which requires the model to be flexible in selecting appropriate knowledge per case; second, the availability of knowledge resources is limited and such resources normally contain noise, which requires the model to be robust in learning from them.

Consequently, in our model, the first layer predicts the relations between candidate noun phrases and the target pronoun based on the contextual information learned by neural networks. The second layer compares the candidates pair-wisely, in which we propose a knowledge attention module to focus on appropriate knowledge based on the given context. Moreover, a softmax pruning is placed in between the two layers to select high confident candidates. The architecture ensures the model being able to leverage both context and external knowledge. Especially, compared with conventional approaches that simply treat external knowledge as rules or features, our model is not only more flexible and effective but also interpretable as it reflects which knowledge source has the higher weight in order to make the decision. Experiments are conducted on a widely used evaluation dataset, where the results prove that the proposed model outperforms all baseline models by a great margin.

Above all, to summarize, this paper makes the following contributions:

## The Task

Following the conventional setting BIBREF1 , the task of pronoun coreference resolution is defined as: for a pronoun $p$ and a candidate noun phrase set ${\mathcal {N}}$ , the goal is to identify the correct non-pronominal references set ${\mathcal {C}}$ . the objective is to maximize the following objective function: 

$${\mathcal {J}}= \frac{\sum _{c \in {\mathcal {C}}}{e^{F(c, p)}}}{\sum _{n \in {\mathcal {N}}}e^{F(n, p)}},$$   (Eq. 8) 

where $c$ is the correct reference and $n$ the candidate noun phrase. $F(\cdot )$ refers to the overall coreference scoring function for each $n$ regarding $p$ . Following BIBREF8 , all non-pronominal noun phrases in the recent three sentences of the pronoun $p$ are selected to form $N$ .

Particularly in our setting, we want to leverage both the local contextual information and external knowledge in this task, thus for each $n$ and $p$ , $F(.)$ is decomposed into two components: 

$$F(n, p) = F_c(n, p) + F_k(n, p),$$   (Eq. 10) 

where $F_c(n, p)$ is the scoring function that predicts the relation between $n$ and $p$ based on the contextual information; $F_k(n, p)$ is the scoring function that predicts the relation between $n$ and $p$ based on the external knowledge. There could be multiple ways to compute $F_c$ and $F_k$ , where a solution proposed in this paper is described as follows.

## The Model

The architecture of our model is shown in Figure 2 , where we use two layers to incorporate contextual information and external knowledge. Specifically, the first layer takes the representations of different $n$ and the $p$ as input and predict the relationship between each pair of $n$ and $p$ , so as to compute $F_c$ . The second layer leverages the external knowledge to compute $F_k$ , which consists of pair-wise knowledge score $f_k$ among all candidate $n$ . To enhance the efficiency of the model, a softmax pruning module is applied to select high confident candidates into the second layer. The details of the aforementioned components are described in the following subsections.

## Encoding Contextual Information

Before $F_c$ is computed, the contextual information is encoded through a span representation (SR) module in the first layer of the model. Following BIBREF12 , we adopt the standard bidirectional LSTM (biLSTM) BIBREF18 and the attention mechanism BIBREF19 to generate the span representation, as shown in Figure 3 . Given that the initial word representations in a span $n_i$ are ${\bf x}_1,...,{\bf x}_T$ ,

we denote their representations ${\bf x}^*_1,...,{\bf x}^*_T$ after encoded by the biLSTM.

Then we obtain the inner-span attention by 

$$a_t = \frac{e^{\alpha _t}}{\sum _{k=1}^{T}e^{\alpha _k}},$$   (Eq. 14) 

where $\alpha _t$ is computed via a standard feed-forward neural network $\alpha _t$ = $NN_\alpha ({\bf x}^*_t)$ . Thus, we have the weighted embedding of each span $\hat{x}_i$ through 

$$\hat{{\bf x}}_i = \sum _{k=1}^{T}a_k \cdot {\bf x}_k.$$   (Eq. 16) 

Afterwards, we concatenate the starting ( ${\bf x}^*_{start}$ ) and ending ( ${\bf x}^*_{end}$ ) embedding of each span, as well as its weighted embedding ( $\hat{{\bf x}}_i$ ) and the length feature ( $\phi (i)$ ) to form its final representation $e$ : 

$${\bf e}_i = [{\bf x}^*_{start},{\bf x}^*_{end},\hat{{\bf x}}_i,\phi (i)].$$   (Eq. 17) 

Once the span representation of $n \in {\mathcal {N}}$ and $p$ are obtained, we compute $F_c$ for each $n$ with a standard feed-forward neural network: 

$$F_c(n, p) = NN_c([{\bf e}_n, {\bf e}_p, {\bf e}_n \odot {\bf e}_p]),$$   (Eq. 18) 

where $\odot $ is the element-wise multiplication.

## Processing External Knowledge

In the second layer of our model, external knowledge is leveraged to evaluate all candidate $n$ so as to give them reasonable $F_k$ scores. In doing so, each candidate is represented as a group of features from different knowledge sources, e.g., `the cat' can be represented as a singular noun, unknown gender creature, and a regular subject of the predicate verb `climb'. For each candidate, we conduct a series of pair-wise comparisons between it and all other ones to result in its $F_k$ score. An attention mechanism is proposed to perform the comparison and selectively use the knowledge features. Consider there exists noise in external knowledge, especially when it is automatically generated, such attention mechanism ensures that, for each candidate, reliable and useful knowledge is utilized rather than ineffective ones. The details of the knowledge attention module and the overall scoring are described as follows.

Knowledge Attention Figure 4 demonstrates the structure of the knowledge attention module, where there are two components: (1) weighting: assigning weights to different knowledge features regarding their importance in the comparison; (2) scoring: valuing a candidate against another one based on their features from different knowledge sources. Assuming that there are $m$ knowledge sources input to our model, each candidate can be represented by $m$ different features, which are encoded as embeddings. Therefore, two candidates $n$ and $n^\prime $ regarding $p$ have their knowledge feature embeddings ${\bf k}_{n,p}^1, {\bf k}_{n,p}^2, ..., {\bf k}_{n,p}^m$ and ${\bf k}_{n^\prime ,p}^1,{\bf k}_{n^\prime ,p}^2,...,{\bf k}_{n^\prime ,p}^m$ , respectively. The weighting component receives all features ${\bf k}$ for $n$ and $n^\prime $ , and the span representations $m$0 and $m$1 as input, where $m$2 and $m$3 help selecting appropriate knowledge based on the context. As a result, for a candidate pair ( $m$4 , $m$5 ) and a knowledge source $m$6 , its knowledge attention score is computed via 

$$\beta _i(n, n^\prime , p) = NN_{ka}([{\bf o}_{n,p}^i, {\bf o}_{n^\prime ,p}^i, {\bf o}_{n,p}^i \odot {\bf o}_{n^\prime ,p}^i]),$$   (Eq. 21) 

where $ {\bf o}_{n,p}^i= [{\bf e}_n, {\bf k}_{n,p}^i]$ and ${\bf o}_{n^\prime ,p}^i = [{\bf e}_{n^\prime }, {\bf k}_{n^\prime ,p}^i]$ are the concatenation of span representation and external knowledge embedding for candidate $n$ and $n^\prime $ respectively. The weight for features from different knowledge sources is thus computed via 

$$w_i = \frac{e^{\beta _i}}{\sum _{j=1}^{m}e^{\beta _j}}.$$   (Eq. 22) 

Similar to the weighting component, for each feature $i$ , we compute its score $f_k^i(n, n^\prime , p)$ for $n$ against $n^\prime $ in the scoring component through 

$$f_k^i(n, n^\prime , p) = NN_{ks}([{\bf k}_{n,p}^i, {\bf k}_{n^\prime ,p}^i, {\bf k}_{n,p}^i \odot {\bf k}_{n^\prime ,p}^i]).$$   (Eq. 23) 

where it is worth noting that we exclude ${\bf e}$ in this component for the reason that, in practice, the dimension of ${\bf e}$ is normally much higher than ${\bf k}$ . As a result, it could dominate the computation if ${\bf e}$ and ${\bf k}$ is concatenated.

Once the weights and scores are obtained, we have a weighted knowledge score for $n$ against $n^\prime $ : 

$$f_k(n, n^\prime , p) = \sum _{i=1}^{m}w_i \cdot f_k^i(n, n^\prime , p).$$   (Eq. 25) 

Overall Knowledge Score After all pairs of $n$ and $n^\prime $ are processed by the attention module, the overall knowledge score for $n$ is computed through the averaged $f_k(n, n^\prime , p)$ over all $n^\prime $ : 

$$F_k(n, p) = \frac{\sum _{n^\prime \in {\mathcal {N}}_o} f_k(n, n^\prime , p)}{|{\mathcal {N}}_o|},$$   (Eq. 26) 

where ${\mathcal {N}}_o = {\mathcal {N}}- n$ for each $n$ .

## Softmax Pruning

Normally, there could be many noun phrases that serve as the candidates for the target pronoun. One potential obstacle in the pair-wise comparison of candidate noun phrases in our model is the squared complexity $O(|{\mathcal {N}}|^2)$ with respect to the size of ${\mathcal {N}}$ . To filter out low confident candidates so as to make the model more efficient, we use a softmax-pruning module between the two layers in our model to select candidates for the next step. The module takes $F_c$ as input for each $n$ , uses a softmax computation: 

$$\hat{F}_c(n, p) = \frac{e^{F_c(n, p)}}{\sum _{n_i \in {\mathcal {N}}}e^{F_c(n_i, p)}}.$$   (Eq. 28) 

where candidates with higher $\hat{F}_c$ are kept, based on a threshold $t$ predefined as the pruning standard. Therefore, if candidates have similar $F_c$ scores, the module allow more of them to proceed to the second layer. Compared with other conventional pruning methods BIBREF12 , BIBREF20 that generally keep a fixed number of candidates, our pruning strategy is more efficient and flexible.

## Data

The CoNLL-2012 shared task BIBREF21 corpus is used as the evaluation dataset, which is selected from the Ontonotes 5.0. Following conventional approaches BIBREF9 , BIBREF11 , for each pronoun in the document, we consider candidate $n$ from the previous two sentences and the current sentence. For pronouns, we consider two types of them following BIBREF9 , i.e., third personal pronoun (she, her, he, him, them, they, it) and possessive pronoun (his, hers, its, their, theirs). Table 1 reports the number of the two type pronouns and the overall statistics for the experimental dataset. According to our selection range of candidate $n$ , on average, each pronoun has 4.6 candidates and 1.3 correct references.

## Knowledge Types

In this study, we use two types of knowledge in our experiments. The first type is linguistic features, i.e., plurality and animacy & gender. We employ the Stanford parser, which generates plurality, animacy, and gender markups for all the noun phrases, to annotate our data. Specifically, the plurality feature denotes each $n$ and $p$ to be singular or plural. For each candidate $n$ , if its plurality status is the same as the target pronoun, we label it 1, otherwise 0. The animacy & gender (AG) feature denotes whether a $n$ or $p$ is a living object, and being male, female, or neutral if it is alive. For each candidate $n$ , if its AG feature matches the target pronoun's, we label it 1, otherwise 0.

The second type is the selectional preference (SP) knowledge. For this knowledge, we create a knowledge base by counting how many times a predicate-argument tuple appears in a corpus and use the resulted number to represent the preference strength. Specifically, we use the English Wikipedia as the base corpus for such counting. Then we parse the entire corpus through the Stanford parser and record all dependency edges in the format of (predicate, argument, relation, number), where predicate is the governor and argument the dependent in the original parsed dependency edge. Later for sentences in the training and test data, we firstly parse each sentence and find out the dependency edge linking $p$ and its corresponding predicate. Then for each candidate $n$ in a sentence, we check the previously created SP knowledge base and find out how many times it appears as the argument of different predicates with the same dependency relation (i.e., nsubj and dobj). The resulted frequency is grouped into the following buckets [1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+] and we use the bucket id as the final SP knowledge. Thus in the previous example:

The dog is chasing the cat but it climbs the tree.

Its parsing result indicates that `it' is the subject of the verb `climb'. Then for `the dog', `the cat', and `the tree', we check their associations with `climb' in the knowledge base and group them in the buckets to form the SP knowledge features.

## Baselines

Several baselines are compared in this work. The first two are conventional unsupervised ones:

[leftmargin=*]

Recent Candidate, which simply selects the most recent noun phrase that appears in front of the target pronoun.

Deterministic model BIBREF22 , which proposes one multi-pass seive model with human designed rules for the coreference resolution task.

Besides the unsupervised models, we also compare with three representative supervised ones:

[leftmargin=*]

Statistical model, proposed by BIBREF23 , uses human-designed entity-level features between clusters and mentions for coreference resolution.

Deep-RL model, proposed by BIBREF24 , a reinforcement learning method to directly optimize the coreference matrix instead of the traditional loss function.

End2end is the current state-of-the-art coreference model BIBREF20 , which performs in an end-to-end manner and leverages both the contextual information and a pre-trained language model BIBREF25 .

Note that the Deterministic, Statistical, and Deep-RL models are included in the Stanford CoreNLP toolkit, and experiments are conducted with their provided code. For End2end, we use their released code and replace its mention detection component with gold mentions for the fair comparison.

To clearly show the effectiveness of the proposed model, we also present a variation of our model as an extra baseline to illustrate the effect of different knowledge incorporation manner:

[leftmargin=*]

Feature Concatenation, a simplified version of the complete model that removes the second knowledge processing layer, but directly treats all external knowledge embeddings as features and concatenates them to span representations.

## Implementation

Following previous work BIBREF20 , we use the concatenation of the 300d GloVe embeddings BIBREF26 and the ELMo BIBREF25 embeddings as the initial word representations. Out-of-vocabulary words are initialized with zero vectors. Hyper-parameters are set as follows. The hidden state of the LSTM module is set to 200, and all the feed-forward networks in our model have two 150-dimension hidden layers. The default pruning threshold $t$ for softmax pruning is set to $10^{-7}$ . All linguistic features (plurality and AG) and external knowledge (SP) are encoded as 20-dimension embeddings.

For model training, we use cross-entropy as the loss function and Adam BIBREF27 as the optimizer. All the aforementioned hyper-parameters are initialized randomly, and we apply dropout rate 0.2 to all hidden layers in the model. Our model treats a candidate as the correct reference if its predicted overall score $F(n,p)$ is larger than 0. The model training is performed with up to 100 epochs, and the best one is selected based on its performance on the development set.

## Experimental Results

Table 2 compares the performance of our model with all baselines. Overall, our model performs the best with respect to all evaluation metrics. Several findings are also observed from the results. First, manually defined knowledge and features are not enough to cover rich contextual information. Deep learning models (e.g., End2end and our proposed models), which leverage text representations for context, outperform other approaches by a great margin, especially on the recall. Second, external knowledge is highly helpful in this task, which is supported by that our model outperforms the End2end model significantly.

Moreover, the comparison between the two variants of our models is also interesting, where the final two-layer model outperforms the Feature Concatenation model. It proves that simply treating external knowledge as the feature, even though they are from the same sources, is not as effective as learning them in a joint framework. The reason behind this result is mainly from the noise in the knowledge source, e.g., parsing error, incorrectly identified relations, etc. For example, the plurality of 17% noun phrases are wrongly labeled in the test data. As a comparison, our knowledge attention might contribute to alleviate such noise when incorporating all knowledge sources.

Effect of Different Knowledge To illustrate the importance of different knowledge sources and the knowledge attention mechanism, we ablate various components of our model and report the corresponding F1 scores on the test data. The results are shown in Table 3 , which clearly show the necessity of the knowledge. Interestingly, AG contributes the most among all knowledge types, which indicates that potentially more cases in the evaluation dataset demand on the AG knowledge than others. More importantly, the results also prove the effectiveness of the knowledge attention module, which contributes to the performance gap between our model and the Feature Concatenation one.

Effect of Different Pruning Thresholds We try different thresholds $t$ for the softmax pruning in selecting reliable candidates. The effects of different thresholds on reducing candidates and overall performance are shown in Figure 5 and 6 respectively. Along with the increase of $t$ , both the max and the average number of pruned candidates drop quickly, so that the space complexity of the model can be reduced accordingly. Particularly, there are as much as 80% candidates can be filtered out when $t = 10^{-1}$ . Meanwhile, when referring to Figure 6 , it is observed that the model performs stable with the decreasing of candidate numbers. Not surprisingly, the precision rises when reducing candidate numbers, yet the recall drops dramatically, eventually results in the drop of F1. With the above observations, the reason we set $t = 10^{-7}$ as the default threshold is straightforward: on this value, one-third candidates are pruned with almost no influence on the model performance in terms of precision, recall, and the F1 score.

## Case Study

To further demonstrate the effectiveness of incorporating knowledge into pronoun coreference resolution, two examples are provided for detailed analysis. The prediction results of the End2end model and our complete model are shown in Table 4 . There are different challenges in both examples. In Example A, `Jesus', `man', and `my son' are all similar (male) noun phrases matching the target pronoun `He'. The End2end model predicts all of them to be correct references because their context provides limited help in distinguishing them. In Example B, the distance between `an accident' and the pronoun `it' is too far. As a result, the `None' result from the End2end model indicates that the contextual information is not enough to make the decision. As a comparison, in our model, integrating external knowledge can help to solve such challenges, e.g., for Example A, SP knowledge helps when Plurality and AG cannot distinguish all candidates.

To clearly illustrate how our model leverages the external knowledge, we visualize the knowledge attention of the correct reference against other candidates via heatmaps in Figure 7 . Two interesting observations are drawn from the visualization. First, given two candidates, if they are significantly different in one feature, our model tends to pay more attention to that feature. Take AG as an example, in Example A, the AG features of all candidates consistently match the pronoun `he' (all male/neutral). Thus the comparison between `my son' and all candidates pay no attention to the AG feature. While in Example B, the target pronoun `it' cannot describe human, thus 'father' and `friend' are 0 on the AG feature while `hospital' and `accident' are 1. As a result, the attention module emphasizes AG more than other knowledge types. Second, The importance of SP is clearly shown in these examples. In example A, Plurality and AG features cannot help, the attention module weights higher on SP because `son' appears 100 times as the argument of the parsed predicate `child' in the SP knowledge base, while other candidates appear much less at that position. In example B, as mentioned above, once AG helps filtering 'hospital' and 'accident', SP plays an important role in distinguishing them because `accident' appears 26 times in the SP knowledge base as the argument of the `fault' from the results of the parser, while `hospital' never appears at that position.

## Related Work

Coreference resolution is a core task for natural language understanding, where it detects mention span and identifies coreference relations among them. As demonstrated in BIBREF12 , mention detection and coreference prediction are the two major focuses of the task. Different from the general coreference task, pronoun coreference resolution has its unique challenge since the semantics of pronouns are often not as clear as normal noun phrases, in general, how to leverage the context and external knowledge to resolve the coreference for pronouns becomes its focus BIBREF1 , BIBREF14 , BIBREF28 .

In previous work, external knowledge including manually defined rules BIBREF1 , BIBREF9 , such as number/gender requirement of different pronouns, and world knowledge BIBREF14 , such as selectional preference BIBREF29 , BIBREF30 and eventuality knowledge BIBREF31 , have been proved to be helpful for pronoun coreference resolution. Recently, with the development of deep learning, BIBREF12 proposed an end-to-end model that learns contextual information with an LSTM module and proved that such knowledge is helpful for coreference resolution when the context is properly encoded. The aforementioned two types of knowledge have their own advantages: the contextual information covers diverse text expressions that are difficult to be predefined while the external knowledge is usually more precisely constructed and able to provide extra information beyond the training data. Different from previous work, we explore the possibility of joining the two types of knowledge for pronoun coreference resolution rather than use only one of them. To the best of our knowledge, this is the first attempt that uses deep learning model to incorporate contextual information and external knowledge for pronoun coreference resolution.

## Conclusion

In this paper, we proposed a two-layer model for pronoun coreference resolution, where the first layer encodes contextual information and the second layer leverages external knowledge. Particularly, a knowledge attention mechanism is proposed to selectively leverage features from different knowledge sources. As an enhancement to existing methods, the proposed model combines the advantage of conventional feature-based models and deep learning models, so that context and external knowledge can be synchronously and effectively used for this task. Experimental results and case studies demonstrate the superiority of the proposed model to state-of-the-art baselines. Since the proposed model adopted an extensible structure, one possible future work is to explore the best way to enhance it with more complicated knowledge resources such as knowledge graphs.

## Acknowledgements

This paper was partially supported by the Early Career Scheme (ECS, No.26206717) from Research Grants Council in Hong Kong. In addition, Hongming Zhang has been supported by the Hong Kong Ph.D. Fellowship and the Tencent Rhino-Bird Elite Training Program. We also thank the anonymous reviewers for their valuable comments and suggestions that help improving the quality of this paper.
