# Schema-Guided Dialogue State Tracking Task at DSTC8

**Paper ID:** 2002.01359

## Abstract

This paper gives an overview of the Schema-Guided Dialogue State Tracking task of the 8th Dialogue System Technology Challenge. The goal of this task is to develop dialogue state tracking models suitable for large-scale virtual assistants, with a focus on data-efficient joint modeling across domains and zero-shot generalization to new APIs. This task provided a new dataset consisting of over 16000 dialogues in the training set spanning 16 domains to highlight these challenges, and a baseline model capable of zero-shot generalization to new APIs. Twenty-five teams participated, developing a range of neural network models, exceeding the performance of the baseline model by a very high margin. The submissions incorporated a variety of pre-trained encoders and data augmentation techniques. This paper describes the task definition, dataset and evaluation methodology. We also summarize the approach and results of the submitted systems to highlight the overall trends in the state-of-the-art.

## Introduction

Virtual assistants help users accomplish tasks including but not limited to finding flights, booking restaurants, by providing a natural language interface to services and APIs on the web. Large-scale assistants like Google Assistant, Amazon Alexa, Apple Siri, Microsoft Cortana etc. need to support a large and constantly increasing number of services, over a wide variety of domains. Consequently, recent work has focused on scalable dialogue systems that can handle tasks across multiple application domains. Data-driven deep learning based approaches for multi-domain modeling have shown promise, both for end-to-end and modular systems involving dialogue state tracking and policy learning. This line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ BIBREF0, Taskmaster-1 BIBREF1, M2M BIBREF2 and FRAMES BIBREF3.

However, building large-scale assistants, as opposed to dialogue systems managing a few APIs, poses a new set of challenges. Apart from the handling a very large variety of domains, such systems need to support heterogeneous services or APIs with possibly overlapping functionality. It should also offer an efficient way of supporting new APIs or services, while requiring little or no additional training data. Furthermore, to reduce maintenance workload and accommodate future growth, such assistants need to be robust to changes in the API's interface or addition of new slot values. Such changes shouldn't require collection of additional training data or retraining the model.

The Schema-Guided Dialogue State Tracking task at the Eighth Dialogue System Technology Challenge explores the aforementioned challenges in context of dialogue state tracking. In a task-oriented dialogue, the dialogue state is a summary of the entire conversation till the current turn. The dialogue state is used to invoke APIs with appropriate parameters as specified by the user over the dialogue history. It is also used by the assistant to generate the next actions to continue the dialogue. DST, therefore, is a core component of virtual assistants.

In this task, participants are required to develop innovative approaches to multi-domain dialogue state tracking, with a focus on data-efficient joint modeling across APIs and zero-shot generalization to new APIs. The task is based on the Schema-Guided Dialogue (SGD) dataset, which, to the best of our knowledge, is the largest publicly available corpus of annotated task-oriented dialogues. With over 16000 dialogues in the training set spanning 26 APIs over 16 domains, it exceeds the existing dialogue corpora in scale. SGD is the first dataset to allow multiple APIs with overlapping functionality within each domain. To adequately test generalization in zero-shot settings, the evaluation sets contain unseen services and domains. The dataset is designed to serve as an effective testbed for intent prediction, slot filling, state tracking and language generation, among other tasks in large-scale virtual assistants.

## Related Work

Dialogue systems have constituted an active area of research for the past few decades. The advent of commercial personal assistants has provided further impetus to dialogue systems research. As virtual assistants incorporate diverse domains, zero-shot modeling BIBREF4, BIBREF5, BIBREF6, domain adaptation and transfer learning techniques BIBREF7, BIBREF8, BIBREF9 have been explored to support new domains in a data efficient manner.

Deep learning based approaches to DST have recently gained popularity. Some of these approaches estimate the dialogue state as a distribution over all possible slot-values BIBREF10, BIBREF11 or individually score all slot-value combinations BIBREF12, BIBREF13. Such approaches are, however, hard to scale to real-world virtual assistants, where the set of possible values for certain slots may be very large (date, time or restaurant name) and even dynamic (movie or event name). Other approaches utilizing a dynamic vocabulary of slot values BIBREF14, BIBREF15 still do not allow zero-shot generalization to new services and APIs BIBREF16, since they use schema elements i.e. intents and slots as fixed class labels.

Although such systems are capable of parsing the dialogue semantics in terms of these fixed intent labels, they lack understanding of the semantics of these labels. For instance, for the user utterance â€œI want to buy tickets for a movie.", such models can predict BuyMovieTickets as the correct intent based on patterns observed in the training data, but don't model either its association with the real world action of buying movie tickets, or its similarity to the action of buying concert or theatre tickets. Furthermore, because of their dependence on a fixed schema, such models are not robust to changes in the schema, and need to be retrained as new slots or intents are added. Use of domain-specific parameters renders some approaches unsuitable for zero-shot application.

## Task

The primary task of this challenge is to develop multi-domain models for DST suitable for the scale and complexity of large scale virtual assistants. Supporting a wide variety of APIs or services with possibly overlapping functionality is an important requirement of such assistants. A common approach to do this involves defining a large master schema that lists all intents and slots supported by the assistant. Each service either adopts this master schema for the representation of the underlying data, or provides logic to translate between its own schema and the master schema.

The first approach involving adoption of the master schema is not ideal if a service wishes to integrate with multiple assistants, since each of the assistants could have their own master schema. The second approach involves definition of logic for translation between master schema and the service's schema, which increases the maintenance workload. Furthermore, it is difficult to develop a master schema catering to all possible use cases.

Additionally, while there are many similar concepts across services that can be jointly modeled, for example, the similarities in logic for querying or specifying the number of movie tickets, flight tickets or concert tickets, the master schema approach does not facilitate joint modeling of such concepts, unless an explicit mapping between them is manually defined. To address these limitations, we propose a schema-guided approach, which eliminates the need for a master schema.

## Task ::: Schema-Guided Approach

Under the Schema-Guided approach, each service provides a schema listing the supported slots and intents along with their natural language descriptions (Figure FIGREF2 shows an example). The dialogue annotations are guided by the schema of the underlying service or API, as shown in Figure FIGREF3. In this example, the departure and arrival cities are captured by analogously functioning but differently named slots in both schemas. Furthermore, values for the number_stops and direct_only slots highlight idiosyncrasies between services interpreting the same concept.

The natural language descriptions present in the schema are used to obtain a semantic representation of intents and slots. The assistant employs a single unified model containing no domain or service specific parameters to make predictions conditioned on these schema elements. Using a single model facilitates representation and transfer of common knowledge across related concepts in different services. Since the model utilizes semantic representation of schema elements as input, it can interface with unseen services or APIs on which it has not been trained. It is also robust to changes like the addition of new intents or slots to the service. In addition, the participants are allowed to use any external datasets or resources to bootstrap their models.

## Dataset

As shown in Table TABREF9, our Schema-Guided Dialogue (SGD) dataset exceeds other datasets in most of the metrics at scale. The especially larger number of domains, slots, and slot values, and the presence of multiple services per domain, are representative of these scale-related challenges. Furthermore, our evaluation sets contain many services, and consequently slots, which are not present in the training set, to help evaluate model performance on unseen services.

## Dataset ::: Data Representation

The dataset consists of conversations between a virtual assistant and a user. Each conversation can span multiple services across various domains. The dialogue is represented as a sequence of turns, each containing a user or system utterance. The annotations for each turn are grouped into frames, where each frame corresponds to a single service. The annotations for user turns include the active intent, the dialogue state and slot spans for the different slots values mentioned in the turn. For system turns, we have the system actions representing the semantics of the system utterance. Each system action is represented using a dialogue act with optional parameters.

In addition to the dialogues, for each service used in the dataset, a normalized representation of the interface exposed is provided as the schema. The schema contains details like the name of the service, the list of tasks supported by the service (intents) and the attributes of the entities used by the service (slots). The schema also contains natural language descriptions of the service, intents and slots which can be used for developing models which can condition their predictions on the schema.

## Dataset ::: Comparison With Other Datasets

To reflect the constraints present in real-world services and APIs, we impose a few constraints on the data. Our dataset does not expose the set of all possible values for certain slots. Having such a list is impractical for slots like date or time because they have infinitely many possible values or for slots like movie or song names, for which new values are periodically added. Such slots are specifically identified as non-categorical slots. In our evaluation sets, we ensured the presence of a significant number of values which were not previously seen in the training set to evaluate the performance of models on unseen values. Some slots like gender, number of people, etc. are classified as categorical and we provide a list of all possible values for them. However, these values are assumed to be not consistent across services. E.g., different services may use (`male', `female'), (`M', `F') or (`he', `she') as possible values for gender slot.

Real-world services can only be invoked with certain slot combinations: e.g. most restaurant reservation APIs do not let the user search for restaurants by date without specifying a location. Although this constraint has no implications on the dialogue state tracking task, it restricts the possible conversational flows. Hence, to prevent flows not supported by actual services, we restrict services to be called with a list of slot combinations. The different service calls supported by a service are listed as intents with each intent specifying a list of required slots. The intent cannot be called without providing values for these required slots. Each intent also contains a list of optional slots with default values which can be overridden by the user.

In our dataset, we also have multiple services per domain with overlapping functionality. The intents across these services are similar but differ in terms of intent names, intent arguments, slot names, etc. In some cases, there is no one to one mapping between slot names (e.g., the num_stops and direct_only slots in Figure FIGREF3). With an ever increasing number of services and service providers, we believe that having multiple similar services per domain is much closer to the situation faced by virtual assistants than having one unique service per domain.

## Dataset ::: Data Collection And Dataset Analysis

Our data collection setup uses a dialogue simulator to generate dialogue outlines first and then paraphrase them to obtain natural utterances. Using a dialogue simulator offers us multiple advantages. First, it ensures the coverage of a large variety of dialogue flows by filtering out similar flows in the simulation phase, thus creating a much diverse dataset. Second, simulated dialogues do not require manual annotation, as opposed to a Wizard-of-Oz setup BIBREF17, which is a common approach utilized in other datasets BIBREF0. It has been shown that such datasets suffer from substantial annotation errors BIBREF18. Thirdly, using a simulator greatly simplifies the data collection task and instructions as only paraphrasing is needed to achieve a natural dialogue. This is particularly important for creating a large dataset spanning multiple domains.

The 20 domains present across the train, dev and test datasets are listed in Table TABREF10, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps briefly and then present analyses of the collected dataset.

All the services are implemented using a SQL engine. Since entity attributes are often correlated, we decided not to sample synthetic entities and instead relied on sampling entities from Freebase. The dialogue simulator interacts with the services to generate valid dialogue outlines. The simulator consists of two agents playing the roles of the user and the system. Both agents interact with each other using a finite set of actions specified through dialogue acts over a probabilistic automaton designed to capture varied dialogue trajectories. At the start of the conversation, the user agent is seeded with a scenario, which is a sequence of intents to be fulfilled. The user agent generates dialogue acts to be output and combines them with values retrieved from the service/API to create the user actions. The system agent responds by following a similar procedure but also ensures that the generated flows are valid. We identified over 200 distinct scenarios for the training set each consisting up to 5 intents from various domains. Finally, the dialogue outlines generated are paraphrased into a natural conversation by crowd workers. We ensure that the annotations for the dialogue state and slots generated by the simulator are preserved and hence need no other annotation. We omit details for brevity: please refer to BIBREF19 for more details.

The entire dataset consists of over 16K dialogues spanning multiple domains. Overall statistics of the dataset and comparison with other datasets can be seen in Table TABREF9. Figure FIGREF8 shows the details of the distribution of dialogue lengths across single-domain and multi-domain dialogues. The single-domain dialogues in our dataset contain an average of 15.3 turns, whereas the multi-domain ones contain 23 turns on average. Figure FIGREF8 shows the frequency of the different dialogue acts contained in the dataset. The dataset also contains a significant number of unseen domains/APIs in the dev and test sets. 77% of the dialogue turns in the test set and 45% of the turns in dev set contain at least one service not present in the training set. This facilitates the development of models which can generalize to new domains with very few labelled examples.

## Submissions

The submissions from 25 teams included a variety of approaches and innovative solutions to specific problems posed by this dataset. For the workshop, we received submissions from 9 of these teams. In this section, we provide a short summary of the approaches followed by these teams. For effective generalization to unseen APIs, most teams used pre-trained encoders to encode schema element descriptions. Unless otherwise mentioned, a pre-trained BERT BIBREF20 encoder was used.

Team 2 BIBREF21: This was the only paper not using a pre-trained encoder, thus providing another important baseline. They rely on separate RNNs to encode service, slot and intent descriptions, and a BiRNN to encode dialogue history. Slot values are inferred using a TRADE-like encoder-decoder setup with a 3-way slot status gate, using the utterance encoding and schema element embeddings as context.

Team 5 BIBREF22: They predict values for categorical slots using a softmax over all candidate values. Non-categorical slot values are predicted by first predicting the status of each slot and then using a BiLSTM-CRF layer for BIO tagging BIBREF23. They also utilize a slot adoption tracker to predict if the values proposed by the system are accepted by the user.

Team 9 BIBREF24: This team submitted the winning entry, beating the second-placed team by around 9% in terms of joint goal accuracy. They use two separate models for categorical and non-categorical slots, and treat numerical categorical slots as non-categorical. They also use the entire dialogue history as input. They perform data augmentation by back translation between English and Chinese, which seems to be one of the distinguishing factors resulting in a much higher accuracy.

Team 12 BIBREF25: They use auxiliary binary features to connect previous intent to current intent, slots to dialogue history and source slots to target slots for slot transfer. Non-categorical slots are modeled similar to question answering by adding a null token and predicting spans for slot values. In-domain and cross-domain slot transfers are modeled as separate binary decisions by passing the slot descriptions as additional inputs.

Team 16 BIBREF26: They convert the tracking task for both categorical and non-categorical slots into a question answering task by feeding in the schema and the previous turns as the context. Similar to the baseline model, prediction is performed in two stages. The status of each slot (active/inactive/dontcare) is predicted using a classifier, following which the value is predicted as a span in the context. The same network is used for the different prediction tasks but the leading token and separator tokens used are different. They observe large gains by fine-tuning the schema embeddings and increasing the number of past turns fed as context.

Team 23 BIBREF27: They use a large scale multi-task model utilizing a single pass of a BERT based model for all tasks. Embeddings are calculated for the intents and slot value by using dialogue history, service and slot descriptions, possible values for categorical slots and are used for the various predictions.

Anonymous Team A BIBREF28: We could not identify which team submitted this model. They use multi-head attention twice to obtain domain-conditioned and slot-conditioned representations of the dialogue history. These representations are concatenated to obtain the full context which is used for the various predictions.

Anonymous Team B BIBREF29: We could not identify which team submitted this model. They use separate NLU systems for the sub tasks of predicting intents, requested slots, slot status, categorical and non-categorical slot values. They use a rule-based DST system with a few additions resulting in significant improvement. The improvements include adding dropout to intent prediction to account for train-test mismatch, using the entire predicted slot status distribution and separate binary predictions for slot transfer.

Anonymous Team C BIBREF30: They use a two-stage model with a candidate tracker for NLU and a candidate classifier to update the dialogue state. A slot tagger identifies slot values, which are used to update the candidate tracker. The candidate classifier uses the utterances and slot/intent descriptions to predict the final dialogue state. They also use an additional loss to penalize incorrect prediction on which slots appear in the current turn.

## Evaluation

We consider the following metrics for automatic evaluation of different submissions. Joint goal accuracy has been used as the primary metric to rank the submissions.

Active Intent Accuracy: The fraction of user turns for which the active intent has been correctly predicted.

Requested Slot F1: The macro-averaged F1 score for requested slots over all eligible turns. Turns with no requested slots in ground truth and predictions are skipped.

Average Goal Accuracy: For each turn, we predict a single value for each slot present in the dialogue state. This is the average accuracy of predicting the value of a slot correctly.

Joint Goal Accuracy: This is the average accuracy of predicting all slot assignments for a given service in a turn correctly.

In order to better reflect model performance in our task's specific setting, we introduce changes in the definitions of evaluation metrics from prior work. These are listed below:

[leftmargin=*]

Joint goal accuracy calculation: Traditionally, joint goal accuracy has been defined as the accuracy of predicting the dialogue state for all domains correctly. This is not practical in our setup, as the large number of services would result in near zero joint goal accuracy if the traditional definition is used. Furthermore, an incorrect dialogue state prediction for a service in the beginning of a dialogue degrades the joint goal accuracy for all future turns, even if the predictions for all other services are correct. Hence, joint goal accuracy calculated this way may not provide as much insight into the performance on different services. To address these concerns, only the services which are active or pertinent in a turn are included in the dialogue state. Thus, a service ceases to be a part of the dialogue state once its intent has been fulfilled.

Fuzzy matching for non-categorical slot values: The presence of non-categorical slots is another distinguishing feature of our dataset. These slots don't have a predefined vocabulary, and their values are predicted as a substring or span of the past user or system utterances. Drawing inspiration from the metrics used for slot tagging in spoken language understanding, we use a fuzzy matching score for non-categorical slots to reward partial matches with the ground truth.

Average goal accuracy: To calculate average goal accuracy, we do not take into account instances when both the ground truth and the predicted values for a slot are empty. Since for a given slot, a large number of utterances have an empty assignment, models can achieve a relatively high average goal accuracy just by predicting an empty assignment for each slot unless specifically excluded as in our evaluation.

## Results

The test set contains a total of 21 services, among which 6 services are also present in the training set (seen services), whereas the remaining 15 are not present in the training set (unseen services). Table TABREF11 shows the evaluation metrics for the different submissions obtained on the test set. It also lists the performance of different submissions on seen and unseen services, helping evaluate the effectiveness in zero-shot settings. Team 9 achieved a very high joint goal accuracy of 86.53%, around 9% higher than the second-placed team. We observed the following trends across submissions:

For unseen services, performance on categorical slots is comparable to that on non-categorical slots. On the other hand, for seen services, the performance on categorical slots is better. This could be because there is less signal to differentiate between the different possible values for a categorical slot when they have not been observed in the training set.

The winning team's performance on seen services is similar to that of the other top teams. However, the winning team has a considerable edge on unseen services, outperforming the second team by around 12% in terms of joint goal accuracy. This margin was observed across both categorical and non-categorical slots.

Among unseen services, when looking at services belonging to unseen domains, the winning team was ahead of the other teams by at least 15%. The performance on categorical slots for unseen domains was about the same as that for seen services and domains. For other teams, there was at least a 20% drop in accuracy of categorical slots in unseen domains vs seen domains and services.

The joint goal accuracy of most of the models was worse by 15 percentage points on an average on the test set as compared to the dev set. This could be because the test set contains a much higher proportion of turns with at least one unseen services as compared to the dev set (77% and 45% respectively).

## Summary

In this paper, we summarized the Schema-Guided Dialogue State Tracking task conducted at the Eighth Dialogue System Technology Challenge. This task challenged participants to develop dialogue state tracking models for large scale virtual assistants, with particular emphasis on joint modeling across different domains and APIs for data-efficiency and zero-shot generalization to new/unseen APIs. In order to encourage the development of such models, we constructed a new dataset spanning 16 domains (and 4 new domains in dev and test sets), defining multiple APIs with overlapping functionality for each of these domains. We advocated the use of schema-guided approach to building large-scale assistants, facilitating data-efficient joint modeling across domains while reducing maintenance workload.

The Schema-Guided Dialogue dataset released as part of this task is the first to highlight many of the aforementioned challenges. As a result, this task led to the development of several models utilizing the schema-guided approach for dialogue state tracking. The models extensively utilized pre-trained encoders like BERT BIBREF20, XLNet BIBREF31 etc. and employed data augmentation techniques to achieve effective zero-shot generalization to new APIs. The proposed schema-guided approach is fairly general and can be used to develop other dialogue system components such as language understanding, policy and response generation. We plan to explore them in future works.

## Summary ::: Acknowledgements

The authors thank Guan-Lin Chao, Amir Fayazi and Maria Wang for their advice and assistance.
