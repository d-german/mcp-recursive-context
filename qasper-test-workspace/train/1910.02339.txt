# Natural- to formal-language generation using Tensor Product Representations

**Paper ID:** 1910.02339

## Abstract

Generating formal-language represented by relational tuples, such as Lisp programs or mathematical expressions, from a natural-language input is an extremely challenging task because it requires to explicitly capture discrete symbolic structural information from the input to generate the output. Most state-of-the-art neural sequence models do not explicitly capture such structure information, and thus do not perform well on these tasks. In this paper, we propose a new encoder-decoder model based on Tensor Product Representations (TPRs) for Natural- to Formal-language generation, called TP-N2F. The encoder of TP-N2F employs TPR 'binding' to encode natural-language symbolic structure in vector space and the decoder uses TPR 'unbinding' to generate a sequence of relational tuples, each consisting of a relation (or operation) and a number of arguments, in symbolic space. TP-N2F considerably outperforms LSTM-based Seq2Seq models, creating a new state of the art results on two benchmarks: the MathQA dataset for math problem solving, and the AlgoList dataset for program synthesis. Ablation studies show that improvements are mainly attributed to the use of TPRs in both the encoder and decoder to explicitly capture relational structure information for symbolic reasoning.

## INTRODUCTION

When people perform explicit reasoning, they can typically describe the way to the conclusion step by step via relational descriptions. There is ample evidence that relational representations are important for human cognition (e.g., BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4). Although a rapidly growing number of researchers use deep learning to solve complex symbolic reasoning and language tasks (a recent review is BIBREF5), most existing deep learning models, including sequence models such as LSTMs, do not explicitly capture human-like relational structure information.

In this paper we propose a novel neural architecture, TP-N2F, to solve natural- to formal-language generation tasks (N2F). In the tasks we study, math or programming problems are stated in natural-language, and answers are given as programs, sequences of relational representations, to solve the problem. TP-N2F encodes the natural-language symbolic structure of the problem in an input vector space, maps this to a vector in an intermediate space, and uses that vector to produce a sequence of output vectors that are decoded as relational structures. Both input and output structures are modelled as Tensor Product Representations (TPRs) BIBREF6. During encoding, NL-input symbolic structures are encoded as vector space embeddings using TPR `binding' (following BIBREF7); during decoding, symbolic constituents are extracted from structure-embedding output vectors using TPR `unbinding' (following BIBREF8, BIBREF9).

Our contributions in this work are as follows. (i) We propose a role-level analysis of N2F tasks. (ii) We present a new TP-N2F model which gives a neural-network-level implementation of a model solving the N2F task under the role-level description proposed in (i). To our knowledge, this is the first model to be proposed which combines both the binding and unbinding operations of TPRs to achieve generation tasks through deep learning. (iii) State-of-the-art performance on two recently developed N2F tasks shows that the TP-N2F model has significant structure learning ability on tasks requiring symbolic reasoning through program synthesis.

## Background: Review of Tensor-Product Representation

The TPR mechanism is a method to create a vector space embedding of complex symbolic structures. The type of a symbol structure is defined by a set of structural positions or roles, such as the left-child-of-root position in a tree, or the second-argument-of-$R$ position of a given relation $R$. In a particular instance of a structural type, each of these roles may be occupied by a particular filler, which can be an atomic symbol or a substructure (e.g., the entire left sub-tree of a binary tree can serve as the filler of the role left-child-of-root). For now, we assume the fillers to be atomic symbols.

The TPR embedding of a symbol structure is the sum of the embeddings of all its constituents, each constituent comprising a role together with its filler. The embedding of a constituent is constructed from the embedding of a role and the embedding of the filler of that role: these are joined together by the TPR `binding' operation, the tensor (or generalized outer) product $\otimes $.

Formally, suppose a symbolic type is defined by the roles $\lbrace r_i \rbrace $, and suppose that in a particular instance of that type, ${S}$, role $r_i$ is bound by filler $f_i$. The TPR embedding of ${S}$ is the order-2 tensor = i i i = i i i where $\lbrace _i \rbrace $ are vector embeddings of the fillers and $\lbrace _i \rbrace $ are vector embeddings of the roles. In Eq. SECREF2, and below, for notational simplicity we conflate order-2 tensors and matrices.

As a simple example, consider the symbolic type string, and choose roles to be $r_1 = $ first_element, $r_2 = $ second_element, etc. Then in the specific string S = cba, the first role $r_1$ is filled by c, and $r_2$ and $r_3$ by b and a, respectively. The TPR for S is $\otimes _1 + \otimes _2 + \otimes _3$, where $, , $ are the vector embeddings of the symbols a, b, c, and $_i$ is the vector embedding of role $r_i$.

A TPR scheme for embedding a set of symbol structures is defined by a decomposition of those structures into roles bound to fillers, an embedding of each role as a role vector, and an embedding of each filler as a filler vector. Let the total number of roles and fillers available be $n_{\mathrm {R}}, n_{\mathrm {F}}$, respectively. Define the matrix of all possible role vectors to be $\in ^{d_{\mathrm {R}}\times n_{\mathrm {R}}}$, with column $i$, $[]_{:i} = _i \in ^{d_{\mathrm {R}}}$, comprising the embedding of $r_i$. Similarly let $\in ^{d_{\mathrm {F}}\times n_{\mathrm {F}}}$ be the matrix of all possible filler vectors. The TPR $\in ^{d_{\mathrm {F}}\times d_{\mathrm {R}}}$. Below, $d_{\mathrm {R}}, n_{\mathrm {R}}, d_{\mathrm {F}}, n_{\mathrm {F}}$ will be hyper-parameters, while $, $ will be learned parameter matrices.

Using summation in Eq.SECREF2 to combine the vectors embedding the constituents of a structure risks non-recoverability of those constituents given the embedding $$ of the the structure as a whole. The tensor product is chosen as the binding operation in order to enable recovery of the filler of any role in a structure ${S}$ given its TPR $$. This can be done with perfect precision if the embeddings of the roles are linearly independent. In that case the role matrix $$ has a left inverse $$: $= $. Now define the unbinding (or dual) vector for role $r_j$, $_j$, to be the $j^{{\mathrm {th}}}$ column of $^\top $: $U_{:j}^\top $. Then, since $[]_{ji} = []_{ji} = _{j:} _{:i} = [^\top _{:j}]^\top _{:i} =_j^\top _i = _i^\top _j$, we have $_i^\top _j = \delta _{ji}$. This means that, to recover the filler of $r_j$ in the structure with TPR $$, we can take its tensor inner product (or matrix-vector product) with $_j$: j = [ i i i] j = i i ij = j

In the architecture proposed here, we will make use of both TPR binding using the tensor product with role vectors $_i$ and TPR unbinding using the tensor inner product with unbinding vectors $_j$. Binding will be used to produce the order-2 tensor $_S$ embedding of the NL problem statement. Unbinding will be used to generate output relational tuples from an order-3 tensor $$. Because they pertain to different representations (of different orders in fact), the binding and unbinding vectors we will use are not related to one another.

## TP-N2F Model

We propose a general TP-N2F neural network architecture operating over TPRs to solve N2F tasks under a proposed role-level description of those tasks. In this description, natural-language input is represented as a straightforward order-2 role structure, and formal-language relational representations of outputs are represented with a new order-3 recursive role structure proposed here. Figure FIGREF3 shows an overview diagram of the TP-N2F model. It depicts the following high-level description.

As shown in Figure FIGREF3, while the natural-language input is a sequence of words, the output is a sequence of multi-argument relational tuples such as $(R \hspace{2.84526pt}A_1 \hspace{2.84526pt}A_2)$, a 3-tuple consisting of a binary relation (or operation) $R$ with its two arguments. The “TP-N2F encoder” uses two LSTMs to produce a pair consisting of a filler vector and a role vector, which are bound together with the tensor product. These tensor products, concatenated, comprise the “context” over which attention will operate in the decoder. The sum of the word-level TPRs, flattened to a vector, is treated as a representation of the entire problem statement; it is fed to the “Reasoning MLP”, which transforms this encoding of the problem into a vector encoding the solution. This is the initial state of the “TP-N2F decoder” attentional LSTM, which outputs at each time step an order-3 tensor representing a relational tuple. To generate a correct tuple from decoder operations, the model must learn to give the order-3 tensor the form of a TPR for a $(R \hspace{2.84526pt}A_1 \hspace{2.84526pt}A_2)$ tuple (detailed explanation in Sec. SECREF7). In the following sections, we first introduce the details of our proposed role-level description for N2F tasks, and then present how our proposed TP-N2F model uses TPR binding and unbinding operations to create a neural network implementation of this description of N2F tasks.

## TP-N2F Model ::: Role-level description of N2F tasks

In this section, we propose a role-level description of N2F tasks, which specifies the filler/role structures of the input natural-language symbolic expressions and the output relational representations.

## TP-N2F Model ::: Role-level description of N2F tasks ::: Role-level description for natural-language input

Instead of encoding each token of a sentence with a non-compositional embedding vector looked up in a learned dictionary, we use a learned role-filler decomposition to compose a tensor representation for each token. Given a sentence $S$ with $n$ word tokens $\lbrace w^0,w^1,...,w^{n-1}\rbrace $, each word token $w^t$ is assigned a learned role vector $^t$, soft-selected from the learned dictionary $$, and a learned filler vector $^t$, soft-selected from the learned dictionary $$ (Sec. SECREF2). The mechanism closely follows that of BIBREF7, and we hypothesize similar results: the role and filler approximately encode the grammatical role of the token and its lexical semantics, respectively. Then each word token $w^t$ is represented by the tensor product of the role vector and the filler vector: $^t=^t \otimes ^t$. In addition to the set of all its token embeddings $\lbrace ^0, \ldots , ^{n-1} \rbrace $, the sentence $S$ as a whole is assigned a TPR equal to the sum of the TPR embeddings of all its word tokens: $_S = \sum _{t=0}^{n-1} ^t$.

Using TPRs to encode natural language has several advantages. First, natural language TPRs can be interpreted by exploring the distribution of tokens grouped by the role and filler vectors they are assigned by a trained model (as in BIBREF7). Second, TPRs avoid the Bag of Word (BoW) confusion BIBREF8: the BoW encoding of Jay saw Kay is the same as the BoW encoding of Kay saw Jay but the encodings are different with TPR embedding, because the role filled by a symbol changes with its context.

## TP-N2F Model ::: Role-level description of N2F tasks ::: Role-level description for relational representations

In this section, we propose a novel recursive role-level description for representing symbolic relational tuples. Each relational tuple contains a relation token and multiple argument tokens. Given a binary relation $rel$, a relational tuple can be written as $(rel \hspace{2.84526pt}arg_1 \hspace{2.84526pt}arg_2)$ where $arg_1,arg_2$ indicate two arguments of relation $rel$. Let us adopt the two positional roles, $p_i^{rel} = $ arg$_i$-of-$rel$ for $i=1,2$. The filler of role $p_i^{rel}$ is $arg_i$. Now let us use role decomposition recursively, noting that the role $p_i^{rel}$ can itself be decomposed into a sub-role $p_i = $ arg$_i$-of-$\underline{\hspace{5.69054pt}}$ which has a sub-filler $rel$. Suppose that $arg_i, rel, p_i$ are embedded as vectors $_i, , _i$. Then the TPR encoding of $p_i^{rel}$ is $_{rel} \otimes _i$, so the TPR encoding of filler $arg_i$ bound to role $p_i^{rel}$ is $_i \otimes (_{rel} \otimes _i)$. The tensor product is associative, so we can omit parentheses and write the TPR for the formal-language expression, the relational tuple $(rel \hspace{2.84526pt}arg_1 \hspace{2.84526pt}arg_2)$, as: = 1 rel 1 + 2 rel 2. Given the unbinding vectors $^{\prime }_i$ for positional role vectors $_i$ and the unbinding vector $^{\prime }_{rel}$ for the vector $_{rel}$ that embeds relation $rel$, each argument can be unbound in two steps as shown in Eqs. SECREF7–SECREF7. i' = [ 1 rel 1 + 2 rel 2 ] i' = i rel

[ i rel ] 'rel = i Here $\cdot $ denotes the tensor inner product, which for the order-3 $$ and order-1 $^{\prime }_i$ in Eq. SECREF7 can be defined as $[\cdot ^{\prime }_i]_{jk} = \sum _l []_{jkl} [^{\prime }_i]_l$; in Eq. SECREF7, $\cdot $ is equivalent to the matrix-vector product.

Our proposed scheme can be contrasted with the TPR scheme in which $(rel \hspace{2.84526pt}arg_1 \hspace{2.84526pt}arg_2)$ is embedded as $_{rel} \otimes _1 \otimes _2$ (e.g., BIBREF11, BIBREF12). In that scheme, an $n$-ary-relation tuple is embedded as an order-($n+1$) tensor, and unbinding an argument requires knowing all the other arguments (to use their unbinding vectors). In the scheme proposed here, an $n$-ary-relation tuple is still embedded as an order-3 tensor: there are just $n$ terms in the sum in Eq. SECREF7, using $n$ position vectors $_1, \dots , _n$; unbinding simply requires knowing the unbinding vectors for these fixed position vectors.

In the model, the order-3 tensor $$ of Eq. SECREF7 has a different status than the order-2 tensor $_S$ of Sec. SECREF5. $_S$ is a TPR by construction, whereas $$ is a TPR as a result of successful learning. To generate the output relational tuples, the decoder assumes each tuple has the form of Eq. SECREF7, and performs the unbinding operations which that structure calls for. In Appendix Sec. SECREF65, it is shown that, if unbinding each of a set of roles from some unknown tensor $$ gives a target set of fillers, then $$ must equal the TPR generated by those role/filler pairs, plus some tensor that is irrelevant because unbinding from it produces the zero vector. In other words, if the decoder succeeds in producing filler vectors that correspond to output relational tuples that match the target, then, as far as what the decoder can see, the tensor that it operates on is the TPR of Eq. SECREF7.

## TP-N2F Model ::: Role-level description of N2F tasks ::: The TP-N2F Scheme for Learning the input-output mapping

To generate formal relational tuples from natural-language descriptions, a learning strategy for the mapping between the two structures is particularly important. As shown in (SECREF8), we formalize the learning scheme as learning a mapping function $f_{\mathrm {mapping}}(\cdot )$, which, given a structural representation of the natural-language input, $_S$, outputs a tensor $_F$ from which the structural representation of the output can be generated. At the role level of description, there's nothing more to be said about this mapping; how it is modeled at the neural network level is discussed in Sec. SECREF10. F = fmapping(S)

## TP-N2F Model ::: The TP-N2F Model for Natural- to Formal-Language Generation

As shown in Figure FIGREF3, the TP-N2F model is implemented with three steps: encoding, mapping, and decoding. The encoding step is implemented by the TP-N2F natural-language encoder (TP-N2F Encoder), which takes the sequence of word tokens as inputs, and encodes them via TPR binding according to the TP-N2F role scheme for natural-language input given in Sec. SECREF5. The mapping step is implemented by an MLP called the Reasoning Module, which takes the encoding produced by the TP-N2F Encoder as input. It learns to map the natural-language-structure encoding of the input to a representation that will be processed under the assumption that it follows the role scheme for output relational-tuples specified in Sec. SECREF7: the model needs to learn to produce TPRs such that this processing generates correct output programs. The decoding step is implemented by the TP-N2F relational tuples decoder (TP-N2F Decoder), which takes the output from the Reasoning Module (Sec. SECREF8) and decodes the target sequence of relational tuples via TPR unbinding. The TP-N2F Decoder utilizes an attention mechanism over the individual-word TPRs $^t$ produced by the TP-N2F Encoder. The detailed implementations are introduced below.

## TP-N2F Model ::: The TP-N2F Model for Natural- to Formal-Language Generation ::: The TP-N2F natural-language Encoder

The TP-N2F encoder follows the role scheme in Sec. SECREF5 to encode each word token $w^t$ by soft-selecting one of $n_{\mathrm {F}}$ fillers and one of $n_{\mathrm {R}}$ roles. The fillers and roles are embedded as vectors. These embedding vectors, and the functions for selecting fillers and roles, are learned by two LSTMs, the Filler-LSTM and the Role-LSTM. (See Figure FIGREF11.) At each time-step $t$, the Filler-LSTM and the Role-LSTM take a learned word-token embedding $^t$ as input. The hidden state of the Filler-LSTM, $_{\mathrm {F}}^t$, is used to compute softmax scores $u_k^{\mathrm {F}}$ over $n_{\mathrm {F}}$ filler slots, and a filler vector $^{t} = ^{\mathrm {F}}$ is computed from the softmax scores (recall from Sec. SECREF2 that $$ is the learned matrix of filler vectors). Similarly, a role vector is computed from the hidden state of the Role-LSTM, $_{\mathrm {R}}^t$. $f_{\mathrm {F}}$ and $f_{\mathrm {R}}$ denote the functions that generate $^{t}$ and $^t$ from the hidden states of the two LSTMs. The token $w^t$ is encoded as $^t$, the tensor product of $^{t}$ and $^t$. $^t$ replaces the hidden vector in each LSTM and is passed to the next time step, together with the LSTM cell-state vector $^t$: see (SECREF10)–(SECREF10). After encoding the whole sequence, the TP-N2F encoder outputs the sum of all tensor products $\sum _t ^t$ to the next module. We use an MLP, called the Reasoning MLP, for TPR mapping; it takes an order-2 TPR from the encoder and maps it to the initial state of the decoder. Detailed equations and implementation are provided in Sec. SECREF22 of the Appendix. Ft = fFiller-LSTM(t,t-1, Ft-1) Rt = fRole-LSTM(t,t-1, Rt-1)

t = t t = fF(Ft) fR(Rt)

## TP-N2F Model ::: The TP-N2F Model for Natural- to Formal-Language Generation ::: The TP-N2F Relational-Tuple Decoder

The TP-N2F Decoder is an RNN that takes the output from the reasoning MLP as its initial hidden state for generating a sequence of relational tuples (Figure FIGREF13). This decoder contains an attentional LSTM called the Tuple-LSTM which feeds an unbinding module: attention operates on the context vector of the encoder, consisting of all individual encoder outputs $\lbrace ^t \rbrace $. The hidden-state $$ of the Tuple-LSTM is treated as a TPR of a relational tuple and is unbound to a relation and arguments. During training, the Tuple-LSTM needs to learn a way to make $$ suitably approximate a TPR. At each time step $t$, the hidden state $^t$ of the Tuple-LSTM with attention (The version in BIBREF13) (SECREF12) is fed as input to the unbinding module, which regards $^t$ as if it were the TPR of a relational tuple with $m$ arguments possessing the role structure described in Sec. SECREF7: $^t \approx \sum _{i=1}^{m} _{i}^t \otimes _{rel}^t \otimes _i$. (In Figure FIGREF13, the assumed hypothetical form of $^t$, as well as that of $_i^t$ below, is shown in a bubble with dashed border.) To decode a binary relational tuple, the unbinding module decodes it from $^t$ using the two steps of TPR unbinding given in (SECREF7)–(SECREF7). The positional unbinding vectors $^{\prime }_{i}$ are learned during training and shared across all time steps. After the first unbinding step (SECREF7), i.e., the inner product of $^t$ with $^{\prime }_i$, we get tensors $_{i}^t$ (SECREF12). These are treated as the TPRs of two arguments $_i^t$ bound to a relation $_{rel}^t$. A relational unbinding vector $_{rel}^{\prime t}$ is computed by a linear function from the sum of the $_{i}^t$ and used to compute the inner product with each $_i^t$ to yield $_i^t$, which are treated as the embedding of argument vectors (SECREF12). Based on the TPR theory, $_{rel}^{\prime t}$ is passed to a linear function to get $_{rel}^t$ as the embedding of a relation vector. Finally, the softmax probability distribution over symbolic outputs is computed for relations and arguments separately. In generation, the most probable symbol is selected. (Detailed equations are in Appendix Sec. SECREF42) t = Atten(fTuple-LSTM(relt,arg1t,arg2t,t-1,ct-1),[0,...,n-1])

1t = t 1' 2t = t 2'

rel't = flinear(1t + 2t) 1t = 1t rel't 2t = 2t rel't

## TP-N2F Model ::: Inference and The Learning Strategy of the TP-N2F Model

During inference time, natural language questions are encoded via the encoder and the Reasoning MLP maps the output of the encoder to the input of the decoder. We use greedy decoding (selecting the most likely class) to decode one relation and its arguments. The relation and argument vectors are concatenated to construct a new vector as the input for the Tuple-LSTM in the next step.

TP-N2F is trained using back-propagation BIBREF14 with the Adam optimizer BIBREF15 and teacher-forcing. At each time step, the ground-truth relational tuple is provided as the input for the next time step. As the TP-N2F decoder decodes a relational tuple at each time step, the relation token is selected only from the relation vocabulary and the argument tokens from the argument vocabulary. For an input ${\mathcal {I}}$ that generates $N$ output relational tuples, the loss is the sum of the cross entropy loss ${\mathcal {L}}$ between the true labels $L$ and predicted tokens for relations and arguments as shown in (SECREF14). LI = i=0N-1L(reli, Lreli) + i=0N-1j=12L(argji, Largji)

## EXPERIMENTS

The proposed TP-N2F model is evaluated on two N2F tasks, generating operation sequences to solve math problems and generating Lisp programs. In both tasks, TP-N2F achieves state-of-the-art performance. We further analyze the behavior of the unbinding relation vectors in the proposed model. Results of each task and the analysis of the unbinding relation vectors are introduced in turn. Details of experiments and datasets are described in Sec. SECREF20 in the Appendix.

## EXPERIMENTS ::: Generating operation sequences to solve math problems

Given a natural-language math problem, we need to generate a sequence of operations (operators and corresponding arguments) from a set of operators and arguments to solve the given problem. Each operation is regarded as a relational tuple by viewing the operator as relation, e.g., $(add, n1, n2)$. We test TP-N2F for this task on the MathQA dataset BIBREF16. The MathQA dataset consists of about 37k math word problems, each with a corresponding list of multi-choice options and the corresponding operation sequence. In this task, TP-N2F is deployed to generate the operation sequence given the question. The generated operations are executed with the execution script from BIBREF16 to select a multi-choice answer. As there are about 30% noisy data (where the execution script returns the wrong answer when given the ground-truth program; see Sec. SECREF20 of the Appendix), we report both execution accuracy (of the final multi-choice answer after running the execution engine) and operation sequence accuracy (where the generated operation sequence must match the ground truth sequence exactly). TP-N2F is compared to a baseline provided by the seq2prog model in BIBREF16, an LSTM-based seq2seq model with attention. Our model outperforms both the original seq2prog, designated SEQ2PROG-orig, and the best reimplemented seq2prog after an extensive hyperparameter search, designated SEQ2PROG-best. Table TABREF16 presents the results. To verify the importance of the TP-N2F encoder and decoder, we conducted experiments to replace either the encoder with a standard LSTM (denoted LSTM2TP) or the decoder with a standard attentional LSTM (denoted TP2LSTM). We observe that both the TPR components of TP-N2F are important for achieving the observed performance gain relative to the baseline.

## EXPERIMENTS ::: Generating program trees from natural-language descriptions

Generating Lisp programs requires sensitivity to structural information because Lisp code can be regarded as tree-structured. Given a natural-language query, we need to generate code containing function calls with parameters. Each function call is a relational tuple, which has a function as the relation and parameters as arguments. We evaluate our model on the AlgoLisp dataset for this task and achieve state-of-the-art performance. The AlgoLisp dataset BIBREF17 is a program synthesis dataset. Each sample contains a problem description, a corresponding Lisp program tree, and 10 input-output testing pairs. We parse the program tree into a straight-line sequence of tuples (same style as in MathQA). AlgoLisp provides an execution script to run the generated program and has three evaluation metrics: the accuracy of passing all test cases (Acc), the accuracy of passing 50% of test cases (50p-Acc), and the accuracy of generating an exactly matching program (M-Acc). AlgoLisp has about 10% noisy data (details in the Appendix), so we report results both on the full test set and the cleaned test set (in which all noisy testing samples are removed). TP-N2F is compared with an LSTM seq2seq with attention model, the Seq2Tree model in BIBREF17, and a seq2seq model with a pre-trained tree decoder from the Tree2Tree autoencoder (SAPS) reported in BIBREF18. As shown in Table TABREF18, TP-N2F outperforms all existing models on both the full test set and the cleaned test set. Ablation experiments with TP2LSTM and LSTM2TP show that, for this task, the TP-N2F Decoder is more helpful than TP-N2F Encoder. This may be because lisp codes rely more heavily on structure representations.

## EXPERIMENTS ::: Interpretation of learned structure

To interpret the structure learned by the model, we extract the trained unbinding relation vectors from the TP-N2F Decoder and reduce the dimension of vectors via Principal Component Analysis. K-means clustering results on the average vectors are presented in Figure FIGREF71 and Figure FIGREF72 (in Appendix A.6). Results show that unbinding vectors for operators or functions with similar semantics tend to be close to each other. For example, with 5 clusters in the MathQA dataset, arithmetic operators such as add, subtract, multiply, divide are clustered together, and operators related to square or volume of geometry are clustered together. With 4 clusters in the AlgoLisp dataset, partial/lambda functions and sort functions are in one cluster, and string processing functions are clustered together. Note that there is no direct supervision to inform the model about the nature of the operations, and the TP-N2F decoder has induced this role structure using weak supervision signals from question/operation-sequence-answer pairs. More clustering results are presented in the Appendix A.6.

## Related work

N2F tasks include many different subtasks such as symbolic reasoning or semantic parsing BIBREF19, BIBREF20, BIBREF21, BIBREF16, BIBREF17, BIBREF18. These tasks require models with strong structure-learning ability. TPR is a promising technique for encoding symbolic structural information and modeling symbolic reasoning in vector space. TPR binding has been used for encoding and exploring grammatical structural information of natural language BIBREF7, BIBREF9. TPR unbinding has also been used to generate natural language captions from images BIBREF8. Some researchers use TPRs for modeling deductive reasoning processes both on a rule-based model and deep learning models in vector space BIBREF22, BIBREF11, BIBREF12. However, none of these previous models takes advantage of combining TPR binding and TPR unbinding to learn structure representation mappings explicitly, as done in our model. Although researchers are paying increasing attention to N2F tasks, most of the proposed models either do not encode structural information explicitly or are specialized to particular tasks. Our proposed TP-N2F neural model can be applied to many tasks.

## CONCLUSION AND FUTURE WORK

In this paper we propose a new scheme for neural-symbolic relational representations and a new architecture, TP-N2F, for formal-language generation from natural-language descriptions. To our knowledge, TP-N2F is the first model that combines TPR binding and TPR unbinding in the encoder-decoder fashion. TP-N2F achieves the state-of-the-art on two instances of N2F tasks, showing significant structure learning ability. The results show that both the TP-N2F encoder and the TP-N2F decoder are important for improving natural- to formal-language generation. We believe that the interpretation and symbolic structure encoding of TPRs are a promising direction for future work. We also plan to combine large-scale deep learning models such as BERT with TP-N2F to take advantage of structure learning for other generation tasks.

## Appendix ::: Implementations of TP-N2F for experiments

In this section, we present details of the experiments of TP-N2F on the two datasets. We present the implementation of TP-N2F on each dataset.

The MathQA dataset consists of about 37k math word problems ((80/12/8)% training/dev/testing problems), each with a corresponding list of multi-choice options and an straight-line operation sequence program to solve the problem. An example from the dataset is presented in the Appendix A.4. In this task, TP-N2F is deployed to generate the operation sequence given the question. The generated operations are executed to generate the solution for the given math problem. We use the execution script from BIBREF16 to execute the generated operation sequence and compute the multi-choice accuracy for each problem. During our experiments we observed that there are about 30% noisy examples (on which the execution script fails to get the correct answer on the ground truth program). Therefore, we report both execution accuracy (the final multi-choice answer after running the execution engine) and operation sequence accuracy (where the generated operation sequence must match the ground truth sequence exactly).

The AlgoLisp dataset BIBREF17 is a program synthesis dataset, which has 79k/9k/10k training/dev/testing samples. Each sample contains a problem description, a corresponding Lisp program tree, and 10 input-output testing pairs. We parse the program tree into a straight-line sequence of commands from leaves to root and (as in MathQA) use the symbol $\#_i$ to indicate the result of the $i^{\mathrm {th}}$ command (generated previously by the model). A dataset sample with our parsed command sequence is presented in the Appendix A.4. AlgoLisp provides an execution script to run the generated program and has three evaluation metrics: accuracy of passing all test cases (Acc), accuracy of passing 50% of test cases (50p-Acc), and accuracy of generating an exactly matched program (M-Acc). AlgoLisp has about 10% noise data (where the execution script fails to pass all test cases on the ground truth program), so we report results both on the full test set and the cleaned test set (in which all noisy testing samples are removed).

We use $d_{\mathrm {R}}, n_{\mathrm {R}}, d_{\mathrm {F}}, n_{\mathrm {F}}$ to indicate the TP-N2F encoder hyperparameters, the dimension of role vectors, the number of roles, the dimension of filler vectors and the number of fillers. $d_{Rel}, d_{Arg},d_{Pos}$ indicate the TP-N2F decoder hyper-parameters, the dimension of relation vectors, the dimension of argument vectors, and the dimension of position vectors.

In the experiment on the MathQA dataset, we use $n_{\mathrm {F}}= 150$, $n_{\mathrm {R}}= 50$, $d_{\mathrm {F}}= 30$, $d_{\mathrm {R}}= 20$, $d_{Rel} = 20$, $d_{Arg} = 10$, $d_{Pos} = 5$ and we train the model for 60 epochs with learning rate 0.00115. The reasoning module only contains one layer. As most of the math operators in this dataset are binary, we replace all operators taking three arguments with a set of binary operators based on hand-encoded rules, and for all operators taking one argument, a padding symbol is appended. For the baseline SEQ2PROG-orig, TP2LSTM and LSTM2TP, we use hidden size 100, single-direction, one-layer LSTM. For the SEQ2PROG-best, we performed a hyperparameter search on the hidden size for both encoder and decoder; the best score is reported.

In the experiment on the AlgoLisp dataset, we use $n_{\mathrm {F}}= 150$, $n_{\mathrm {R}}= 50$, $d_{\mathrm {F}}= 30$, $d_{\mathrm {R}}= 30$, $d_{Rel} = 30$, $d_{Arg} = 20$, $d_{Pos} = 5$ and we train the model for 50 epochs with learning rate 0.00115. We also use one-layer in the reasoning module like in MathQA. For this dataset, most function calls take three arguments so we simply add padding symbols for those functions with fewer than three arguments.

## Appendix ::: Detailed equations of TP-N2F ::: TP-N2F encoder

Filler-LSTM in TP-N2F encoder

This is a standard LSTM, governed by the equations:

$\varphi , \tanh $ are the logistic sigmoid and tanh functions applied elementwise. $\flat $ flattens (reshapes) a matrix in $^{d_{\mathrm {F}} \times d_{\mathrm {R}}}$ into a vector in $^{d_{\mathrm {T}}}$, where $d_{\mathrm {T}} = d_{\mathrm {F}} d_{\mathrm {R}}$. $\odot $ is elementwise multiplication. The variables have the following dimensions: ft, ft, ft, ft, ft, ft, ff, fg, fi, fo, ♭(t-1) RdT

wt Rd

ff, fg, fi, fo RdT d

ff, fg, fi, fo RdT dT

Filler vector

The filler vector for input token $w^t$ is $^t$, defined through an attention vector over possible fillers, $_{\mathrm {f}}^t$:

($W_{\mathrm {f}}$ is the same as $$ of Sec. SECREF2.) The variables' dimensions are: fa RnF dT

ft RnF

f RdF nF

t RdF $T$ is the temperature factor, which is fixed at 0.1.

Role-LSTM in TP-N2F encoder

Similar to the Filler-LSTM, the Role-LSTM is also a standard LSTM, governed by the equations:

The variable dimensions are: rt, rt, rt, rt, rt, rt, rf, rg, ri, ro, ♭(t-1) RdT

wt Rd

rf, rg, ri, ro RdT d

rf, rg, ri, ro RdT dT

Role vector

The role vector for input token $w^t$ is determined analogously to its filler vector:

The dimensions are: ra RnR dT

rt RnR

r RdR nR

t RdR

Binding

The TPR for the filler/role binding for token $w^t$ is then:

where t RdR dF

## Appendix ::: Detailed equations of TP-N2F ::: Structure Mapping

$^0 \in \mathbb {R}^{d_{\mathrm {H}}}$, where $d_{\mathrm {H}} = d_{\mathrm {A}}, d_{\mathrm {O}}, d_{\mathrm {P}}$ are dimension of argument vector, operator vector and position vector. $f_{\mathrm {mapping}}$ is implemented with a MLP (linear layer followed by a tanh) for mapping the $_t \in \mathbb {R}^{d_{\mathrm {T}}}$ to the initial state of decoder $^0$.

## Appendix ::: Detailed equations of TP-N2F ::: TP-N2F decoder

Tuple-LSTM

The output tuples are also generated via a standard LSTM:

Here, $\gamma $ is the concatenation function. $_{Rel}^{t-1}$ is the trained embedding vector for the Relation of the input binary tuple, $_{Arg1}^{t-1}$ is the embedding vector for the first argument and $_{Arg2}^{t-1}$ is the embedding vector for the second argument. Then the input for the Tuple LSTM is the concatenation of the embedding vectors of relation and arguments, with dimension $d_{\mathrm {dec}}$. t, t, t, t, t, inputt, f, g, i, o, ♭(t-1) RdH

dt Rddec

f, g, i, o RdH ddec

f, g, i, o RdH dH

t RdH ${\mathrm {Atten}}$ is the attention mechanism used in BIBREF13, which computes the dot product between $_{\mathrm {input}}^t$ and each $_{t^{\prime }}$. Then a linear function is used on the concatenation of $_{\mathrm {input}}^t$ and the softmax scores on all dot products to generate $^t$. The following equations show the attention mechanism:

${\mathrm {score}}$ is the score function of the attention. In this paper, the score function is dot product. T RdH n

t Rn

t RdH

RdH (dT+n)

Unbinding

At each timestep $t$, the 2-step unbinding process described in Sec. SECREF7 operates first on an encoding of the triple as a whole, $$, using two unbinding vectors $_i^{\prime }$ that are learned but fixed for all tuples. This first unbinding gives an encoding of the two operator-argument bindings, $_i$. The second unbinding operates on the $_i$, using a generated unbinding vector for the operator, $_{rel}^{\prime }$, giving encodings of the arguments, $_i$. The generated unbinding vector for the operator, $^{\prime }$, and the generated encodings of the arguments, $_i$, each produce a probability distribution over symbolic operator outputs $Rel$ and symbolic argument outputs $Arg_i$; these probabilities are used in the cross-entropy loss function. For generating a single symbolic output, the most-probable symbols are selected.

The dimensions are: rel't RdO

1t, 2t RdA

'1, '2 RdP

1t, 2t RdA dO

dual RdH

rt RnO dO

at RnA dA

rt RnR

a1t, a2t RnA

## Appendix ::: The tensor that is input to the decoder's Unbinding Module is a TPR

Here we show that, if learning is successful, the order-3 tensor $$ that each iteration of the decoder's Tuple LSTM feeds to the decoder's Unbinding Module (Figure FIGREF13) will be a TPR of the form assumed in Eq. SECREF7, repeated here: = j j rel j. The operations performed by the decoder are given in Eqs. SECREF7–SECREF7, and Eqs. SECREF12–SECREF12, rewritten here: i' = i

i rel' = i This is the standard TPR unbinding operation, used recursively: first with the unbinding vectors for positions, $_i^{\prime }$, then with the unbinding vector for the operator, $_{rel}^{\prime }$. It therefore suffices to analyze a single unbinding; the result can then be used recursively. This in effect reduces the problem to the order-2 case. What we will show is: given a set of unbinding vectors $\lbrace _i^{\prime } \rbrace $ which are dual to a set of role vectors $\lbrace _i \rbrace $, with $i$ ranging over some index set $I$, if $$ is an order-2 tensor such that 'i = i, i I then = i I i i + TPR + for some tensor $$ that annihilates all the unbinding vectors: 'i = 0, i I. If learning is successful, the processing in the decoder will generate the target relational tuple $(R, A_1, A_2)$ by obeying Eq. SECREF65 in the first unbinding, where we have $_i^{\prime } = _i^{\prime }, _i = _i, I = \lbrace 1, 2\rbrace $, and obeying Eq. SECREF65 in the second unbinding, where we have $_i^{\prime } = _{rel}^{\prime }, _i^{\prime } = _i$, with $I =$ the set containing only the null index.

Treat rank-2 tensors as matrices; then unbinding is simply matrix-vector multiplication. Assume the set of unbinding vectors is linearly independent (otherwise there would in general be no way to satisfy Eq. SECREF65 exactly, contrary to assumption). Then expand the set of unbinding vectors, if necessary, into a basis $\lbrace ^{\prime }_k\rbrace _{k \in K \supseteq I}$. Find the dual basis, with $_k$ dual to $^{\prime }_k$ (so that $_l^\top _j^{\prime } = \delta _{lj}$). Because $\lbrace ^{\prime }_k\rbrace _{k \in K}$ is a basis, so is $\lbrace _k\rbrace _{k \in K}$, so any matrix $$ can be expanded as $= \sum _{k \in K} _k _k^{\top }$. Since $^{\prime }_i = _i, \forall i \in I$ are the unbinding conditions (Eq. SECREF65), we must have $_i = _i, i \in I$. Let $_{{\mathrm {TPR}}} \equiv \sum _{i \in I} _i _i^{\top }$. This is the desired TPR, with fillers $_i$ bound to the role vectors $_i$ which are the duals of the unbinding vectors $_i^{\prime }$ ($i \in I$). Then we have $= _{{\mathrm {TPR}}} + $ (Eq. SECREF65) where $\equiv \sum _{j \in K, j \notin I} _j _j^{\top }$; so $_i^{\prime } = {\mathbf {0}}, i \in I$ (Eq. SECREF65). Thus, if training is successful, the model must have learned how to feed the decoder with order-3 TPRs with the structure posited in Eq. SECREF65.

The argument so far addresses the case where the unbinding vectors are linearly independent, making it possible to satisfy Eq. SECREF65 exactly. In relatively high-dimensional vector spaces, it will often happen that even when the number of unbinding vectors exceeds the dimension of their space by a factor of 2 or 3 (which applies to the TP-N2F models presented here), there is a set of role vectors $\lbrace _k \rbrace _{k \in K}$ approximately dual to $\lbrace ^{\prime }_k \rbrace _{k \in K}$, such that $_l^\top _j^{\prime } = \delta _{lj} \hspace{2.84526pt}\forall l, j \in K$ holds to a good approximation. (If the distribution of normalized unbinding vectors is approximately uniform on the unit sphere, then choosing the approximate dual vectors to equal the unbinding vectors themselves will do, since they will be nearly orthonormal BIBREF10. If the $\lbrace ^{\prime }_k \rbrace _{k \in K}$ are not normalized, we just rescale the role vectors, choosing $_k = _k^{\prime } / \Vert _k^{\prime } \Vert ^2$.) When the number of such role vectors exceeds the dimension of the embedding space, they will be overcomplete, so while it is still true that any matrix $$ can be expanded as above ($= \sum _{k \in K} _k _k^{\top }$), this expansion will no longer be unique. So while it remains true that $$ a TPR, it is no longer uniquely decomposable into filler/role pairs. The claim above does not claim uniqueness in this sense, and remains true.)

## Appendix ::: Dataset samples ::: Data sample from MathQA dataset

Problem: The present polulation of a town is 3888. Population increase rate is 20%. Find the population of town after 1 year?

Options: a) 2500, b) 2100, c) 3500, d) 3600, e) 2700

Operations: multiply(n0,n1), divide(#0,const-100), add(n0,#1)

## Appendix ::: Dataset samples ::: Data sample from AlgoLisp dataset

Problem: Consider an array of numbers and a number, decrements each element in the given array by the given number, what is the given array?

Program Nested List: (map a (partial1 b –))

Command-Sequence: (partial1 b –), (map a #0)

## Appendix ::: Generated programs comparison

In this section, we display some generated samples from the two datasets, where the TP-N2F model generates correct programs but LSTM-Seq2Seq does not.

Question: A train running at the speed of 50 km per hour crosses a post in 4 seconds. What is the length of the train?

TP-N2F(correct):

(multiply,n0,const1000) (divide,#0,const3600) (multiply,n1,#1)

LSTM(wrong):

(multiply,n0,const0.2778) (multiply,n1,#0)

Question: 20 is subtracted from 60 percent of a number, the result is 88. Find the number?

TP-N2F(correct):

(add,n0,n2) (divide,n1,const100) (divide,#0,#1)

LSTM(wrong):

(add,n0,n2) (divide,n1,const100) (divide,#0,#1) (multiply,#2,n3) (subtract,#3,n0)

Question: The population of a village is 14300. It increases annually at the rate of 15 percent. What will be its population after 2 years?

TP-N2F(correct):

(divide,n1,const100) (add,#0,const1) (power,#1,n2) (multiply,n0,#2)

LSTM(wrong):

(multiply,const4,const100) (sqrt,#0)

Question: There are two groups of students in the sixth grade. There are 45 students in group a, and 55 students in group b. If, on a particular day, 20 percent of the students in group a forget their homework, and 40 percent of the students in group b forget their homework, then what percentage of the sixth graders forgot their homework?

TP-N2F(correct):

(add,n0,n1) (multiply,n0,n2) (multiply,n1,n3) (divide,#1,const100) (divide,#2,const100) (add,#3,#4) (divide,#5,#0) (multiply,#6,const100)

LSTM(wrong):

(multiply,n0,n1) (subtract,n0,n1) (divide,#0,#1)

Question: 1 divided by 0.05 is equal to

TP-N2F(correct):

(divide,n0,n1)

LSTM(wrong):

(divide,n0,n1) (multiply,n2,#0)

Question: Consider a number a, compute factorial of a

TP-N2F(correct):

( <=,arg1,1 ) ( -,arg1,1 ) ( self,#1 ) ( *,#2,arg1 ) ( if,#0,1,#3 ) ( lambda1,#4 ) ( invoke1,#5,a )

LSTM(wrong):

( <=,arg1,1 ) ( -,arg1,1 ) ( self,#1 ) ( *,#2,arg1 ) ( if,#0,1,#3 ) ( lambda1,#4 ) ( len,a ) ( invoke1,#5,#6 )

Question: Given an array of numbers and numbers b and c, add c to elements of the product of elements of the given array and b, what is the product of elements of the given array and b?

TP-N2F(correct):

( partial, b,* ) ( partial1,c,+ ) ( map,a,#0 ) ( map,#2,#1 )

LSTM(wrong):

( partial1,b,+ ) ( partial1,c,+ ) ( map,a,#0 ) ( map,#2,#1 )

Question: You are given an array of numbers a and numbers b , c and d , let how many times you can replace the median in a with sum of its digits before it becomes a single digit number and b be the coordinates of one end and c and d be the coordinates of another end of segment e , your task is to find the length of segment e rounded down

TP-N2F(correct):

( digits arg1 ) ( len #0 ) ( == #1 1 ) ( digits arg1 ) ( reduce #3 0 + ) ( self #4 ) ( + 1 #5 ) ( if #2 0 #6 ) ( lambda1 #7 ) ( sort a ) ( len a ) ( / #10 2 ) ( deref #9 #11 ) ( invoke1 #8 #12 ) ( - #13 c ) ( digits arg1 ) ( len #15 ) ( == #16 1 ) ( digits arg1 ) ( reduce #18 0 + ) ( self #19 ) ( + 1 #20 ) ( if #17 0 #21 ) ( lambda1 #22 ) ( sort a ) ( len a ) ( / #25 2 ) ( deref #24 #26 ) ( invoke1 #23 #27 ) ( - #28 c ) ( * #14 #29 ) ( - b d ) ( - b d ) ( * #31 #32 ) ( + #30 #33 ) ( sqrt #34 ) ( floor #35 )

LSTM(wrong): ( digits arg1 ) ( len #0 ) ( == #1 1 ) ( digits arg1 ) ( reduce #3 0 + ) ( self #4 ) ( + 1 #5 ) ( if #2 0 #6 ) ( lambda1 #7 ) ( sort a ) ( len a ) ( / #10 2 ) ( deref #9 #11 ) ( invoke1 #8 #12 c ) ( - #13 ) ( - b d ) ( - b d ) ( * #15 #16 ) ( * #14 #17 ) ( + #18 ) ( sqrt #19 ) ( floor #20 )

Question: Given numbers a , b , c and e , let d be c , reverse digits in d , let a and the number in the range from 1 to b inclusive that has the maximum value when its digits are reversed be the coordinates of one end and d and e be the coordinates of another end of segment f , find the length of segment f squared

TP-N2F(correct):

( digits c ) ( reverse #0 ) ( * arg1 10 ) ( + #2 arg2 ) ( lambda2 #3 ) ( reduce #1 0 #4 ) ( - a #5 ) ( digits c ) ( reverse #7 ) ( * arg1 10 ) ( + #9 arg2 ) ( lambda2 #10 ) ( reduce #8 0 #11 ) ( - a #12 ) ( * #6 #13 ) ( + b 1 ) ( range 0 #15 ) ( digits arg1 ) ( reverse #17 ) ( * arg1 10 ) ( + #19 arg2 ) ( lambda2 #20 ) ( reduce #18 0 #21 ) ( digits arg2 ) ( reverse #23 ) ( * arg1 10 ) ( + #25 arg2 ) ( lambda2 #26 ) ( reduce #24 0 #27 ) ( > #22 #28 ) ( if #29 arg1 arg2 ) ( lambda2 #30 ) ( reduce #16 0 #31 ) ( - #32 e ) ( + b 1 ) ( range 0 #34 ) ( digits arg1 ) ( reverse #36 ) ( * arg1 10 ) ( + #38 arg2 ) ( lambda2 #39 ) ( reduce #37 0 #40 ) ( digits arg2 ) ( reverse #42 ) ( * arg1 10 ) ( + #44 arg2 ) ( lambda2 #45 ) ( reduce #43 0 #46 ) ( > #41 #47 ) ( if #48 arg1 arg2 ) ( lambda2 #49 ) ( reduce #35 0 #50 ) ( - #51 e ) ( * #33 #52 ) ( + #14 #53 )

LSTM(wrong):

( - a d ) ( - a d ) ( * #0 #1 ) ( digits c ) ( reverse #3 ) ( * arg1 10 ) ( + #5 arg2 ) ( lambda2 #6 ) ( reduce #4 0 #7 ) ( - #8 e ) ( + b 1 ) ( range 0 #10 ) ( digits arg1 ) ( reverse #12 ) ( * arg1 10 ) ( + #14 arg2 ) ( lambda2 #15 ) ( reduce #13 0 #16 ) ( digits arg2 ) ( reverse #18 ) ( * arg1 10 ) ( + #20 arg2 ) ( lambda2 #21 ) ( reduce #19 0 #22 ) ( > #17 #23 ) ( if #24 arg1 arg2 ) ( lambda2 #25 ) ( reduce #11 0 #26 ) ( - #27 e ) ( * #9 #28 ) ( + #2 #29 )

## Appendix ::: Unbinding relation vector clustering

We run K-means clustering on both datasets with $k = 3,4,5,6$ clusters and the results are displayed in Figure FIGREF71 and Figure FIGREF72. As described before, unbinding-vectors for operators or functions with similar semantics tend to be closer to each other. For example, in the MathQA dataset, arithmetic operators such as add, subtract, multiply, divide are clustered together at middle, and operators related to geometry such as square or volume are clustered together at bottom left. In AlgoLisp dataset, basic arithmetic functions are clustered at middle, and string processing functions are clustered at right.
