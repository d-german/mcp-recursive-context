# Enhance word representation for out-of-vocabulary on Ubuntu dialogue corpus

**Paper ID:** 1802.02614

## Abstract

Ubuntu dialogue corpus is the largest public available dialogue corpus to make it feasible to build end-to-end deep neural network models directly from the conversation data. One challenge of Ubuntu dialogue corpus is the large number of out-of-vocabulary words. In this paper we proposed a method which combines the general pre-trained word embedding vectors with those generated on the task-specific training set to address this issue. We integrated character embedding into Chen et al's Enhanced LSTM method (ESIM) and used it to evaluate the effectiveness of our proposed method. For the task of next utterance selection, the proposed method has demonstrated a significant performance improvement against original ESIM and the new model has achieved state-of-the-art results on both Ubuntu dialogue corpus and Douban conversation corpus. In addition, we investigated the performance impact of end-of-utterance and end-of-turn token tags.

## Introduction

The ability for a machine to converse with human in a natural and coherent manner is one of challenging goals in AI and natural language understanding. One problem in chat-oriented human-machine dialog system is to reply a message within conversation contexts. Existing methods can be divided into two categories: retrieval-based methods BIBREF0 , BIBREF1 , BIBREF2 and generation based methods BIBREF3 . The former is to rank a list of candidates and select a good response. For the latter, encoder-decoder framework BIBREF3 or statistical translation method BIBREF4 are usually used to generate a response. It is not easy to main the fluency of the generated texts.

Ubuntu dialogue corpus BIBREF5 is the public largest unstructured multi-turns dialogue corpus which consists of about one-million two-person conversations. The size of the corpus makes it attractive for the exploration of deep neural network modeling in the context of dialogue systems. Most deep neural networks use word embedding as the first layer. They either use fixed pre-trained word embedding vectors generated on a large text corpus or learn word embedding for the specific task. The former is lack of flexibility of domain adaptation. The latter requires a very large training corpus and significantly increases model training time. Word out-of-vocabulary issue occurs for both cases. Ubuntu dialogue corpus also contains many technical words (e.g. “ctrl+alt+f1", “/dev/sdb1"). The ubuntu corpus (V2) contains 823057 unique tokens whereas only 22% tokens occur in the pre-built GloVe word vectors. Although character-level representation which models sub-word morphologies can alleviate this problem to some extent BIBREF6 , BIBREF7 , BIBREF8 , character-level representation still have limitations: learn only morphological and orthographic similarity, other than semantic similarity (e.g. `car' and `bmw') and it cannot be applied to Asian languages (e.g. Chinese characters).

In this paper, we generate word embedding vectors on the training corpus based on word2vec BIBREF9 . Then we propose an algorithm to combine the generated one with the pre-trained word embedding vectors on a large general text corpus based on vector concatenation. The new word representation maintains information learned from both general text corpus and task-domain. The nice property of the algorithm is simplicity and little extra computational cost will be added. It can address word out-of-vocabulary issue effectively. This method can be applied to most NLP deep neural network models and is language-independent. We integrated our methods with ESIM(baseline model) BIBREF10 . The experimental results have shown that the proposed method has significantly improved the performance of original ESIM model and obtained state-of-the-art results on both Ubuntu Dialogue Corpus and Douban Conversation Corpus BIBREF11 . On Ubuntu Dialogue Corpus (V2), the improvement to the previous best baseline model (single) on INLINEFORM0 is 3.8% and our ensemble model on INLINEFORM1 is 75.9%. On Douban Conversation Corpus, the improvement to the previous best model (single) on INLINEFORM2 is 3.6%.

Our contributions in this paper are summarized below:

The rest paper is organized as follows. In Section SECREF2 , we review the related work. In Section SECREF3 we provide an overview of ESIM (baseline) model and describe our methods to address out-of-vocabulary issues. In Section SECREF4 , we conduct extensive experiments to show the effectiveness of the proposed method. Finally we conclude with remarks and summarize our findings and outline future research directions.

## Related work

Character-level representation has been widely used in information retrieval, tagging, language modeling and question answering. BIBREF12 represented a word based on character trigram in convolution neural network for web-search ranking. BIBREF7 represented a word by the sum of the vector representation of character n-gram. Santos et al BIBREF13 , BIBREF14 and BIBREF8 used convolution neural network to generate character-level representation (embedding) of a word. The former combined both word-level and character-level representation for part-of-speech and name entity tagging tasks while the latter used only character-level representation for language modeling. BIBREF15 employed a deep bidirectional GRU network to learn character-level representation and then concatenated word-level and character-level representation vectors together. BIBREF16 used a fine-grained gating mechanism to combine the word-level and character-level representation for reading comprehension. Character-level representation can help address out-of-vocabulary issue to some extent for western languages, which is mainly used to capture character ngram similarity.

The other work related to enrich word representation is to combine the pre-built embedding produced by GloVe and word2vec with structured knowledge from semantic network ConceptNet BIBREF17 and merge them into a common representation BIBREF18 . The method obtained very good performance on word-similarity evaluations. But it is not very clear how useful the method is for other tasks such as question answering. Furthermore, this method does not directly address out-of-vocabulary issue.

Next utterance selection is related to response selection from a set of candidates. This task is similar to ranking in search, answer selection in question answering and classification in natural language inference. That is, given a context and response pair, assign a decision score BIBREF19 . BIBREF1 formalized short-text conversations as a search problem where rankSVM was used to select response. The model used the last utterance (a single-turn message) for response selection. On Ubuntu dialogue corpus, BIBREF5 proposed Long Short-Term Memory(LSTM) BIBREF20 siamese-style neural architecture to embed both context and response into vectors and response were selected based on the similarity of embedded vectors. BIBREF21 built an ensemble of convolution neural network (CNN) BIBREF22 and Bi-directional LSTM. BIBREF19 employed a deep neural network structure BIBREF23 where CNN was applied to extract features after bi-directional LSTM layer. BIBREF24 treated each turn in multi-turn context as an unit and joined word sequence view and utterance sequence view together by deep neural networks. BIBREF11 explicitly used multi-turn structural info on Ubuntu dialogue corpus to propose a sequential matching method: match each utterance and response first on both word and sub-sequence levels and then aggregate the matching information by recurrent neural network.

The latest developments have shown that attention and matching aggregation are effective in NLP tasks such as question/answering and natural language inference. BIBREF25 introduced context-to-query and query-to-context attentions mechanisms and employed bi-directional LSTM network to capture the interactions among the context words conditioned on the query. BIBREF26 compared a word in one sentence and the corresponding attended word in the other sentence and aggregated the comparison vectors by summation. BIBREF10 enhanced local inference information by the vector difference and element-wise product between the word in premise an the attended word in hypothesis and aggregated local matching information by LSTM neural network and obtained the state-of-the-art results on the Stanford Natural Language Inference (SNLI) Corpus. BIBREF27 introduced several local matching mechanisms before aggregation, other than only word-by-word matching.

## Our model

In this section, we first review ESIM model BIBREF10 and introduce our modifications and extensions. Then we introduce a string matching algorithm for out-of-vocabulary words.

## ESIM model

In our notation, given a context with multi-turns INLINEFORM0 with length INLINEFORM1 and a response INLINEFORM2 with length INLINEFORM3 where INLINEFORM4 and INLINEFORM5 is the INLINEFORM6 th and INLINEFORM7 th word in context and response, respectively. For next utterance selection, the response is selected based on estimating a conditional probability INLINEFORM8 which represents the confidence of selecting INLINEFORM9 from the context INLINEFORM10 . Figure FIGREF6 shows high-level overview of our model and its details will be explained in the following sections.

Word Representation Layer. Each word in context and response is mapped into INLINEFORM0 -dimensional vector space. We construct this vector space with word-embedding and character-composed embedding. The character-composed embedding, which is newly introduced here and was not part of the original forumulation of ESIM, is generated by concatenating the final state vector of the forward and backward direction of bi-directional LSTM (BiLSTM). Finally, we concatenate word embedding and character-composed embedding as word representation.

Context Representation Layer. As in base model, context and response embedding vector sequences are fed into BiLSTM. Here BiLSTM learns to represent word and its local sequence context. We concatenate the hidden states at each time step for both directions as local context-aware new word representation, denoted by INLINEFORM0 and INLINEFORM1 for context and response, respectively. DISPLAYFORM0 

 where INLINEFORM0 is word vector representation from the previous layer.

Attention Matching Layer. As in ESIM model, the co-attention matrix INLINEFORM0 where INLINEFORM1 . INLINEFORM2 computes the similarity of hidden states between context and response. For each word in context, we find the most relevant response word by computing the attended response vector in Equation EQREF8 . The similar operation is used to compute attended context vector in Equation . DISPLAYFORM0 

 After the above attended vectors are calculated, vector difference and element-wise product are used to enrich the interaction information further between context and response as shown in Equation EQREF9 and . DISPLAYFORM0 

 where the difference and element-wise product are concatenated with the original vectors.

Matching Aggregation Layer. As in ESIM model, BiLSTM is used to aggregate response-aware context representation as well as context-aware response representation. The high-level formula is given by DISPLAYFORM0 

Pooling Layer. As in ESIM model, we use max pooling. Instead of using average pooling in the original ESIM model, we combine max pooling and final state vectors (concatenation of both forward and backward one) to form the final fixed vector, which is calculated as follows: DISPLAYFORM0 

Prediction Layer. We feed INLINEFORM0 in Equation into a 2-layer fully-connected feed-forward neural network with ReLu activation. In the last layer the sigmoid function is used. We minimize binary cross-entropy loss for training.

## Methods for out-of-vocabulary

Many pre-trained word embedding vectors on general large text-corpus are available. For domain-specific tasks, out-of-vocabulary may become an issue. Here we propose algorithm SECREF12 to combine pre-trained word vectors with those word2vec BIBREF9 generated on the training set. Here the pre-trainined word vectors can be from known methods such as GloVe BIBREF28 , word2vec BIBREF9 and FastText BIBREF7 .

[H] InputInputOutputOutput A dictionary with word embedding vectors of dimension INLINEFORM0 for INLINEFORM1 . INLINEFORM2 

 INLINEFORM0 INLINEFORM1 INLINEFORM2 INLINEFORM3 res INLINEFORM4 INLINEFORM5 INLINEFORM6 INLINEFORM7 res INLINEFORM8 res INLINEFORM9 Return res Combine pre-trained word embedding with those generated on training set.

where INLINEFORM0 is vector concatenation operator. The remaining words which are in INLINEFORM1 and are not in the above output dictionary are initialized with zero vectors. The above algorithm not only alleviates out-of-vocabulary issue but also enriches word embedding representation.

## Dataset

We evaluate our model on the public Ubuntu Dialogue Corpus V2 BIBREF29 since this corpus is designed for response selection study of multi turns human-computer conversations. The corpus is constructed from Ubuntu IRC chat logs. The training set consists of 1 million INLINEFORM0 triples where the original context and corresponding response are labeled as positive and negative response are selected randomly on the dataset. On both validation and test sets, each context contains one positive response and 9 negative responses. Some statistics of this corpus are presented in Table TABREF15 .

Douban conversation corpus BIBREF11 which are constructed from Douban group (a popular social networking service in China) is also used in experiments. Response candidates on the test set are collected by Lucene retrieval model, other than negative sampling without human judgment on Ubuntu Dialogue Corpus. That is, the last turn of each Douban dialogue with additional keywords extracted from the context on the test set was used as query to retrieve 10 response candidates from the Lucene index set (Details are referred to section 4 in BIBREF11 ). For the performance measurement on test set, we ignored samples with all negative responses or all positive responses. As a result, 6,670 context-response pairs were left on the test set. Some statistics of Douban conversation corpus are shown below:

## Implementation details

Our model was implemented based on Tensorflow BIBREF30 . ADAM optimization algorithm BIBREF31 was used for training. The initial learning rate was set to 0.001 and exponentially decayed during the training . The batch size was 128. The number of hidden units of biLSTM for character-level embedding was set to 40. We used 200 hidden units for both context representation layers and matching aggregation layers. In the prediction layer, the number of hidden units with ReLu activation was set to 256. We did not use dropout and regularization.

Word embedding matrix was initialized with pre-trained 300-dimensional GloVe vectors BIBREF28 . For character-level embedding, we used one hot encoding with 69 characters (68 ASCII characters plus one unknown character). Both word embedding and character embedding matrix were fixed during the training. After algorithm SECREF12 was applied, the remaining out-of-vocabulary words were initialized as zero vectors. We used Stanford PTBTokenizer BIBREF32 on the Ubuntu corpus. The same hyper-parameter settings are applied to both Ubuntu Dialogue and Douban conversation corpus. For the ensemble model, we use the average prediction output of models with different runs. On both corpuses, the dimension of word2vec vectors generated on the training set is 100.

## Overall Results

Since the output scores are used for ranking candidates, we use Recall@k (recall at position k in 10 candidates, denotes as R@1, R@2 below), P@1 (precision at position 1), MAP(mean average precision) BIBREF33 , MRR (Mean Reciprocal Rank) BIBREF34 to measure the model performance. Table TABREF23 and Table TABREF24 show the performance comparison of our model and others on Ubuntu Dialogue Corpus V2 and Douban conversation corpus, respectively.

On Douban conversation corpus, FastText BIBREF7 pre-trained Chinese embedding vectors are used in ESIM + enhanced word vector whereas word2vec generated on training set is used in baseline model (ESIM). It can be seen from table TABREF23 that character embedding enhances the performance of original ESIM. Enhanced Word representation in algorithm SECREF12 improves the performance further and has shown that the proposed method is effective. Most models (RNN, CNN, LSTM, BiLSTM, Dual-Encoder) which encode the whole context (or response) into compact vectors before matching do not perform well. INLINEFORM0 directly models sequential structure of multi utterances in context and achieves good performance whereas ESIM implicitly makes use of end-of-utterance(__eou__) and end-of-turn (__eot__) token tags as shown in subsection SECREF41 .

## Evaluation of several word embedding representations

In this section we evaluated word representation with the following cases on Ubuntu Dialogue corpus and compared them with that in algorithm SECREF12 .

Used the fixed pre-trained GloVe vectors .

Word embedding were initialized by GloVe vectors and then updated during the training.

Generated word2vec embeddings on the training set BIBREF9 and updated them during the training (dropout).

Used the pre-built ConceptNet NumberBatch BIBREF39 .

Used the fixed pre-built FastText vectors where word vectors for out-of-vocabulary words were computed based on built model.

Enhanced word representation in algorithm SECREF12 .

We used gensim to generate word2vec embeddings of dim 100.

It can be observed that tuning word embedding vectors during the training obtained the worse performance. The ensemble of word embedding from ConceptNet NumberBatch did not perform well since it still suffers from out-of-vocabulary issues. In order to get insights into the performance improvement of WP5, we show word coverage on Ubuntu Dialogue Corpus.

__eou__ and __eot__ are missing from pre-trained GloVe vectors. But this two tokens play an important role in the model performance shown in subsection SECREF41 . For word2vec generated on the training set, the unique token coverage is low. Due to the limited size of training corpus, the word2vec representation power could be degraded to some extent. WP5 combines advantages of both generality and domain adaptation.

## Evaluation of enhanced representation on a simple model

In order to check whether the effectiveness of enhanced word representation in algorithm SECREF12 depends on the specific model and datasets, we represent a doc (context, response or query) as the simple average of word vectors. Cosine similarity is used to rank the responses. The performances of the simple model on the test sets are shown in Figure FIGREF40 .

where WikiQA BIBREF40 is an open-domain question answering dataset from Microsoft research. The results on the enhanced vectors are better on the above three datasets. This indicates that enhanced vectors may fuse the domain-specific info into pre-built vectors for a better representation.

## The roles of utterance and turn tags

There are two special token tags (__eou__ and __eot__) on ubuntu dialogue corpus. __eot__ tag is used to denote the end of a user's turn within the context and __eou__ tag is used to denote of a user utterance without a change of turn. Table TABREF42 shows the performance with/without two special tags.

It can be observed that the performance is significantly degraded without two special tags. In order to understand how the two tags helps the model identify the important information, we perform a case study. We randomly selected a context-response pair where model trained with tags succeeded and model trained without tags failed. Since max pooling is used in Equations EQREF11 and , we apply max operator to each context token vector in Equation EQREF10 as the signal strength. Then tokens are ranked in a descending order by it. The same operation is applied to response tokens.

It can be seen from Table TABREF43 that __eou__ and __eot__ carry useful information. __eou__ and __eot__ captures utterance and turn boundary structure information, respectively. This may provide hints to design a better neural architecture to leverage this structure information.

## Conclusion and future work

We propose an algorithm to combine pre-trained word embedding vectors with those generated on training set as new word representation to address out-of-vocabulary word issues. The experimental results have shown that the proposed method is effective to solve out-of-vocabulary issue and improves the performance of ESIM, achieving the state-of-the-art results on Ubuntu Dialogue Corpus and Douban conversation corpus. In addition, we investigate the performance impact of two special tags: end-of-utterance and end-of-turn. In the future, we may design a better neural architecture to leverage utterance structure in multi-turn conversations.
