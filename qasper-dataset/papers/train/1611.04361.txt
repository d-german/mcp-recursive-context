# Attending to Characters in Neural Sequence Labeling Models

**Paper ID:** 1611.04361

## Abstract

Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.

## Introduction

 This work is licenced under a Creative Commons Attribution 4.0 International Licence.

Licence details: http://creativecommons.org/licenses/by/4.0/

Many NLP tasks, including named entity recognition (NER), part-of-speech (POS) tagging and shallow parsing can be framed as types of sequence labeling. The development of accurate and efficient sequence labeling models is thereby useful for a wide range of downstream applications. Work in this area has traditionally involved task-specific feature engineering – for example, integrating gazetteers for named entity recognition, or using features from a morphological analyser in POS-tagging. Recent developments in neural architectures and representation learning have opened the door to models that can discover useful features automatically from the data. Such sequence labeling systems are applicable to many tasks, using only the surface text as input, yet are able to achieve competitive results BIBREF0 , BIBREF1 .

Current neural models generally make use of word embeddings, which allow them to learn similar representations for semantically or functionally similar words. While this is an important improvement over count-based models, they still have weaknesses that should be addressed. The most obvious problem arises when dealing with out-of-vocabulary (OOV) words – if a token has never been seen before, then it does not have an embedding and the model needs to back-off to a generic OOV representation. Words that have been seen very infrequently have embeddings, but they will likely have low quality due to lack of training data. The approach can also be sub-optimal in terms of parameter usage – for example, certain suffixes indicate more likely POS tags for these words, but this information gets encoded into each individual embedding as opposed to being shared between the whole vocabulary.

In this paper, we construct a task-independent neural network architecture for sequence labeling, and then extend it with two different approaches for integrating character-level information. By operating on individual characters, the model is able to infer representations for previously unseen words and share information about morpheme-level regularities. We propose a novel architecture for combining character-level representations with word embeddings using a gating mechanism, also referred to as attention, which allows the model to dynamically decide which source of information to use for each word. In addition, we describe a new objective for model training where the character-level representations are optimised to mimic the current state of word embeddings.

We evaluate the neural models on 8 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that including a character-based component in the sequence labeling model provides substantial performance improvements on all the benchmarks. In addition, the attention-based architecture achieves the best results on all evaluations, while requiring a smaller number of parameters.

## Bidirectional LSTM for sequence labeling

We first describe a basic word-level neural network for sequence labeling, following the models described by Lample2016 and Rei2016, and then propose two alternative methods for incorporating character-level information.

Figure 1 shows the general architecture of the sequence labeling network. The model receives a sequence of tokens $(w_1, ..., w_T)$ as input, and predicts a label corresponding to each of the input tokens. The tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings $(x_1, ..., x_T)$ . Next, the embeddings are given as input to two LSTM BIBREF2 components moving in opposite directions through the text, creating context-specific representations. The respective forward- and backward-conditioned representations are concatenated for each word position, resulting in representations that are conditioned on the whole sequence: 

$$\overrightarrow{h_t} = LSTM(x_t, \overrightarrow{h_{t-1}})\hspace{30.0pt}
\overleftarrow{h_t} = LSTM(x_t, \overleftarrow{h_{t+1}})\hspace{30.0pt}
h_t = [\overrightarrow{h_t};\overleftarrow{h_t}]$$   (Eq. 1) 

We include an extra narrow hidden layer on top of the LSTM, which proved to be a useful modification based on development experiments. An additional hidden layer allows the model to detect higher-level feature combinations, while constraining it to be small forces it to focus on more generalisable patterns: 

$$d_t = tanh(W_d h_t)$$   (Eq. 2) 

where $W_d$ is a weight matrix between the layers, and the size of $d_t$ is intentionally kept small.

Finally, to produce label predictions, we use either a softmax layer or a conditional random field (CRF, Lafferty2001). The softmax calculates a normalised probability distribution over all the possible labels for each word: 

$$P(y_t = k | d_t) = \frac{e^{W_{o,k} d_t}}{\sum _{\tilde{k} \in K} e^{W_{o,\tilde{k}} d_t}}$$   (Eq. 3) 

where $P (y_t = k|d_t )$ is the probability of the label of the $t$ -th word ( $y_t$ ) being $k$ , $K$ is the set of all possible labels, and $W_{o,k}$ is the $k$ -th row of output weight matrix $W_o$ . To optimise this model, we minimise categorical crossentropy, which is equivalent to minimising the negative log-probability of the correct labels: 

$$E = - \sum _{t=1}^{T} log(P(y_t| d_t))$$   (Eq. 4) 

Following Huang2015, we can also use a CRF as the output layer, which conditions each prediction on the previously predicted label. In this architecture, the last hidden layer is used to predict confidence scores for the word having each of the possible labels. A separate weight matrix is used to learn transition probabilities between different labels, and the Viterbi algorithm is used to find an optimal sequence of weights. Given that $y$ is a sequence of labels $[y_1, ..., y_T]$ , then the CRF score for this sequence can be calculated as: 

$$s(y) = \sum _{t=1}^T A_{t,y_t} + \sum _{t=0}^T B_{y_t,y_{t+1}}$$   (Eq. 5) 

$$A_{t,y_t} = W_{o,y_t} d_t$$   (Eq. 6) 

where $A_{t,y_t}$ shows how confident the network is that the label on the $t$ -th word is $y_t$ . $B_{y_t,y_{t+1}}$ shows the likelihood of transitioning from label $y_t$ to label $y_{t+1}$ , and these values are optimised during training. The output from the model is the sequence of labels with the largest score $s(y)$ , which can be found efficiently using the Viterbi algorithm. In order to optimise the CRF model, the loss function maximises the score for the correct label sequence, while minimising the scores for all other sequences: 

$$E = - s(y) + log \sum _{\tilde{y} \in \widetilde{Y}} e^{s(\tilde{y})}$$   (Eq. 7) 

where $\widetilde{Y}$ is the set of all possible label sequences.

## Character-level sequence labeling

Distributed embeddings map words into a space where semantically similar words have similar vector representations, allowing the models to generalise better. However, they still treat words as atomic units and ignore any surface- or morphological similarities between different words. By constructing models that operate over individual characters in each word, we can take advantage of these regularities. This can be particularly useful for handling unseen words – for example, if we have never seen the word cabinets before, a character-level model could still infer a representation for this word if it has previously seen the word cabinet and other words with the suffix -s. In contrast, a word-level model can only represent this word with a generic out-of-vocabulary representation, which is shared between all other unseen words.

Research into character-level models is still in fairly early stages, and models that operate exclusively on characters are not yet competitive to word-level models on most tasks. However, instead of fully replacing word embeddings, we are interested in combining the two approaches, thereby allowing the model to take advantage of information at both granularity levels. The general outline of our approach is shown in Figure 2 . Each word is broken down into individual characters, these are then mapped to a sequence of character embeddings $(c_1, ..., c_R)$ , which are passed through a bidirectional LSTM: 

$$\overrightarrow{h^*_i} = LSTM(c_i, \overrightarrow{h^*_{i-1}}) \hspace{30.0pt}
\overleftarrow{h^*_i} = LSTM(c_i, \overleftarrow{h^*_{i+1}})$$   (Eq. 9) 

We then use the last hidden vectors from each of the LSTM components, concatenate them together, and pass the result through a separate non-linear layer. 

$$h^* = [\overrightarrow{h^*_R};\overleftarrow{h^*_1}] \hspace{30.0pt}
m = tanh(W_m h^*)$$   (Eq. 10) 

where $W_m$ is a weight matrix mapping the concatenated hidden vectors from both LSTMs into a joint word representation $m$ , built from individual characters.

We now have two alternative feature representations for each word – $x_t$ from Section "Bidirectional LSTM for sequence labeling" is an embedding learned on the word level, and $m^{(t)}$ is a representation dynamically built from individual characters in the $t$ -th word of the input text. Following Lample2016, one possible approach is to concatenate the two vectors and use this as the new word-level representation for the sequence labeling model: 

$$\widetilde{x} = [x; m]$$   (Eq. 11) 

This approach, also illustrated in Figure 2 , assumes that the word-level and character-level components learn somewhat disjoint information, and it is beneficial to give them separately as input to the sequence labeler.

## Attention over character features

Alternatively, we can have the word embedding and the character-level component learn the same semantic features for each word. Instead of concatenating them as alternative feature sets, we specifically construct the network so that they would learn the same representations, and then allow the model to decide how to combine the information for each specific word.

We first construct the word representation from characters using the same architecture – a bidirectional LSTM operates over characters, and the last hidden states are used to create vector $m$ for the input word. Instead of concatenating this with the word embedding, the two vectors are added together using a weighted sum, where the weights are predicted by a two-layer network: 

$$z = \sigma (W^{(3)}_z tanh(W^{(1)}_{z} x + W^{(2)}_{z} m)) \hspace{30.0pt}
\widetilde{x} = z\cdot x + (1-z) \cdot m$$   (Eq. 13) 

where $W^{(1)}_{z}$ , $W^{(2)}_{z}$ and $W^{(3)}_{z}$ are weight matrices for calculating $z$ , and $\sigma ()$ is the logistic function with values in the range $[0,1]$ . The vector $z$ has the same dimensions as $x$ or $m$ , acting as the weight between the two vectors. It allows the model to dynamically decide how much information to use from the character-level component or from the word embedding. This decision is done for each feature separately, which adds extra flexiblity – for example, words with regular suffixes can share some character-level features, whereas irregular words can store exceptions into word embeddings. Furthermore, previously unknown words are able to use character-level regularities whenever possible, and are still able to revert to using the generic OOV token when necessary.

The main benefits of character-level modeling are expected to come from improved handling of rare and unseen words, whereas frequent words are likely able to learn high-quality word-level embeddings directly. We would like to take advantage of this, and train the character component to predict these word embeddings. Our attention-based architecture requires the learned features in both word representations to align, and we can add in an extra constraint to encourage this. During training, we add a term to the loss function that optimises the vector $m$ to be similar to the word embedding $x$ : 

$$\widetilde{E} = E + \sum _{t=1}^{T} g_t (1 - cos(m^{(t)}, x_t)) \hspace{30.0pt}
g_t =
{\left\lbrace \begin{array}{ll}
0, & \text{if}\ w_t = OOV \\
1, & \text{otherwise}
\end{array}\right.}$$   (Eq. 14) 

Equation 14 maximises the cosine similarity between $m^{(t)}$ and $x_t$ . Importantly, this is done only for words that are not out-of-vocabulary – we want the character-level component to learn from the word embeddings, but this should exclude the OOV embedding, as it is shared between many words. We use $g_t$ to set this cost component to 0 for any OOV tokens.

While the character component learns general regularities that are shared between all the words, individual word embeddings provide a way for the model to store word-specific information and any exceptions. Therefore, while we want the character-based model to shift towards predicting high-quality word embeddings, it is not desireable to optimise the word embeddings towards the character-level representations. This can be achieved by making sure that the optimisation is performed only in one direction; in Theano BIBREF3 , the disconnected_grad function gives the desired effect.

## Datasets

We evaluate the sequence labeling models and character architectures on 8 different datasets. Table 1 contains information about the number of labels and dataset sizes for each of them.

## Experiment settings

For data prepocessing, all digits were replaced with the character '0'. Any words that occurred only once in the training data were replaced by the generic OOV token for word embeddings, but were still used in the character-level components. The word embeddings were initialised with publicly available pretrained vectors, created using word2vec BIBREF12 , and then fine-tuned during model training. For the general-domain datasets we used 300-dimensional vectors trained on Google News; for the biomedical datasets we used 200-dimensional vectors trained on PubMed and PMC. The embeddings for characters were set to length 50 and initialised randomly.

The LSTM layer size was set to 200 in each direction for both word- and character-level components. The hidden layer $d$ has size 50, and the combined representation $m$ has the same length as the word embeddings. CRF was used as the output layer for all the experiments – we found that this gave most benefits to tasks with larger numbers of possible labels. Parameters were optimised using AdaDelta BIBREF13 with default learning rate $1.0$ and sentences were grouped into batches of size 64. Performance on the development set was measured at every epoch and training was stopped if performance had not improved for 7 epochs; the best-performing model on the development set was then used for evaluation on the test set. In order to avoid any outlier results due to randomness in the model initialisation, we trained each configuration with 10 different random seeds and present here the averaged results.

When evaluating on each dataset, we report the measures established in previous work. Token-level accuracy is used for PTB-POS and GENIA-POS; $F_{0.5}$ score over the erroneous words for FCEPUBLIC; the official evaluation script for BC2GM which allows for alternative correct entity spans; and microaveraged mention-level $F_{1}$ score for the remaining datasets.

## Results

While optimising the hyperparameters for each dataset separately would likely improve individual performance, we conduct more controlled experiments on a task-independent model. Therefore, we use the same hyperparameters from Section "Experiment settings" on all datasets, and the development set is only used for the stopping condition. With these experiments, we wish to determine 1) on which sequence labeling tasks do character-based models offer an advantange, and 2) which character-based architecture performs better.

Results for the different model architectures on all 8 datasets are shown in Table 2 . As can be seen, including a character-based component in the sequence labeling architecture improves performance on every benchmark. The NER datasets have the largest absolute improvement – the model is able to learn character-level patterns for names, and also improve the handling of any previously unseen tokens.

Compared to concatenating the word- and character-level representations, the attention-based character model outperforms the former on all evaluations. The mechanism for dynamically deciding how much character-level information to use allows the model to better handle individual word representations, giving it an advantage in the experiments. Visualisation of the attention values in Figure 3 shows that the model is actively using character-based features, and the attention areas vary between different words.

The results of this general tagging architecture are competitive, even when compared to previous work using hand-crafted features. The network achieves 97.27% on PTB-POS compared to 97.55% by Huang2015, and 72.70% on JNLPBA compared to 72.55% by Zhou2004. In some cases, we are also able to beat the previous best results – 87.99% on BC2GM compared to 87.48% by Campos2015, and 41.88% on FCEPUBLIC compared to 41.1% by Rei2016. Lample2016 report a considerably higher result of 90.94% on CoNLL03, indicating that the chosen hyperparameters for the baseline system are suboptimal for this specific task. Compared to the experiments presented here, their model used the IOBES tagging scheme instead of the original IOB, and embeddings pretrained with a more specialised method that accounts for word order.

It is important to also compare the parameter counts of alternative neural architectures, as this shows their learning capacity and indicates their time requirements in practice. Table 3 contains the parameter counts on three representative datasets. While keeping the model hyperparameters constant, the character-level models require additional parameters for the character composition and character embeddings. However, the attention-based model uses fewer parameters compared to the concatenation approach. When the two representations are concatenated, the overall word representation size is increased, which in turn increases the number of parameters required for the word-level bidirectional LSTM. Therefore, the attention-based character architecture achieves improved results even with a smaller parameter footprint.

## Related work

There is a wide range of previous work on constructing and optimising neural architectures applicable to sequence labeling. Collobert2011 described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy2014a experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang2015 described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition. Rei2016 evaluated a range of neural architectures, including convolutional and recurrent networks, on the task of error detection in learner writing. The word-level sequence labeling model described in this paper follows the previous work, combining useful design choices from each of them. In addition, we extended the model with two alternative character-level architectures, and evaluated its performance on 8 different datasets.

Character-level models have the potential of capturing morpheme patterns, thereby improving generalisation on both frequent and unseen words. In recent years, there has been an increase in research into these models, resulting in several interesting applications. Ling2015b described a character-level neural model for machine translation, performing both encoding and decoding on individual characters. Kim2016 implemented a language model where encoding is performed by a convolutional network and LSTM over characters, whereas predictions are given on the word-level. Cao2016 proposed a method for learning both word embeddings and morphological segmentation with a bidirectional recurrent network over characters. There is also research on performing parsing BIBREF14 and text classification BIBREF15 with character-level neural models. Ling2015a proposed a neural architecture that replaces word embeddings with dynamically-constructed character-based representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample2016 described a model where the character-level representation is combined with word embeddings through concatenation. In this work, we proposed an alternative architecture, where the representations are combined using an attention mechanism, and evaluated both approaches on a range of tasks and datasets. Recently, Miyamoto2016 have also described a related method for the task of language modelling, combining characters and word embeddings using gating.

## Conclusion

Developments in neural network research allow for model architectures that work well on a wide range of sequence labeling datasets without requiring hand-crafted data. While word-level representation learning is a powerful tool for automatically discovering useful features, these models still come with certain weaknesses – rare words have low-quality representations, previously unseen words cannot be modeled at all, and morpheme-level information is not shared with the whole vocabulary.

In this paper, we investigated character-level model components for a sequence labeling architecture, which allow the system to learn useful patterns from sub-word units. In addition to a bidirectional LSTM operating over words, a separate bidirectional LSTM is used to construct word representations from individual characters. We proposed a novel architecture for combining the character-based representation with the word embedding by using an attention mechanism, allowing the model to dynamically choose which information to use from each information source. In addition, the character-level composition function is augmented with a novel training objective, optimising it to predict representations that are similar to the word embeddings in the model.

The evaluation was performed on 8 different sequence labeling datasets, covering a range of tasks and domains. We found that incorporating character-level information into the model improved performance on every benchmark, indicating that capturing features regarding characters and morphmes is indeed useful in a general-purpose tagging system. In addition, the attention-based model for combining character representations outperformed the concatenation method used in previous work in all evaluations. Even though the proposed method requires fewer parameters, the added ability of controlling how much character-level information is used for each word has led to improved performance on a range of different tasks.
