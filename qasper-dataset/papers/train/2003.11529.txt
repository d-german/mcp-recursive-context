# Masakhane -- Machine Translation For Africa

**Paper ID:** 2003.11529

## Abstract

Africa has over 2000 languages. Despite this, African languages account for a small portion of available resources and publications in Natural Language Processing (NLP). This is due to multiple factors, including: a lack of focus from government and funding, discoverability, a lack of community, sheer language complexity, difficulty in reproducing papers and no benchmarks to compare techniques. To begin to address the identified problems, MASAKHANE, an open-source, continent-wide, distributed, online research effort for machine translation for African languages, was founded. In this paper, we discuss our methodology for building the community and spurring research from the African continent, as well as outline the success of the community in terms of addressing the identified problems affecting African NLP.

## The State of African NLP

2144 of all 7111 (30.15%) living languages today are African languages BIBREF1. But only a small portion of linguistic resources for NLP research are built for African languages. As a result, there are only few NLP publications: In all ACL conferences in 2019, only 5 out of 2695 (0.19%) author affiliations were based in Africa BIBREF2. This stark contrast of linguistic richness versus poor representation of African languages in NLP is caused by multiple factors.

First of all, African societies do not see hope for African languages being accepted as primary means of communication BIBREF3. As a result, few efforts to fund NLP or translation for African languages exist, despite the potential impact. This lack of focus has had a ripple effect.

The few existing resources are not easily discoverable, published in closed journals, non-indexed local conferences, or remain undigitized, surviving only in private collections BIBREF4. This opaqueness impedes researchers' ability to reproduce and build upon existing results, and to develop, compete on and progress public benchmarks BIBREF5.

African researchers are disproportionately affected by socio-economic factors, and are often hindered by visa issues BIBREF6 and costs of flights from and within Africa BIBREF7. They are distributed and disconnected on the continent, and rarely have the opportunity to commune, collaborate and share.

Furthermore, African languages are of high linguistic complexity and variety, with diverse morphologies and phonologies, including lexical and grammatical tonal patterns, and many are practiced within multilingual societies with frequent code switching BIBREF8, BIBREF9, BIBREF10. Because of this complexity, cross-lingual generalization from success in languages like English are not guaranteed.

## Contribution

Founded at the Deep Learning Indaba 2019, Masakhane constitutes an open-source, continent-wide, distributed, online research effort for machine translation for African languages. Its goals are threefold:

For Africa: To build a community of NLP researchers, connect and grow it, spurring and sharing further research, to enable language preservation and increase its global visibility and relevance.

For NLP researchers: To build data sets and tools to facilitate NLP research on African languages, and to pose new research problems to enrich the NLP research landscape.

For the global researchers community: To discover best practices for distributed research, to be applied by other emerging research communities.

## Methodology and Results

Masakhane's strategy is to offer barrier-free open access to first hands-on NLP experiences with African languages, fighting the above-mentioned opaqueness. With an easy-to-use open source platform, it allows individuals to train neural machine translation (NMT) models on a parallel corpus for a language of their choice, and share the results with an online community. The online community is based on weekly meetings, an active Slack workspace, and a GitHub repository (github.com/masakhane-io), so that members can support each other and connect despite geographical distances. No academic prerequisites are required for participation, since tertiary education enrolments are minimal in sub-saharan Africa BIBREF11.

A Jupyter Notebook features documented data preparation, model configuration, training and evaluation. It runs on Google Colab with a single (free) GPU for a small limited number of hours, such that participants do not require expensive hardware. The NMT models are built using Joey NMT BIBREF12, which comes with a beginner-friendly documentation. Participants submit and publish their data, code and results for training on their language to improve reproducibility and discoverability. To lower the barrier of data collection, the JW300 multilingual dataset BIBREF13 with parallel corpora for English to 101 African languages is integrated into the notebook. With the goal of improving translation quality by transfer learning across languages in the future, global test sets with English sources are extracted from JW300, and excluded from training data for any language pair to avoid potential data leakage for cross-lingual transfer.

As of February 14, 2020, the Masakhane community consists of 144 participants from 17 African countries with diverse educations and occupations (Figure FIGREF4), and 2 countries outside Africa (USA and Germany). So far, 30 translation results for 28 African languages have been published by 25 contributors on GitHub.

## Future Roadmap

Masakhane aims to continue to grow and facilitate engagement within the community, especially helping inactive users contribute benchmarks and fostering mentoring relations. In the next year, the project will expand to different NLP tasks beyond NMT in order to reach a broader audience. Qualitative analysis on model performance as well as investigations of automatic evaluation metrics will spur healthy competition on results. Masakhane will also provide notebooks for transfer and un/self-supervised learning to push translation quality. In terms of data collection, the size and domain of global test sets will be expanded.
