# AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses

**Paper ID:** 2001.05467

## Abstract

Many sequence-to-sequence dialogue models tend to generate safe, uninformative responses. There have been various useful efforts on trying to eliminate them. However, these approaches either improve decoding algorithms during inference, rely on hand-crafted features, or employ complex models. In our work, we build dialogue models that are dynamically aware of what utterances or tokens are dull without any feature-engineering. Specifically, we start with a simple yet effective automatic metric, AvgOut, which calculates the average output probability distribution of all time steps on the decoder side during training. This metric directly estimates which tokens are more likely to be generated, thus making it a faithful evaluation of the model diversity (i.e., for diverse models, the token probabilities should be more evenly distributed rather than peaked at a few dull tokens). We then leverage this novel metric to propose three models that promote diversity without losing relevance. The first model, MinAvgOut, directly maximizes the diversity score through the output distributions of each batch; the second model, Label Fine-Tuning (LFT), prepends to the source sequence a label continuously scaled by the diversity score to control the diversity level; the third model, RL, adopts Reinforcement Learning and treats the diversity score as a reward signal. Moreover, we experiment with a hybrid model by combining the loss terms of MinAvgOut and RL. All four models outperform their base LSTM-RNN model on both diversity and relevance by a large margin, and are comparable to or better than competitive baselines (also verified via human evaluation). Moreover, our approaches are orthogonal to the base model, making them applicable as an add-on to other emerging better dialogue models in the future.

## Introduction

Many modern dialogue generation models use a sequence-to-sequence architecture as their backbone BIBREF0, following its success when applied to Machine Translation (MT) BIBREF1. However, dialogue tasks also have a requirement different from that of MT: the response not only has to be "correct" (coherent and relevant), but also needs to be diverse and informative. However, seq2seq has been reported by many previous works to have low corpus-level diversity BIBREF2, BIBREF3, BIBREF0, BIBREF4, as it tends to generate safe, terse, and uninformative responses, such as "I don't know.". These responses unnecessarily make a dialogue system much less interactive than it should be.

To increase the diversity of dialogue responses, the first step is to faithfully evaluate how diverse a response is. There are metrics used by previous work that are correlated to diversity, but not strongly, such as ratio of distinct tokens BIBREF2 and response length BIBREF5. However, a response can be long but extremely boring in meaning, such as "I am sure that I don't know about it.", or short but interesting (i.e., contains a lot of information), such as "Dad was mean.". Only investigating discrete token output by the model is also not ideal, because these tokens are only a single realization of the model's output probability distribution at each time step, which unavoidably loses valuable information indicated by the whole distribution. BIBREF6 (BIBREF6) manually collect a shortlist of dull responses, and during training discourage the model from producing such utterances. However, an important drawback of hand-crafted rules is that the set of dull tokens or utterances is static, while in fact it usually evolves during training: when the current dull tokens are eliminated, another set of them might reveal themselves.

In our work, we begin with a simple yet effective approach to measure how diverse a response is. This metric, which we name "Average Output Probability Distribution", or AvgOut, draws information directly from the training-in-session model itself. We calculate it by keeping track of the exponential average of all output probability distributions on the decoder side during training. This metric dynamically measures which tokens the model is biased toward without any hand-crafted rules, thus making it a faithful evaluation of the model diversity (i.e., for diverse models, the token probabilities should be more evenly distributed rather than peaked at a few dull tokens). In addition, since AvgOut is a one-dimensional categorical distribution rather than a dimensionless numerical value like entropy, it naturally carries and conveys more information about model diversity.

We then propose three models that leverage our novel metric to promote diversity in dialogue generation. The first MinAvgOut model minimizes the dot product of current batch AvgOut and the exponential average AvgOut across batches, which encourages low-frequency tokens to be generated. The second LFT model uses a labeled transduction method and scales a "diversity label" by the diversity score of the ground-truth target sequence during training, while during testing can generate responses of different levels of diversity by tweaking the intended diversity score. The third RL model leverages reinforcement learning, where our novel metric is applied to discrete tokens and serve as a reward signal. In addition, since MinAvgOut regularizes directly on the continuous distribution while RL calculates its reward based on discrete sampled tokens, we simply add up the loss terms of the two models, creating an even stronger hybrid model.

We first employ diverse automatic metrics, including Distinct-1 and -2 from previous work BIBREF2 and our novel metric Diveristy-iAUC (which calculates one minus the sum of normalized frequencies of the most frequent tokens produced by the model), plus activity/entity F1s, to evaluate the diversity and relevance of the generated responses. We then conduct human evaluations to verify that these models not only outperform their base model LSTM by a large margin, but are also comparable to or better than an advanced decoding algorithm MMI BIBREF2 and a very competitive model VHRED BIBREF7 on the Ubuntu dataset.

## AvgOut as an Effective Diversity Metric

By only keeping a static shortlist of boring responses or tokens, one basically assumes that we humans should decide which tokens are dull. However, we argue that we should instead look from the model's perspective to identify dull tokens, because even if the model outputs a word that we consider rare, including it in too many responses is still considered a dull behavior. Motivated by this thought experiment, we propose a novel metric, Average Output Probability Distribution (AvgOut), that dynamically keeps track of which tokens the model is biased toward. To calculate this, during training, we average out all the output probability distributions for each time step of the decoder for the whole mini-batch. The resulting vector $D^{\prime }$ will reflect each token's probability of being generated from the model's perspective. Note that we do not use discrete ground-truth tokens to evaluate the model's bias, because there is a fine distinction between the two: a statistics of frequency on ground-truth tokens is an evaluation of the corpus's bias, while AvgOut is an evaluation of what bias the model has learned because by generating dull responses more frequently than the training corpus has, it is the model itself that we should adjust. Also note that the reason we take the average is that a single output distribution will largely depend on the context and the previous target tokens (which are fed as inputs to the decoder during training), but on average the distribution should be a faithful evaluation on which words are more likely to be generated from model's perspective.

To avoid batches that have AvgOut significantly different from those of other batches, which would lead the model astray, we keep the exponential average of this metric across batches to make it less biased toward any specific batch. Let it be $D$. After training on a mini-batch and obtain $D^{\prime }$, we update $D$ like the following:

where $\gamma $ is $0.01$ in our experiments.

Another consideration of AvgOut is that theoretically we can have two choices. The first is to use the output distributions when we are teacher-forcing (i.e., only feeding ground-truth tokens); the other is to let the model use its own predictions during greedy/beam-search decoding or sampling. We reason that the former is a much better estimation of the model's bias, because the latter will result in a cascading enlargement of the model bias due to the auto-regressive nature of LSTM-RNN models (i.e., the tokens fed to the decoder are themselves also polluted by the model's bias). Our early experimental results also agreed with the above reasoning.

Although we try to come up with the most faithful evaluation of how diverse a response is, our approach certainly has its drawbacks too. For example, using very frequent words but less frequent combinations of them may result in a good response which will be penalized by our metric. A natural solution to this is to also use bigram and trigram diversities and take a linear combination of them, which on a high-level is similar to BLEU BIBREF8. However, considering even bigram distribution takes up $O(|V|^2)$ space and calculation time, hence we did not try it due to limited resources. However, as will be presented in Section SECREF5, regularizing unigram distributions can already greatly help on higher-gram diversities, while also improving relevance.

## Three Models to Leverage AvgOut

AvgOut can play at least three roles. First, it can be used to directly supervise output distribution during training; second, it can be used as a prior in labeled sequence transduction methods to control diversity of the generated response; and third, it can be used as a reward signal for Reinforcement Learning to encourage diverse sampled responses. In this section, we begin with a base vanilla seq2seq model, and next present our three models to diversify responses based on AvgOut.

Our base model LSTM is identical to that proposed by BIBREF1 (BIBREF1), which consists of a single-layer bi-directional LSTM-RNN BIBREF9 encoder and a single-layer LSTM-RNN decoder with additive attention.

## Three Models to Leverage AvgOut ::: Regularization by Minimizing Continuous-AvgOut

Our MinAvgOut model (Figure FIGREF3) directly integrates AvgOut into the loss function by summarizing it into a single numerical value named Continuous-AvgOut. We do this by taking the dot-product of $D$ and $D^{\prime }$ (Figure FIGREF6). The intuition behind this simple calculation is that $D$ can also be viewed as a set of weights which add up to $1.0$, since it is a probability vector. By taking the dot product, we are actually calculating a weighted average of each probability in $D^{\prime }$. To evaluate how diverse the model currently is, the duller tokens should obviously carry higher weights since they contribute more to the "dullness" of the whole utterance. Assuming that $D$ is a column vector, the continuous diversity score is $B_c$, and the resulting extra loss term is $L_B$, the total loss $L$ is given by:

where $\alpha $ is a coefficient to balance the regularization loss with the maximum likelihood loss (a.k.a. teacher forcing loss) $L_{ML}$. This is important because the regularization term continues to discourage the model from generating the ground-truth token, which we need to balance by ML loss to reduce the impact (otherwise the model will be led astray). Note that since $D$ is a moving average which does not depend on the model parameters of the current mini-batch, only $D^{\prime }$ will result in gradient flow during back-propagation, which is what we intend.

## Three Models to Leverage AvgOut ::: Label-Fine-Tuning Model

We also borrow the continuous version of the Label-Fine-Tuning (LFT) model from BIBREF10 (BIBREF10), which is an extension of the discrete labeled sequence transduction methods BIBREF11. The LFT model leverages a continuous label to serve as a prior for generating the target sequence. This label corresponds to an embedding just like a normal token does, but can be scaled by a continuous value. This model is applicable to our case because the diversity score of a response can also be viewed as a style, ranging from $0.0$ to $1.0$. Specifically, we add to the vocabulary a diversity label and scale its embedding vector with the intended diversity score of the target sequence. During training, this score is obtained by evaluating the diversity of the ground-truth target sequence (see Figure FIGREF8); during test time, we instead feed the model a diversity label scaled by a score of our choice (i.e., when we want the model to generate a more diverse response, we scale the label's embedding by a higher score, while to generate a duller response, we scale the embedding by a lower one).

## Three Models to Leverage AvgOut ::: Reward-Based Reinforcement Learning

We also explore a model (see Figure FIGREF11) which regularizes on the discrete token level, because merely monitoring output probability distribution may ignore certain bad styles such as repetition (e.g. "I don't don't know."). We use Discrete-AvgOut to calculate the continuous diversity score of a discrete sequence. Let $\lbrace G_1, G_2, ..., G_{N_G}\rbrace $ be a sequence of $N_G$ tokens sampled by the model during training. Then from $D$, we extract the probabilities $\lbrace P_1, P_2, ..., P_{N_G}\rbrace $ corresponding to each generated token. The diversity score $B_{d}$ on these discrete tokens will be:

where $N_{unique}$ is the number of unique tokens in the sampled sequence (see Figure FIGREF12). Note that this division explicitly discourages the model from outputting repeated tokens, because when that happens, the nominator will stay the same, while the denominator will decrease, resulting in a lower diversity score. Also note that MinAvgOut can be complementary to RL since calculating diversity scores based on discrete tokens unavoidably loses valuable information from the output distribution before argmax is taken. In Section SECREF5, we show with both automatic and human evaluations that this combination indeed achieves the best results among our models. Following BIBREF12 (BIBREF12), our loss function consists of two terms. The first term is the Maximum Likelihood loss ($L_{\textsc {ml}}$); the other is the Reinforcement Learning loss ($L_{\textsc {rl}}$). The total loss $L$ is then:

where $\beta $ is a hyperparameter indicating how much weight we want to assign to the rl part of the loss, $x$ is the source sequence, $\lbrace y_t^*\rbrace $ are the ground truth tokens and $\lbrace y_t^s\rbrace $ are the sampled tokens. We use a policy gradient method BIBREF13 to calculate the RL loss. Specifically, we sample a response for each context $x$, and assign to it a reward $R$, which is equal to $B_d$ because we want to encourage the model to be diverse. We also use a baseline $R_b$ that helps reduce variance during training BIBREF14. In our case this baseline is again the exponential average of all $B_d$ in previous mini-batches.

## Experimental Setup ::: Dataset and Task

We use the task-oriented Ubuntu Dialogue dataset BIBREF15, because it not only has F1 metrics to evaluate the relevance of responses, but the dialogues in them are also open-ended to allow enough space for diversity. We also chose this dataset because previous work, e.g., HRED BIBREF3 and VHRED BIBREF7 both used Ubuntu to showcase their diversity-promotion models. Due to the popularity of this dataset, we were able to reproduce almost all models on this same dataset and have a meaningful comparison on their effectiveness of eliminating dullness. As future work, we plan to apply our models to other datasets where diversity is desired.

## Experimental Setup ::: Automatic Evaluation

To measure the relevance of the model responses, we follow BIBREF16 (BIBREF16) and evaluate on F1's for both activities (technical verbs, e.g., "upload", "install") and entities (technical nouns, e.g., "root", "internet"). The F1's are computed by mapping the ground-truth and model responses to their corresponding activity-entity representations BIBREF16, who considered F1 to be "particularly suited for the goal-oriented Ubuntu Dialogue Corpus". We did not evaluate on BLEU score BIBREF8 because BIBREF17 showed that BLEU does not correlate well with dialogue quality. BIBREF18 (BIBREF18) also made similar observations on BLEU. To evaluate diversity, we employ two evaluation metrics from previous work, namely Distinct-1 and Distinct-2 BIBREF2. These are the ratios between the number of unique tokens and all tokens for unigrams and bigrams, respectively. In addition, we propose a novel diversity graph and its corresponding metric, which we name Diversity-32 and Diversity-AUC, respectively. We gather statistics of sentence, unigram, bigram and trigram, and sort their normalized frequencies from highest to lowest. Observing that all four graphs follow long-tail distributions, we only keep the highest 32 frequencies and plot them. We then calculate one minus the Area under Curve (Diversity-AUC) for each graph, which draws a high-level picture of how diverse a model is.

## Experimental Setup ::: Human Evaluation

Although we proposed the effective AvgOut metric, we did find that the model sometimes still cheats to gain higher automatic diversity score. For example, as can be seen in the selected output examples (Section SECREF5), the model tends to generate words with typo since these are rarer tokens as compared to their correct counterparts. This is unavoidable for noisy datasets like Ubuntu. Thus, without human evaluation, we can never be sure if our models are good or they only look good because our metrics are exploited.

We thus also conducted human studies on Amazon MTurk to evaluate the generated responses with pairwise comparison for dialogue quality. We compare our models with an advanced decoding algorithm MMI BIBREF2 and two models, namely LSTM BIBREF0 and VHRED BIBREF7, both with additive attention. To our best knowledge, LSTM and VHRED were the primary models with which F1's were reported on the Ubuntu dataset. Following BIBREF5 (BIBREF5), we employ two criteria: Plausibility and Content Richness. The first criterion measures whether the response is plausible given the context, while the second gauges whether the response is diverse and informative. The utterances were randomly shuffled to anonymize model identity. We only allowed annotators located in the US-located with at least an approval rate of $98\%$ and $10,000$ approved HITs. We collected 100 annotations in total after rejecting those completed by people who assign exactly the same score to all model responses. Since we evaluated 7 models, we collected 700 annotations in total, which came from a diverse pool of annotators.

## Experimental Setup ::: Training Details

For each of the three models, the hidden size of the encoder is 256, while the decoder hidden size is 512. For MinAvgOut, the coefficient of the regularization loss term $\alpha $ is $100.0$; For LFT, during inference we feed a score of $0.015$ since it achieves a good balance between response coherence and diversity. For RL, the coefficient of the RL term $\beta $ is $100.0$. For the hybrid model MinAvgOut + RL, $\alpha $ and $\beta $ share a coefficient of $50.0$.

## Results and Analysis ::: Automatic Evaluation Results

We employ several complementary metrics to capture different aspects of the model. The F1 results are shown in Table TABREF24. Among all single models, LFT performs the best, followed by MinAvgOut. RL is also comparable with previous state-of-the-art models VHRED (attn) and Reranking-RL. We think that this is because LFT exerts no force in pulling the model predictions away from the ground-truth tokens, but rather just makes itself aware of how dull each response is. Consequently, its responses appear more relevant than the other two approaches. Moreover, the hybrid model (last row) outperforms all other models by a large margin. One might expect that minimizing AVGOUT causes the models to move further away from the ground-truth tokens, so that it will hurt relevance. However, our F1 results show that as the responses become more diverse, they are more likely to include information more related and specific to the input contexts, which actually makes the model gain on both diversity and relevance. This will be further confirmed by the output examples in Table TABREF29.

We also present Diversity-32 graphs (Figure FIGREF16) and report Diversity-AUC as well as Distinct-1 and -2 for each model (Table TABREF25). We can see that all our models have significantly better sentence-level diversity than VHRED, let alone LSTM. For unigram diversity, they are also better than LSTM, though hard to distinguish from VHRED. Both bigram and trigram graphs reveal that all models are more diverse than LSTM, except that RL shows lower diversity than the other models, which agree with our F1 results. Note that since our models are only trained based on unigram output distributions, the bigram and trigram diversities are still far away from that of the ground-truth, which points to future direction. That said, the table does show that encouraging unigram diversity can already have positive influence on higher grams as well. Also note that the hybrid model (last row) does not achieve the best result in terms of diversity. We hypothesize that this is because RL, which is usually harder to optimize than ML losses, faces exacerbated issues when combined with a strong MinAvgOut loss, which tries to pull the model output distribution away from the token distribution in the training corpus.

Neither Distinct-1 nor -2 correlates well with our observation and evaluation of diversity and relevance. We reason that this is because these metrics only capture how many distinct tokens are used rather than each token's frequency, which is easier to be exploited because whether each token is used unnecessarily often (a strong sign of dullness) is not reflected in this measure.

## Results and Analysis ::: Human Evaluation Results

As mentioned in experimental setup, we conducted human evaluations on our models for both Plausibility and Content Richness, as well as calculating their average (to show overall score) and their difference (to show balance between the two criteria) (Table TABREF26). We can see from the table that all our models are statistically significantly better than the baseline models on both Plausibility and Content Richness, except that RL is slightly weaker on Content Richness, which agrees with the trend in automatic evaluations. Although MinAvgOut+RL model only ranks the second on average score (statistically equivalent to MinAvgOut) in human evaluation, it achieves a good balance, and it also ranks the second in automatic diversity and the first in F1 values. We thus consider it to be our best model.

## Results and Analysis ::: Selected Output Examples

We present two selected examples of generated responses from the investigated models (Table TABREF29). We can see that all our models learn to attend well to the context, generating coherent and informative responses.

## Related Work ::: Measurements of Response Diversity

Multiple metrics and approaches have been proposed to measure dialogue diversity. Some focus more on how similar the responses are to the ground-truth sequences, such as Word Error Rate BIBREF3 and BLEU BIBREF20, while the others explicitly have diversity in mind when being created, such as Distinct-1 and -2 BIBREF2. The key difference between AvgOut and the previous work is that first, our metric is dynamic with no feature-engineering; second, ours is versatile enough to be applied to both continuous distributions and discrete sequences, while theirs are only for discrete tokens; third, ours can be used for both sentence-level and corpus-level evaluation, while theirs are only meaningful as corpus-level metrics because they measure the extent of repetition across responses rather than for a single response.

## Related Work ::: Diversity-Promoting Dialogue Models

Researchers have different opinions on why dull responses are generated, which lead to various solutions. They can be roughly divided into four categories. The first category considers using conditional likelihood as a decoding objective the culprit BIBREF5, BIBREF2, BIBREF21, BIBREF22. They thus focus on improving the decoding algorithm during training. The second category traces the cause of the low-diversity problem back to the lack of model variability. They then adopt Variational Autoencoders and rely on sampling from a latent random variable as an additional prior to the decoder BIBREF7, BIBREF23, BIBREF24. The third category thinks that the issue is a lack of universal background knowledge and common sense beyond the input context. They consequently aim to integrate prior knowledge into the generation process BIBREF25, BIBREF26, BIBREF27, BIBREF28. The fourth category believes that the underlying model itself needs improvement. Some use hierarchical LSTM-RNN to encourage the model to capture high-level context BIBREF3; some use more advanced attention mechanism such as multi-head attention BIBREF29; and some use either more complicated architectures or models prone to degeneracies, such as Generative Adversarial Networks BIBREF30, Deep Reinforcement Learning BIBREF6 and Mixture Models BIBREF31. Our RL model has the same architecture as the Reinforcement Learning model, except with different rewards. BIBREF32 (BIBREF32) consider the reason for dull responses as the model's over-confidence. They then propose to add to the loss function a regularization term to maximize the entropy of the output probability distribution. Interestingly, they only proposed this simple approach rather than actually implementing it. Our MinAvgOut approach is related to their idea. Our approach is also related to posterior regularization BIBREF33, BIBREF34, BIBREF35, but our work is neural-based.

## Conclusion

We proposed a novel measure AvgOut to dynamically evaluate how diverse a model or a response is based on the models' own parameters, which themselves evolve during training. We then leveraged this effective measure to train three models, plus a hybrid model, to eliminate dull responses for dialogue generation tasks. In addition, we designed novel automatic metrics to evaluate the trained models on diversity, in addition to the ones from previous work. Both automatic and human evaluations consolidated that our models are able to generate more diverse and relevant responses, even when compared with state-of-the-art approaches. For future work, we plan to apply these models to different generative tasks where diversity is desired.

## Acknowledgments

We thank the reviewers for their helpful comments. This work was supported by NSF-CAREER Award #1846185, ONR #N00014-18-1-2871, and awards from Google, Facebook, Salesforce (views are not of the funding agency).
