# Measuring Conversational Fluidity in Automated Dialogue Agents

**Paper ID:** 1910.11790

## Abstract

We present an automated evaluation method to measure fluidity in conversational dialogue systems. The method combines various state of the art Natural Language tools into a classifier, and human ratings on these dialogues to train an automated judgment model. Our experiments show that the results are an improvement on existing metrics for measuring fluidity.

## Introduction

Conversational interactions between humans and Artificial Intelligence (AI) agents could amount to as much as thousands of interactions a day given recent developments BIBREF0. This surge in human-AI interactions has led to an interest in developing more fluid interactions between agent and human. The term `fluidity', when we refer to dialogue systems, tries to measure the concept of how humanlike communication is between a human and an AI entity. Conversational fluidity has historically been measured using metrics such as perplexity, recall, and F1-scores. However, one finds various drawbacks using these metrics. During the automatic evaluation stage of the second Conversational Intelligence Challenge (ConvAI2) BIBREF1 competition, it was noted that consistently replying with “I am you to do and your is like” would outperform the F1-score of all the models in the competition. This nonsensical phrase was constructed simply by picking several frequent words from the training set. Also, Precision at K, or the more specific Hits@1 metric has been used historically in assessing retrieval based aspects of the agent. This is defined as the accuracy of the next dialogue utterance when choosing between the gold response and N–1 distractor responses. Since these metrics are somewhat flawed, human evaluations were used in conjunction. Multiple attempts have been made historically to try to develop automatic metrics to assess dialogue fluidity. One of the earliest Eckert et al. (1997), used a stochastic system which regulated user-generated dialogues to debug and evaluate chatbots BIBREF2. In the same year, Marilyn et al. (1997) proposed the PARADISE BIBREF3 model. This framework was developed to evaluate dialogue agents in spoken conversations. A few years later the BLEU BIBREF4 metric was proposed. Subsequently, for almost two decades, this metric has been one of the few to be widely adopted by the research community. The method, which compares the matches in n-grams from the translated outputted text and the input text proved to be quick, inexpensive and has therefore been widely used. Therefore, we use the BLEU metric as a baseline to compare the quality of our proposed model.

## Introduction ::: Datasets

For this study, we use two types of data namely single-turn and multi-turn. The first type, single-turn, is defined such that each instance is made up of one statement and one response. This pair is usually a fragment of a larger dialogue. When given to humans for evaluation of fluidity, we ask to give a score on characteristics such as “How related is the response to the statement?” or “Does the response contain repeated text from the user's statement?”. These are all things that should not be affected by the fact that no history or context is provided and therefore, can still be classified reasonably. Contrary to the single turn datasets, the second type is the multi-turn dataset. This contains multiple instances of statements and responses, building on each other to create a fuller conversation. With these kinds of datasets, one can also evaluate and classify the data on various other attributes. An example of such evaluations would be something like “Does this response continue on the flow of the conversation?” or “Is the chatbot using repetitive text from previous responses?”. The details of how we collected each dataset are detailed below.

Single-Turn: This dataset consists of single-turn instances of statements and responses from the MiM chatbot developed at Constellation AI BIBREF5. The responses provided were then evaluated using Amazon Mechanical Turk (AMT) workers. A total of five AMT workers evaluated each of these pairs. The mean of the five evaluations is then used as the target variable. A sample can be seen in Table TABREF3. This dataset was used during experiments with results published in the Results section.

Multi-Turn: This dataset is taken from the ConvAI2 challenge and consists of various types of dialogue that have been generated by human-computer conversations. At the end of each dialogue, an evaluation score has been given, for each dialogue, between 1–4.

## Method

This section discusses the methods and used to develop our attributes and the technical details of how they are combined to create a final classification layer.

## Method ::: BERT Next Sentence Prediction

BERT BIBREF6 is a state-of-the-art model, which has been pre-trained on a large corpus and is suitable to be fine-tuned for various downstream NLP tasks. The main innovation between this model and existing language models is in how the model is trained. For BERT, the text conditioning happens on both the left and right context of every word and is therefore bidirectional. In previous models BIBREF7, a unidirectional language model was usually used in the pre-training. With BERT, two fully unsupervised tasks are performed. The Masked Language Model and the Next Sentence Prediction (NSP).

For this study, the NSP is used as a proxy for the relevance of response. Furthermore, in order to improve performance, we fine-tune on a customized dataset which achieved an accuracy of 82.4%. For the main analysis, we used the single-turn dataset, which gave us a correlation of 0.28 between the mean of the AMT evaluation and the BERT NSP. Next, we put each score into a category. For example, if the average score is 2.3, this would be placed in category 2. We then displayed the percentage of positive and negative predictions in a histogram for each of the categories. As seen in Figure FIGREF5, a clear pattern is seen between the higher scores and the positive prediction, and the lower scores and the negative predictions. details of how they are combined to create a final classification layer.

## Method ::: Repetition Control

This attribute is calculated by checking each statement and response for various types of repetition by using n-gram overlap. The motivation for including this as an attribute in dialogue fluidity is that repetitive words or n-grams can be bothersome to the end-user.

Repetitions are measured according to whether they are internal, external or partner. We calculate a percentage based on the single-turn utterance or the entire multi-turn conversation. We use unigram, bigram, and trigram for each repetition type based off BIBREF8.

We calculate a correlation of each repetition module with respect to human evaluations in order to understand the impact. For the single-turn dataset, the correlation is -0.09 and 0.07 for the internal and partner repetition attribute respectively. For the multi-turn dataset the correlation was -0.05 and -0.02 for the internal and partner repetition attribute respectively. This low correlation is reasonable and was expected. Measuring repetition in this way is not expected to provide huge classification power. However, we will attempt to exploit differences in correlation between these attributes and ones described below, which will provide some classification power.

## Method ::: Balance of Dialogue

For this attribute, we calculated the number of questions asked. For this particular case, we are not able to measure a correlation with respect to human evaluations.

## Method ::: Short-Safe Answers

Here, we checked for the length of the utterance and the presence of a Named Entity. We checked the correlation of this attribute with the human evaluation scores. The correlation score attained on the single-turn dataset was -0.09, while for the multi-turn dataset the correlation was 0. The full pipeline can be seen diagrammatically in Figure FIGREF9.

## Results

To create a final metric, we combine the individual components from Section 2 as features into a Support Vector Machine. The final results for our F1-score from this classification technique are 0.52 and 0.31 for the single and multi-turn data respectively.

We compare our results for both the single-turn and multi-turn experiments to the accuracies on the test data based off the BLEU score. We see an increase of 6% for our method with respect to the BLEU score in the single turn data, and a no change when using the multi-turn test set.

## Conclusion

This study aimed to implement an automatic metric to assess the fluidity of dialogue systems. We wanted to test if any set of attributes could show a high correlation to manual evaluations, thus replacing it entirely. As highlighted in the ConvAI2 challenge, automatic metrics are not reliable for a standalone evaluation of low-level dialogue outputs. For this study, three attributes were investigated. Tests were carried out based on these proposed attributes by making use of single and multi-turn datasets. These attributes, combined with the BERT model, showed that our classifier performed better than the BLEU model for the single-turn dataset. However, no improvement was seen on the multi-turn dataset.

Concerning feature importance, we observed that internal repetition and NSP are the most important attributes when used to classify fluidity. We believe that further work can be carried out in finding a more discriminating set of attributes.
