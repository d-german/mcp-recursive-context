# Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction

**Paper ID:** 1911.09886

## Abstract

A relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. There may be multiple relation tuples present in a text and they may share one or both entities among them. Extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tuples makes it more challenging. Most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. In this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. In the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. Next, we propose a pointer network-based decoding approach where an entire tuple is generated at every time step. Experiments on the publicly available New York Times corpus show that our proposed approaches outperform previous work and achieve significantly higher F1 scores.

## Introduction

Distantly-supervised information extraction systems extract relation tuples with a set of pre-defined relations from text. Traditionally, researchers BIBREF0, BIBREF1, BIBREF2 use pipeline approaches where a named entity recognition (NER) system is used to identify the entities in a sentence and then a classifier is used to find the relation (or no relation) between them. However, due to the complete separation of entity detection and relation classification, these models miss the interaction between multiple relation tuples present in a sentence.

Recently, several neural network-based models BIBREF3, BIBREF4 were proposed to jointly extract entities and relations from a sentence. These models used a parameter-sharing mechanism to extract the entities and relations in the same network. But they still find the relations after identifying all the entities and do not fully capture the interaction among multiple tuples. BIBREF5 (BIBREF5) proposed a joint extraction model based on neural sequence tagging scheme. But their model could not extract tuples with overlapping entities in a sentence as it could not assign more than one tag to a word. BIBREF6 (BIBREF6) proposed a neural encoder-decoder model for extracting relation tuples with overlapping entities. However, they used a copy mechanism to copy only the last token of the entities, thus this model could not extract the full entity names. Also, their best performing model used a separate decoder to extract each tuple which limited the power of their model. This model was trained with a fixed number of decoders and could not extract tuples beyond that number during inference. Encoder-decoder models are powerful models and they are successful in many NLP tasks such as machine translation, sentence generation from structured data, and open information extraction.

In this paper, we explore how encoder-decoder models can be used effectively for extracting relation tuples from sentences. There are three major challenges in this task: (i) The model should be able to extract entities and relations together. (ii) It should be able to extract multiple tuples with overlapping entities. (iii) It should be able to extract exactly two entities of a tuple with their full names. To address these challenges, we propose two novel approaches using encoder-decoder architecture. We first propose a new representation scheme for relation tuples (Table TABREF1) such that it can represent multiple tuples with overlapping entities and different lengths of entities in a simple way. We employ an encoder-decoder model where the decoder extracts one word at a time like machine translation models. At the end of sequence generation, due to the unique representation of the tuples, we can extract the tuples from the sequence of words. Although this model performs quite well, generating one word at a time is somewhat unnatural for this task. Each tuple has exactly two entities and one relation, and each entity appears as a continuous text span in a sentence. The most effective way to identify them is to find their start and end location in the sentence. Each relation tuple can then be represented using five items: start and end location of the two entities and the relation between them (see Table TABREF1). Keeping this in mind, we propose a pointer network-based decoding framework. This decoder consists of two pointer networks which find the start and end location of the two entities in a sentence, and a classification network which identifies the relation between them. At every time step of the decoding, this decoder extracts an entire relation tuple, not just a word. Experiments on the New York Times (NYT) datasets show that our approaches work effectively for this task and achieve state-of-the-art performance. To summarize, the contributions of this paper are as follows:

(1) We propose a new representation scheme for relation tuples such that an encoder-decoder model, which extracts one word at each time step, can still find multiple tuples with overlapping entities and tuples with multi-token entities from sentences. We also propose a masking-based copy mechanism to extract the entities from the source sentence only.

(2) We propose a modification in the decoding framework with pointer networks to make the encoder-decoder model more suitable for this task. At every time step, this decoder extracts an entire relation tuple, not just a word. This new decoding framework helps in speeding up the training process and uses less resources (GPU memory). This will be an important factor when we move from sentence-level tuple extraction to document-level extraction.

(3) Experiments on the NYT datasets show that our approaches outperform all the previous state-of-the-art models significantly and set a new benchmark on these datasets.

## Task Description

A relation tuple consists of two entities and a relation. Such tuples can be found in sentences where an entity is a text span in a sentence and a relation comes from a pre-defined set $R$. These tuples may share one or both entities among them. Based on this, we divide the sentences into three classes: (i) No Entity Overlap (NEO): A sentence in this class has one or more tuples, but they do not share any entities. (ii) Entity Pair Overlap (EPO): A sentence in this class has more than one tuple, and at least two tuples share both the entities in the same or reverse order. (iii) Single Entity Overlap (SEO): A sentence in this class has more than one tuple and at least two tuples share exactly one entity. It should be noted that a sentence can belong to both EPO and SEO classes. Our task is to extract all relation tuples present in a sentence.

## Encoder-Decoder Architecture

In this task, input to the system is a sequence of words, and output is a set of relation tuples. In our first approach, we represent each tuple as entity1 ; entity2 ; relation. We use `;' as a separator token to separate the tuple components. Multiple tuples are separated using the `$\vert $' token. We have included one example of such representation in Table TABREF1. Multiple relation tuples with overlapping entities and different lengths of entities can be represented in a simple way using these special tokens (; and $\vert $). During inference, after the end of sequence generation, relation tuples can be extracted easily using these special tokens. Due to this uniform representation scheme, where entity tokens, relation tokens, and special tokens are treated similarly, we use a shared vocabulary between the encoder and decoder which includes all of these tokens. The input sentence contains clue words for every relation which can help generate the relation tokens. We use two special tokens so that the model can distinguish between the beginning of a relation tuple and the beginning of a tuple component. To extract the relation tuples from a sentence using the encoder-decoder model, the model has to generate the entity tokens, find relation clue words and map them to the relation tokens, and generate the special tokens at appropriate time. Our experiments show that the encoder-decoder models can achieve this quite effectively.

## Encoder-Decoder Architecture ::: Embedding Layer & Encoder

We create a single vocabulary $V$ consisting of the source sentence tokens, relation names from relation set $R$, special separator tokens (`;', `$\vert $'), start-of-target-sequence token (SOS), end-of-target-sequence token (EOS), and unknown word token (UNK). Word-level embeddings are formed by two components: (1) pre-trained word vectors (2) character embedding-based feature vectors. We use a word embedding layer $\mathbf {E}_w \in \mathbb {R}^{\vert V \vert \times d_w}$ and a character embedding layer $\mathbf {E}_c \in \mathbb {R}^{\vert A \vert \times d_c}$, where $d_w$ is the dimension of word vectors, $A$ is the character alphabet of input sentence tokens, and $d_c$ is the dimension of character embedding vectors. Following BIBREF7 (BIBREF7), we use a convolutional neural network with max-pooling to extract a feature vector of size $d_f$ for every word. Word embeddings and character embedding-based feature vectors are concatenated ($\Vert $) to obtain the representation of the input tokens.

A source sentence $\mathbf {S}$ is represented by vectors of its tokens $\mathbf {x}_1, \mathbf {x}_2,....,\mathbf {x}_n$, where $\mathbf {x}_i \in \mathbb {R}^{(d_w+d_f)}$ is the vector representation of the $i$th word and $n$ is the length of $\mathbf {S}$. These vectors $\mathbf {x}_i$ are passed to a bi-directional LSTM BIBREF8 (Bi-LSTM) to obtain the hidden representation $\mathbf {h}_i^E$. We set the hidden dimension of the forward and backward LSTM of the Bi-LSTM to be $d_h/2$ to obtain $\mathbf {h}_i^E \in \mathbb {R}^{d_h}$, where $d_h$ is the hidden dimension of the sequence generator LSTM of the decoder described below.

## Encoder-Decoder Architecture ::: Word-level Decoder & Copy Mechanism

A target sequence $\mathbf {T}$ is represented by only word embedding vectors of its tokens $\mathbf {y}_0, \mathbf {y}_1,....,\mathbf {y}_m$ where $\mathbf {y}_i \in \mathbb {R}^{d_w}$ is the embedding vector of the $i$th token and $m$ is the length of the target sequence. $\mathbf {y}_0$ and $\mathbf {y}_m$ represent the embedding vector of the SOS and EOS token respectively. The decoder generates one token at a time and stops when EOS is generated. We use an LSTM as the decoder and at time step $t$, the decoder takes the source sentence encoding ($\mathbf {e}_t \in \mathbb {R}^{d_h}$) and the previous target word embedding ($\mathbf {y}_{t-1}$) as the input and generates the hidden representation of the current token ($\mathbf {h}_t^D \in \mathbb {R}^{d_h}$). The sentence encoding vector $\mathbf {e}_t$ can be obtained using attention mechanism. $\mathbf {h}_t^D$ is projected to the vocabulary $V$ using a linear layer with weight matrix $\mathbf {W}_v \in \mathbb {R}^{\vert V \vert \times d_h}$ and bias vector $\mathbf {b}_v \in \mathbb {R}^{\vert V \vert }$ (projection layer).

$\mathbf {o}_t$ represents the normalized scores of all the words in the embedding vocabulary at time step $t$. $\mathbf {h}_{t-1}^D$ is the previous hidden state of the LSTM.

The projection layer of the decoder maps the decoder output to the entire vocabulary. During training, we use the gold label target tokens directly. However, during inference, the decoder may predict a token from the vocabulary which is not present in the current sentence or the set of relations or the special tokens. To prevent this, we use a masking technique while applying the softmax operation at the projection layer. We mask (exclude) all words of the vocabulary except the current source sentence tokens, relation tokens, separator tokens (`;', `$\vert $'), UNK, and EOS tokens in the softmax operation. To mask (exclude) some word from softmax, we set the corresponding value in $\hat{\mathbf {o}}_t$ at $-\infty $ and the corresponding softmax score will be zero. This ensures the copying of entities from the source sentence only. We include the UNK token in the softmax operation to make sure that the model generates new entities during inference. If the decoder predicts an UNK token, we replace it with the corresponding source word which has the highest attention score. During inference, after decoding is finished, we extract all tuples based on the special tokens, remove duplicate tuples and tuples in which both entities are the same or tuples where the relation token is not from the relation set. This model is referred to as WordDecoding (WDec) henceforth.

## Encoder-Decoder Architecture ::: Pointer Network-Based Decoder

In the second approach, we identify the entities in the sentence using their start and end locations. We remove the special tokens and relation names from the word vocabulary and word embeddings are used only at the encoder side along with character embeddings. We use an additional relation embedding matrix $\mathbf {E}_r \in \mathbb {R}^{\vert R \vert \times d_r}$ at the decoder side of our model, where $R$ is the set of relations and $d_r$ is the dimension of relation vectors. The relation set $R$ includes a special relation token EOS which indicates the end of the sequence. Relation tuples are represented as a sequence $T=y_0, y_1,....,y_m$, where $y_t$ is a tuple consisting of four indexes in the source sentence indicating the start and end location of the two entities and a relation between them (see Table TABREF1). $y_0$ is a dummy tuple that represents the start tuple of the sequence and $y_m$ functions as the end tuple of the sequence which has EOS as the relation (entities are ignored for this tuple). The decoder consists of an LSTM with hidden dimension $d_h$ to generate the sequence of tuples, two pointer networks to find the two entities, and a classification network to find the relation of a tuple. At time step $t$, the decoder takes the source sentence encoding ($\mathbf {e}_t \in \mathbb {R}^{d_h}$) and the representation of all previously generated tuples ($\mathbf {y}_{prev}=\sum _{j=0}^{t-1}\mathbf {y}_{j}$) as the input and generates the hidden representation of the current tuple, $\mathbf {h}_t^D \in \mathbb {R}^{d_h}$. The sentence encoding vector $\mathbf {e}_t$ is obtained using an attention mechanism as explained later. Relation tuples are a set and to prevent the decoder from generating the same tuple again, we pass the information about all previously generated tuples at each time step of decoding. $\mathbf {y}_j$ is the vector representation of the tuple predicted at time step $j < t$ and we use the zero vector ($\mathbf {y}_0=\overrightarrow{0}$) to represent the dummy tuple $y_0$. $\mathbf {h}_{t-1}^D$ is the hidden state of the LSTM at time step $t-1$.

## Encoder-Decoder Architecture ::: Relation Tuple Extraction

After obtaining the hidden representation of the current tuple $\mathbf {h}_t^D$, we first find the start and end pointers of the two entities in the source sentence. We concatenate the vector $\mathbf {h}_t^D$ with the hidden vectors $\mathbf {h}_i^E$ of the encoder and pass them to a Bi-LSTM layer with hidden dimension $d_p$ for forward and backward LSTM. The hidden vectors of this Bi-LSTM layer $\mathbf {h}_i^k \in \mathbb {R}^{2d_p}$ are passed to two feed-forward networks (FFN) with softmax to convert each hidden vector into two scalar values between 0 and 1. Softmax operation is applied across all the words in the input sentence. These two scalar values represent the probability of the corresponding source sentence token to be the start and end location of the first entity. This Bi-LSTM layer with the two feed-forward layers is the first pointer network which identifies the first entity of the current relation tuple.

where $\mathbf {W}_s^1 \in \mathbb {R}^{1 \times 2d_p}$, $\mathbf {W}_e^1 \in \mathbb {R}^{1 \times 2d_p}$, ${b}_s^1$, and ${b}_e^1$ are the weights and bias parameters of the feed-forward layers. ${s}_i^1$, ${e}_i^1$ represent the normalized probabilities of the $i$th source word being the start and end token of the first entity of the predicted tuple. We use another pointer network to extract the second entity of the tuple. We concatenate the hidden vectors $\mathbf {h}_i^k$ with $\mathbf {h}_t^D$ and $\mathbf {h}_i^E$ and pass them to the second pointer network to obtain ${s}_i^2$ and ${e}_i^2$, which represent the normalized probabilities of the $i$th source word being the start and end of the second entity. These normalized probabilities are used to find the vector representation of the two entities, $\mathbf {a}_t^1$ and $\mathbf {a}_t^2$.

We concatenate the entity vector representations $\mathbf {a}_t^1$ and $\mathbf {a}_t^2$ with $\mathbf {h}_t^D$ and pass it to a feed-forward network (FFN) with softmax to find the relation. This feed-forward layer has a weight matrix $\mathbf {W}_r \in \mathbb {R}^{\vert R \vert \times (8d_p + d_h)}$ and a bias vector $\mathbf {b}_r \in \mathbb {R}^{\vert R \vert }$.

$\mathbf {r}_t$ represents the normalized probabilities of the relation at time step $t$. The relation embedding vector $\mathbf {z}_t$ is obtained using $\mathrm {argmax}$ of $\mathbf {r}_t$ and $\mathbf {E}_r$. $\mathbf {y}_t \in \mathbb {R}^{(8d_p + d_r)}$ is the vector representation of the tuple predicted at time step $t$. During training, we pass the embedding vector of the gold label relation in place of the predicted relation. So the $\mathrm {argmax}$ function does not affect the back-propagation during training. The decoder stops the sequence generation process when the predicted relation is EOS. This is the classification network of the decoder.

During inference, we select the start and end location of the two entities such that the product of the four pointer probabilities is maximized keeping the constraints that the two entities do not overlap with each other and $1 \le b \le e \le n$ where $b$ and $e$ are the start and end location of the corresponding entities. We first choose the start and end location of entity 1 based on the maximum product of the corresponding start and end pointer probabilities. Then we find entity 2 in a similar way excluding the span of entity 1 to avoid overlap. The same procedure is repeated but this time we first find entity 2 followed by entity 1. We choose that pair of entities which gives the higher product of four pointer probabilities between these two choices. This model is referred to as PtrNetDecoding (PNDec) henceforth.

## Encoder-Decoder Architecture ::: Attention Modeling

We experimented with three different attention mechanisms for our word-level decoding model to obtain the source context vector $\mathbf {e}_t$:

(1) Avg.: The context vector is obtained by averaging the hidden vectors of the encoder: $\mathbf {e}_t=\frac{1}{n}\sum _{i=1}^n \mathbf {h}_i^E$

(2) N-gram: The context vector is obtained by the N-gram attention mechanism of BIBREF9 (BIBREF9) with N=3.

$\textnormal {a}_i^g=(\mathbf {h}_n^{E})^T \mathbf {V}^g \mathbf {w}_i^g$, $\alpha ^g = \mathrm {softmax}(\mathbf {a}^g)$

$\mathbf {e}_t=[\mathbf {h}_n^E \Vert \sum _{g=1}^N \mathbf {W}^g (\sum _{i=1}^{\vert G^g \vert } \alpha _i^g \mathbf {w}_i^g)$]

Here, $\mathbf {h}_n^E$ is the last hidden state of the encoder, $g \in \lbrace 1, 2, 3\rbrace $ refers to the word gram combination, $G^g$ is the sequence of g-gram word representations for the input sentence, $\mathbf {w}_i^g$ is the $i$th g-gram vector (2-gram and 3-gram representations are obtained by average pooling), $\alpha _i^g$ is the normalized attention score for the $i$th g-gram vector, $\mathbf {W} \in \mathbb {R}^{d_h \times d_h}$ and $\mathbf {V} \in \mathbb {R}^{d_h \times d_h}$ are trainable parameters.

(3) Single: The context vector is obtained by the attention mechanism proposed by BIBREF10 (BIBREF10). This attention mechanism gives the best performance with the word-level decoding model.

$\mathbf {u}_t^i = \mathbf {W}_{u} \mathbf {h}_i^E, \quad \mathbf {q}_t^i = \mathbf {W}_{q} \mathbf {h}_{t-1}^D + \mathbf {b}_{q}$,

$\textnormal {a}_t^i = \mathbf {v}_a \tanh (\mathbf {q}_t^i + \mathbf {u}_t^i), \quad \alpha _t = \mathrm {softmax}(\mathbf {a}_t)$,

$\mathbf {e}_t = \sum _{i=1}^n \alpha _t^i \mathbf {h}_i^E$

where $\mathbf {W}_u \in \mathbb {R}^{d_h \times d_h}$, $\mathbf {W}_q \in \mathbb {R}^{d_h \times d_h}$, and $\mathbf {v}_a \in \mathbb {R}^{d_h}$ are all trainable attention parameters and $\mathbf {b}_q \in \mathbb {R}^{d_h}$ is a bias vector. $\alpha _t^i$ is the normalized attention score of the $i$th source word at the decoding time step $t$.

For our pointer network-based decoding model, we use three variants of the single attention model. First, we use $\mathbf {h}_{t-1}^D$ to calculate $\mathbf {q}_t^i$ in the attention mechanism. Next, we use $\mathbf {y}_{prev}$ to calculate $\mathbf {q}_t^i$, where $\mathbf {W}_q \in \mathbb {R}^{(8d_p + d_r) \times d_h}$. In the final variant, we obtain the attentive context vector by concatenating the two attentive vectors obtained using $\mathbf {h}_{t-1}^D$ and $\mathbf {y}_{prev}$. This gives the best performance with the pointer network-based decoding model. These variants are referred to as $\mathrm {dec_{hid}}$, $\mathrm {tup_{prev}}$, and $\mathrm {combo}$ in Table TABREF17.

## Encoder-Decoder Architecture ::: Loss Function

We minimize the negative log-likelihood loss of the generated words for word-level decoding ($\mathcal {L}_{word}$) and minimize the sum of negative log-likelihood loss of relation classification and the four pointer locations for pointer network-based decoding ($\mathcal {L}_{ptr}$).

$v_t^b$ is the softmax score of the target word at time step $t$ for the word-level decoding model. $r$, $s$, and $e$ are the softmax score of the corresponding true relation label, true start and end pointer location of an entity. $b$, $t$, and $c$ refer to the $b$th training instance, $t$th time step of decoding, and the two entities of a tuple respectively. $B$ and $T$ are the batch size and maximum time step of the decoder respectively.

## Experiments ::: Datasets

We focus on the task of extracting multiple tuples with overlapping entities from sentences. We choose the New York Times (NYT) corpus for our experiments. This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations. We name this version as NYT29. We select 10% of the original training data and use it as the validation dataset. The remaining 90% is used for training. We include statistics of the training and test datasets in Table TABREF11.

## Experiments ::: Parameter Settings

We run the Word2Vec BIBREF12 tool on the NYT corpus to initialize the word embeddings. The character embeddings and relation embeddings are initialized randomly. All embeddings are updated during training. We set the word embedding dimension $d_w=300$, relation embedding dimension $d_r=300$, character embedding dimension $d_c=50$, and character-based word feature dimension $d_f=50$. To extract the character-based word feature vector, we set the CNN filter width at 3 and the maximum length of a word at 10. The hidden dimension $d_h$ of the decoder LSTM cell is set at 300 and the hidden dimension of the forward and the backward LSTM of the encoder is set at 150. The hidden dimension of the forward and backward LSTM of the pointer networks is set at $d_p=300$. The model is trained with mini-batch size of 32 and the network parameters are optimized using Adam BIBREF13. Dropout layers with a dropout rate fixed at $0.3$ are used in our network to avoid overfitting.

## Experiments ::: Baselines and Evaluation Metrics

We compare our model with the following state-of-the-art joint entity and relation extraction models:

(1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM. Sequence LSTM is used to identify all the entities first and then Tree LSTM is used to find the relation between all pairs of entities.

(2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder. They used a Cartesian product of entity tags and relation tags to encode the entity and relation information together. This model does not work when tuples have overlapping entities.

(3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations. It copies only the last token of an entity from the source sentence. Their best performing multi-decoder model is trained with a fixed number of decoders where each decoder extracts one tuple.

(4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction. A high-level RL finds the relation and a low-level RL identifies the two entities using a sequence tagging approach. This sequence tagging approach cannot always ensure extraction of exactly two entities.

(5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them. They use graph convolution network (GCN) to predict the relations of every edge and then filter out some of the relations.

(6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data. The encoder uses the source tokens as its vocabulary and the decoder uses the entire Wikidata BIBREF15 entity IDs and relation IDs as its vocabulary. The encoder takes the source sentence as input and the decoder outputs the two entity IDs and relation ID for every tuple. During training, it uses the mapping of entity names and their Wikidata IDs of the entire Wikidata for proper alignment. Our task of extracting relation tuples with the raw entity names from a sentence is more challenging since entity names are not of fixed length. Our more generic approach is also helpful for extracting new entities which are not present in the existing knowledge bases such as Wikidata. We use their N-gram attention mechanism in our model to compare its performance with other attention models (Table TABREF17).

We use the same evaluation method used by BIBREF11 (BIBREF11) in their experiments. We consider the extracted tuples as a set and remove the duplicate tuples. An extracted tuple is considered as correct if the corresponding full entity names are correct and the relation is also correct. We report precision, recall, and F1 score for comparison.

## Experiments ::: Experimental Results

Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores and PNDec achieves $4.2\%$ and $2.9\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.

## Analysis and Discussion ::: Ablation Studies

We include the performance of different attention mechanisms with our WordDecoding model, effects of our masking-based copy mechanism, and ablation results of three variants of the single attention mechanism with our PtrNetDecoding model in Table TABREF17. WordDecoding with single attention achieves the highest F1 score on both datasets. We also see that our copy mechanism improves F1 scores by around 4â€“7% in each attention mechanism with both datasets. PtrNetDecoding achieves the highest F1 scores when we combine the two attention mechanisms with respect to the previous hidden vector of the decoder LSTM ($\mathbf {h}_{t-1}^D$) and representation of all previously extracted tuples ($\mathbf {y}_{prev}$).

## Analysis and Discussion ::: Performance Analysis

From Table TABREF15, we see that CopyR, HRL, and our models achieve significantly higher F1 scores on the NYT24 dataset than the NYT29 dataset. Both datasets have a similar set of relations and similar texts (NYT). So task-wise both datasets should pose a similar challenge. However, the F1 scores suggest that the NYT24 dataset is easier than NYT29. The reason is that NYT24 has around 72.0% of overlapping tuples between the training and test data (% of test tuples that appear in the training data with different source sentences). In contrast, NYT29 has only 41.7% of overlapping tuples. Due to the memorization power of deep neural networks, it can achieve much higher F1 score on NYT24. The difference between the F1 scores of WordDecoding and PtrNetDecoding on NYT24 is marginally higher than NYT29, since WordDecoding has more trainable parameters (about 27 million) than PtrNetDecoding (about 24.5 million) and NYT24 has very high tuple overlap. However, their ensemble versions achieve closer F1 scores on both datasets.

Despite achieving marginally lower F1 scores, the pointer network-based model can be considered more intuitive and suitable for this task. WordDecoding may not extract the special tokens and relation tokens at the right time steps, which is critical for finding the tuples from the generated sequence of words. PtrNetDecoding always extracts two entities of varying length and a relation for every tuple. We also observe that PtrNetDecoding is more than two times faster and takes one-third of the GPU memory of WordDecoding during training and inference. This speedup and smaller memory consumption are achieved due to the fewer number of decoding steps of PtrNetDecoding compared to WordDecoding. PtrNetDecoding extracts an entire tuple at each time step, whereas WordDecoding extracts just one word at each time step and so requires eight time steps on average to extract a tuple (assuming that the average length of an entity is two). The softmax operation at the projection layer of WordDecoding is applied across the entire vocabulary and the vocabulary size can be large (more than 40,000 for our datasets). In case of PtrNetDecoding, the softmax operation is applied across the sentence length (maximum of 100 in our experiments) and across the relation set (24 and 29 for our datasets). The costly softmax operation and the higher number of decoding time steps significantly increase the training and inference time for WordDecoding. The encoder-decoder model proposed by BIBREF9 (BIBREF9) faces a similar softmax-related problem as their target vocabulary contains the entire Wikidata entity IDs and relation IDs which is in the millions. HRL, which uses a deep reinforcement learning algorithm, takes around 8x more time to train than PtrNetDecoding with a similar GPU configuration. The speedup and smaller memory consumption will be useful when we move from sentence-level extraction to document-level extraction, since document length is much higher than sentence length and a document contains a higher number of tuples.

## Analysis and Discussion ::: Error Analysis

The relation tuples extracted by a joint model can be erroneous for multiple reasons such as: (i) extracted entities are wrong; (ii) extracted relations are wrong; (iii) pairings of entities with relations are wrong. To see the effects of the first two reasons, we analyze the performance of HRL and our models on entity generation and relation generation separately. For entity generation, we only consider those entities which are part of some tuple. For relation generation, we only consider the relations of the tuples. We include the performance of our two models and HRL on entity generation and relation generation in Table TABREF20. Our proposed models perform better than HRL on both tasks. Comparing our two models, PtrNetDecoding performs better than WordDecoding on both tasks, although WordDecoding achieves higher F1 scores in tuple extraction. This suggests that PtrNetDecoding makes more errors while pairing the entities with relations. We further analyze the outputs of our models and HRL to determine the errors due to ordering of entities (Order), mismatch of the first entity (Ent1), and mismatch of the second entity (Ent2) in Table TABREF21. WordDecoding generates fewer errors than the other two models in all the categories and thus achieves the highest F1 scores on both datasets.

## Related Work

Traditionally, researchers BIBREF0, BIBREF1, BIBREF2, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25 used a pipeline approach for relation tuple extraction where relations were identified using a classification network after all entities were detected. BIBREF26 (BIBREF26) used an encoder-decoder model to extract multiple relations present between two given entities.

Recently, some researchers BIBREF3, BIBREF4, BIBREF27, BIBREF28 tried to bring these two tasks closer together by sharing their parameters and optimizing them together. BIBREF5 (BIBREF5) used a sequence tagging scheme to jointly extract the entities and relations. BIBREF6 (BIBREF6) proposed an encoder-decoder model with copy mechanism to extract relation tuples with overlapping entities. BIBREF11 (BIBREF11) proposed a joint extraction model based on reinforcement learning (RL). BIBREF14 (BIBREF14) used a graph convolution network (GCN) where they treated each token in a sentence as a node in a graph and edges were considered as relations. BIBREF9 (BIBREF9) used an N-gram attention mechanism with an encoder-decoder model for completion of knowledge bases using distant supervised data.

Encoder-decoder models have been used for many NLP applications such as neural machine translation BIBREF29, BIBREF10, BIBREF30, sentence generation from structured data BIBREF31, BIBREF32, and open information extraction BIBREF33, BIBREF34. Pointer networks BIBREF35 have been used to extract a text span from text for tasks such as question answering BIBREF36, BIBREF37. For the first time, we use pointer networks with an encoder-decoder model to extract relation tuples from sentences.

## Conclusion

Extracting relation tuples from sentences is a challenging task due to different length of entities, the presence of multiple tuples, and overlapping of entities among tuples. In this paper, we propose two novel approaches using encoder-decoder architecture to address this task. Experiments on the New York Times (NYT) corpus show that our proposed models achieve significantly improved new state-of-the-art F1 scores. As future work, we would like to explore our proposed models for a document-level tuple extraction task.

## Acknowledgments

We would like to thank the anonymous reviewers for their valuable and constructive comments on this paper.
