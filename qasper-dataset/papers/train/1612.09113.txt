# Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies

**Paper ID:** 1612.09113

## Abstract

In this paper we present a novel Neural Network algorithm for conducting semisupervised learning for sequence labeling tasks arranged in a linguistically motivated hierarchy. This relationship is exploited to regularise the representations of supervised tasks by backpropagating the error of the unsupervised task through the supervised tasks. We introduce a neural network where lower layers are supervised by downstream tasks and the final layer task is an auxiliary unsupervised task. The architecture shows improvements of up to two percentage points F β=1 for Chunking compared to a plausible baseline.

## Introduction

It is natural to think of NLP tasks existing in a hierarchy, with each task building upon the previous tasks. For example, Part of Speech (POS) is known to be an extremely strong feature for Noun Phrase Chunking, and downstream tasks such as greedy Language Modeling (LM) can make use of information about the syntactic and semantic structure recovered from junior tasks in making predictions.

Conversely, information about downstream tasks should also provide information that aids generalisation for junior downstream tasks, a form of semi-supervised learning. Arguably, there is a two-way relationship between each pair of tasks.

Following work such as sogaard2016deep, that exploits such hierarchies in a fully supervised setting, we represent this hierarchical relationship within the structure of a multi-task Recurrent Neural Network (RNN), where junior tasks in the hierarchy are supervised on inner layers and the parameters are jointly optimised during training. Joint optimisation within a hierarchical network acts as a form of regularisation in two ways: first, it forces the network to learn general representations within the parameters of the shared hidden layers BIBREF0 ; second, there is a penalty on the supervised junior layers for forming a representation and making predictions that are inconsistent with senior tasks. Intuitively, we can see how this can be beneficial - when humans receive new information from one task that is inconsistent with with our internal representation of a junior task we update both representations to maintain a coherent view of the world.

By incorporating an unsupervised auxiliary task (e.g. plank2016multilingual) as the most senior layer we can use this structure for semi-supervised learning - the error on the unsupervised tasks penalises junior tasks when their representations and predictions are not consistent. It is the aim of this paper to demonstrate that organising a network in such a way can improve performance. To that end, although we do not achieve state of the art results, we see a small but consistent performance improvement against a baseline. A diagram of our model can be seen in Figure 1 .

Our Contributions:

## Linguistically Motivated Task Hierarchies

When we speak and understand language we are arguably performing many different linguistic tasks at once. At the top level we might be trying to formulate the best possible sequence of words given all of the contextual and prior information, but this requires us to do lower-level tasks like understanding the syntactic and semantic roles of the words we choose in a specific context.

This paper seeks to examine the POS tagging, Chunking and Language Modeling hierarchy and demonstrate that, by developing an algorithm that both exploits this structure and optimises all three jointly, we can improve performance.

## Motivating our Choice of Tasks

In the original introductory paper to Noun Phrase Chunking, abney1991parsing, Chunking is motivated by describing a three-phase process - first, you read the words and assign a Part of Speech tag, you then use a ‘Chunker’ to group these words together into chunks depending on the context and the Parts of Speech, and finally you build a parse tree on top of the chunks.

The parallels between this linguistic description of parsing and our architecture are clear; first, we build a prediction for POS, we then use this prediction to assist in parsing by Chunk, which we then use for greedy Language Modeling. In this hierarchy, we consider Language Modeling as auxiliary - designed to improve performance on POS and Chunking, and so therefore results are not presented for this task.

## Our Model

In our model we represent linguistically motivated hierarchies in a multi-task Bi-Directional Recurrent Neural Network where junior tasks in the hierarchy are supervised at lower layers.This architecture builds upon sogaard2016deep, but is adapted in two ways: first, we add an unsupervised sequence labeling task (Language Modeling), second, we add a low-dimensional embedding layer between tasks in the hierarchy to learn dense representations of label tags. In addition to sogaard2016deep.

Work such as mirowski-vlachos:2015:ACL-IJCNLP in which incorporating syntactic dependencies improves performance, demonstrates the benefits of incorporating junior tasks in prediction.

Our neural network has one hidden layer, after which each successive task is supervised on the next layer. In addition, we add skip connections from the hidden layer to the senior supervised layers to allow layers to ignore information from junior tasks.

A diagram of our network can be seen in Figure 1 .

## Supervision of Multiple Tasks

Our model has 3 sources of error signals - one for each task. Since each task is categorical we use the discrete cross entropy to calculate the loss for each task: $
H(p, q) = - \sum _{i}^{n_{labels}} p(label_i) \ log \ q(label_i)
$ 

Where $n_{labels}$ is the number of labels in the task, $q(label_i)$ is the probability of label $i$ under the predicted distribution, and $p(label_i)$ is the probability of label $i$ in the true distribution (in this case, a one-hot vector).

During training with fully supervised data (POS, Chunk and Language Modeling), we optimise the mean cross entropy: $
Loss(x,y) = \frac{1}{n} \sum _{i}^{n} H(y, f_{task_i}(x))
$ 

Where $f_{task_i}(x)$ is the predicted distribution on task number $i$ from our model.

When labels are missing, we drop the associated cross entropy terms from the loss, and omit the cross entropy calculation from the forward pass.

## Bi-Directional RNNs

Our network is a Bi-Directional Recurrent Neural Network (Bi-RNN) (schuster1997bidirectional) with Gated Recurrent Units (GRUs) (cho2014properties, chung2014empirical).

In a Bi-Directional RNN we run left-to-right through the sentence, and then we run right-to-left. This gives us two hidden states at time step t - one from the left-to-right pass, and one from the right-to-left pass. These are then combined to provide a probability distribution for the tag token conditioned on all of the other words in the sentence.

## Implementation Details

During training we alternate batches of data with POS and Chunk and Language Model labels with batches of just Language Modeling according to some probability $ 0 < \gamma < 1$ .

We train our model using the ADAM (kingma2014adam) optimiser for 100 epochs, where one epoch corresponds to one pass through the labelled data. We train in batch sizes of $32\times 32$ .

## Data Sets

We present our experiments on two data sets - CoNLL 2000 Chunking data set (tjong2000introduction) which is derived from the Penn Tree Bank newspaper text (marcus1993building), and the Genia biomedical corpus (kim2003genia), derived from biomedical article abstracts.

These two data sets were chosen since they perform differently under the same classifiers BIBREF1 . The unlabelled data for semi-supervised learning for newspaper text is the Penn Tree Bank, and for biomedical text it a custom data set of Pubmed abstracts.

## Baseline Results

We compare the results of our model to a baseline multi-task architecture inspired by yang2016multi. In our baseline model there are no explicit connections between tasks - the only shared parameters are in the hidden layer.

We also present results for our hierarchical model where there is no training on unlabelled data (but there is the LM) and confirm previous results that arranging tasks in a hierarchy improves performance. Results for both models can be seen for POS in Table 2 and for Chunk in Table 1 .

## Semi-Supervised Experiments

Experiments showing the effects of our semi-supervised learning regime on models initialised both with and without pre-trained word embeddings can be seen in Tables 3 and 4 .

In models without pre-trained word embeddings we see a significant improvement associated with the semi-supervised regime.

However, we observe that for models with pre-trained word embeddings, the positive impact of semi-supervised learning is less significant. This is likely due to the fact some of the regularities learned using the language model are already contained within the embedding. In fact, the training schedule of SENNA is similar to that of neural language modelling (collobert2011natural).

Two other points are worthy of mention in the experiments with 100 % of the training data. First, the impact of semi-supervised learning on biomedical data is significantly less than on newspaper data. This is likely due to the smaller overlap between vocabularies in the training set and vocabularies in the test set. Second, the benefits for POS are smaller than they are for Chunking - this is likely due to the POS weights being more heavily regularised by receiving gradients from both the Chunking and Language Modeling loss.

Finally, we run experiments with only a fraction of the training data to see whether our semi-supervised approach makes our models more robust (Tables 3 and 4 ). Here, we find variable but consistent improvement in the performance of our tasks even at 1 % of the original training data.

## Label Embeddings

Our model structure includes an embedding layer between each task. This layer allows us to learn low-dimensional vector representations of labels, and expose regularities in a way similar to e.g. mikolov2013distributed.

We demonstrate this in Figure 2 where we present a T-SNE visualisation of our label embeddings for Chunking and observe clusters along the diagonal.

## Conclusions & Further Work

In this paper we have demonstrated two things: a way to use hierarchical neural networks to conduct semi-supervised learning and the associated performance improvements, and a way to learn low-dimensional embeddings of labels.

Future work would investigate how to address Catastrophic Forgetting BIBREF2 (the problem in Neural Networks of forgetting previous tasks when training on a new task), which leads to the requirement for the mix parameter $\gamma $ in our algorithm, and prevents such models such as ours from scaling to larger supervised task hierarchies where the training data may be various and disjoint.
