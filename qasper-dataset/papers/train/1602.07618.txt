# From quantum foundations via natural language meaning to a theory of everything

**Paper ID:** 1602.07618

## Abstract

In this paper we argue for a paradigmatic shift from `reductionism' to `togetherness'. In particular, we show how interaction between systems in quantum theory naturally carries over to modelling how word meanings interact in natural language. Since meaning in natural language, depending on the subject domain, encompasses discussions within any scientific discipline, we obtain a template for theories such as social interaction, animal behaviour, and many others.

## ...in the beginning was ⊗\otimes 

No physicists! ...the symbol INLINEFORM0 above does not stand for the operation that turns two Hilbert spaces into the smallest Hilbert space in which the two given ones bilinearly embed. No category-theoreticians! ...neither does it stand for the composition operation that turns any pair of objects (and morphisms) in a monoidal category into another object, and that is subject to a horrendous bunch of conditions that guaranty coherence with the remainder of the structure. Instead, this is what it means: INLINEFORM1 

More specifically, it represents the togetherness of foo INLINEFORM0 and foo INLINEFORM1 without giving any specification of who/what foo INLINEFORM2 and foo INLINEFORM3 actually are. Differently put, it's the new stuff that emerges when foo INLINEFORM4 and foo INLINEFORM5 get together. If they don't like each other at all, this may be a fight. If they do like each other a lot, this may be a marriage, and a bit later, babies. Note that togetherness is vital for the emergence to actually take place, given that it is quite hard to either have a fight, a wedding, or a baby, if there is nobody else around.

It is of course true that in von Neumann's formalisation of quantum theory the tensor product of Hilbert spaces (also denoted by INLINEFORM0 ) plays this role BIBREF0 , giving rise to the emergent phenomenon of entanglement BIBREF1 , BIBREF2 . And more generally, in category theory one can axiomatise composition of objects (again denoted by INLINEFORM1 ) within a symmetric monoidal category BIBREF3 , giving rise to elements that don't simply arise by pairing, just like in the case of the Hilbert space tensor product.

However, in the case of von Neumann's formalisation of quantum theory we are talking about a formalisation which, despite being widely used, its creator von Neumann himself didn't even like BIBREF4 . Moreover, in this formalism INLINEFORM0 only arises as a secondary construct, requiring a detailed description of foo INLINEFORM1 and foo INLINEFORM2 , whose togetherness it describes. What we are after is a `foo-less' conception of INLINEFORM3 . The composition operation INLINEFORM4 in symmetric monoidal categories heads in that direction. However, by making an unnecessary commitment to set-theory, it makes things unnecessarily complicated BIBREF5 . Moreover, while this operation is general enough to accommodate togetherness, it doesn't really tell us anything about it.

The title of this section is a metaphor aimed at confronting the complete disregard that the concept of togetherness has suffered in the sciences, and especially, in physics, where all of the effort has been on describing the individual, typically by breaking its description down to that of even smaller individuals. While, without any doubt, this has been a useful endeavour, it unfortunately has evolved in a rigid doctrine, leaving no space for anything else. The most extreme manifestation of this dogma is the use of the term `theory of everything' in particle physics. We will provide an alternative conceptual template for a theory of everything, supported not only by scientific examples, but also by everyday ones.

Biology evolved from chopping up individual animals in laboratories, to considering them in the context of other other animals and varying environments. the result is the theory of evolution of species. Similarly, our current (still very poor) understanding of the human brain makes it clear that the human brain should not be studied as something in isolation, but as something that fundamentally requires interaction with other brains BIBREF6 . In contemporary audio equipment, music consists of nothing but a strings of zeros and ones. Instead, the entities that truly make up music are pitch, sound, rhythm, chord progression, crescendo, and so on. And in particular, music is not just a bag of these, since their intricate interaction is even more important than these constituents themselves. The same is true for film, where it isn't even that clear what it is made up from, but it does include such things as (easily replaceable) actors, decors, cameras, which all are part of a soup stirred by a director. But again, in contemporary video equipment, it is nothing but a string of zeros and ones.

In fact, everything that goes on in pretty much all modern devices is nothing but zeros and ones. While it was Turing's brilliance to realise that this could in fact be done, and provided a foundation for the theory of computability BIBREF7 , this is in fact the only place where the zeros and ones are truly meaningful, in the form of a Turing machine. Elsewhere, it is nothing but a (universal) representation, with no conceptual qualities regarding the subject matter.

## Formalising togetherness 1: not there yet

So, how does one go about formalising the concept of togetherness? While we don't want an explicit description of the foo involved, we do need some kind of means for identifying foo. Therefore, we simple give each foo a name, say INLINEFORM0 . Then, INLINEFORM1 represents the togetherness of INLINEFORM2 and INLINEFORM3 . We also don't want an explicit description of INLINEFORM4 , so how can we say anything about INLINEFORM5 without explicitly describing INLINEFORM6 , INLINEFORM7 and INLINEFORM8 ?

Well, rather than describing these systems themselves, we could describe their relationships. For example, in a certain theory togetherness could obey the following equation: INLINEFORM0 

That is, togetherness of two copies of something is exactly the same as a single copy, or in simpler terms, one is as good as two. For example, if one is in need of a plumber to fix a pipe, one only needs one. The only thing a second plumber would contribute is a bill for the time he wasted coming to your house. Obviously, this is not the kind of togetherness that we are really interested in, given that this kind adds nothing at all.

A tiny bit more interesting is the case that two is as good as three: INLINEFORM0 

e.g. when something needs to be carried on a staircase, but there really is only space for two people to be involved. Or, when INLINEFORM0 is female and INLINEFORM1 is male, and the goal is reproduction, we have: INLINEFORM2 

(ignoring testosterone induced scuffles and the benefits of natural selection.)

We really won't get very far this manner. One way in which things can be improved is by replacing equations by inequalities. For example, while: INLINEFORM0 

simply means that one of the two is redundant, instead: INLINEFORM0 

can mean that from INLINEFORM0 we can produce INLINEFORM1 , and: INLINEFORM2 

can mean that from INLINEFORM0 and INLINEFORM1 together we can produce INLINEFORM2 , and: INLINEFORM3 

can mean that in the presence of INLINEFORM0 from INLINEFORM1 we can produce INLINEFORM2 , i.e. that INLINEFORM3 is a catalyst.

What we have now is a so-called resource theory, that is, a theory which captures how stuff we care about can be interconverted BIBREF8 . Resource theories allow for quantitative analysis, for example, in terms of a conversion rate: INLINEFORM0 

So evidently we have some genuine substance now.

## Formalising togetherness 2: that's better

But we can still do a lot better. What a resource theory fails to capture (on purpose in fact) is the actual process that converts one resource into another one. So let's fix that problem, and explicitly account for processes.

In terms of togetherness, this means that we bring the fun foo INLINEFORM0 and foo INLINEFORM1 can have together explicitly in the picture. Let: INLINEFORM2 

denote some process that transforms INLINEFORM0 into INLINEFORM1 .Then, given two such processes INLINEFORM2 and INLINEFORM3 we can also consider their togetherness: INLINEFORM4 

Moreover, some processes can be sequentially chained: INLINEFORM0 

We say `some', since INLINEFORM0 has to produce INLINEFORM1 , in order for: INLINEFORM2 

to take place.

Now, here one may end up in a bit of a mess if one isn't clever. In particular, with a bit of thinking one quickly realises that one wants some equations to be obeyed, for example: (f1f2)f3 = f1(f2f3) h(gf) = (hg)f and a bit more sophisticated, also: (g1g2)(f1f2)=(g1f1)(g2f2) There may even be some more equations that one wants to have, but which ones? This turns out to be a very difficult problem. Too difficult in the light of our limited existence in this world. The origin of this problem is that we treat INLINEFORM0 , and also INLINEFORM1 , as algebraic connectives, and that algebra has its roots in set-theory. The larger-than-life problem can be avoided in a manner that is equally elegant as it is simple.

To state that things are together, we just write them down together: INLINEFORM0 

There really is no reason to adjoin the symbol INLINEFORM0 between them. Now, this INLINEFORM1 and INLINEFORM2 will play the role of an input or an output of processes transforming them. Therefore, it will be useful to represent them by a wire: INLINEFORM3 

Then, a process transforming INLINEFORM0 into INLINEFORM1 can be represented by a box: INLINEFORM2 

Togetherness of processes now becomes: INLINEFORM0 

and chaining processes becomes: INLINEFORM0 

In particular, equations ( SECREF3 ), ( SECREF3 ) and ( SECREF3 ) become: INLINEFORM0 

That is, all equations have become tautologies!

## Anti-cartesian togetherness

One important kind of processes are states: INLINEFORM0 

These are depicted without any inputs, where `no wire' can be read as `nothing' (or `no-foo'). The opposite notion is that of an effect, that is, a process without an output: INLINEFORM0 

borrowing terminology from quantum theory.

We can now identify those theories in which togetherness doesn't yield anything new. Life in such a world is pretty lonely...

A theory of togetherness is cartesian if each state: INLINEFORM0 

decomposes as follows: INLINEFORM0 

So cartesianness means that all possible realisations of two foo-s can be achieved by pairing realisations of the individual foo-s involved. In short, a whole can be described in term of its parts, rendering togetherness a void concept. So very lonely and indeed... But, wait a minute. Why is it then the case that so much of traditional mathematics follows this cartesian template, and that even category theory for a long time has followed a strict cartesian stance? Beats me. Seriously...beats me!

Anyway, an obvious consequence of this is that for those areas where togetherness is a genuinely non-trivial concept, traditional mathematical structures aren't always that useful. That is maybe why social sciences don't make much use of any kind of modern pure mathematics.

And now for something completely different:

A theory of togetherness is anti-cartesian if for each INLINEFORM0 there exists INLINEFORM1 , a special state INLINEFORM2 and a special effect INLINEFORM3 : INLINEFORM4 

which are such that the following equation holds: yanking

The reason for `anti' in the name of this kind of togetherness is the fact that when a theory of togetherness is both cartesian and anti-cartesian, then it is nothing but a theory of absolute death, i.e. it describes a world in which nothing ever happens. Indeed, we have: INLINEFORM0 

That is, the identity is a constant process, always outputting the state INLINEFORM0 , independent of what the input is. And if that isn't weird enough, any arbitrary process INLINEFORM1 does the same: INLINEFORM2 

Therefore, any anti-cartesian theory of togetherness that involves some aspect of change cannot be cartesian, and hence will have interesting stuff emerging from togetherness.

## Example 1: quantum theory

Anti-cartesian togetherness is a very particular alternative to cartesian togetherness (contra any theory that fails to be cartesian). So one may wonder whether there are any interesting examples. And yes, there are! One example is quantum entanglement in quantum theory. That is in fact where the author's interest in anti-cartesian togetherness started BIBREF15 , BIBREF16 , BIBREF14 . As shown in these papers, equation ( SECREF4 ) pretty much embodies the phenomenon of quantum teleportation BIBREF19 . The full-blown description of quantum teleportation goes as follows BIBREF20 , BIBREF21 , BIBREF13 : telefull It is not important to fully understand the details here. What is important is to note that the bit of this diagram corresponding to equation ( SECREF4 ) is the bold wire which zig-zags through it: telefullbit The thin wires and the boxes labelled INLINEFORM0 are related to the fact that quantum theory is non-deterministic. By conditioning on particular measurement outcomes, teleportation simplifies to BIBREF13 : teledet Equality of the left-hand-side and of the right-hand-side follows directly from equation ( SECREF4 ). While in this picture we eliminated quantum non-determinism by conditioning on a measurement outcome, there still is something very `quantum' going on here: Alice's (conditioned) measurement is nothing like a passive observation, but a highly non-trivial intervention that makes Alice's state INLINEFORM1 appear at Bob's side: mapsto

Let's analyse more carefully what's going on here by explicitly distinguishing the top layer and the bottom layer of this diagram: bottomtop The bottom part: bottom consists of the state INLINEFORM0 together with a special INLINEFORM1 -state, while the top part: top includes the corresponding INLINEFORM2 -effect, as well as an output. By making the bottom part and the top part interact, and, in particular, the INLINEFORM3 and the INLINEFORM4 , the state INLINEFORM5 ends up at the output of the top part.

A more sophisticated variation on the same theme makes it much clearer which mechanism is going on here. Using equation ( SECREF4 ), the diagram: telecompl1pre reduces to: INLINEFORM0 

The grey dot labeled INLINEFORM0 is some (at this point not important) unitary quantum operation BIBREF13 . Let us again consider the bottom and top parts: telecompl1 The top part is a far more sophisticated measurement consisting mainly of INLINEFORM1 -s. Also the bottom part is a lot more sophisticated, involving many INLINEFORM2 -s. These now cause a highly non-trivial interaction of the three states INLINEFORM3 , INLINEFORM4 and INLINEFORM5 . Why we have chosen this particular example will become clear in the next section. What is important to note is that the overall state and overall effect have to be chosen in a very particular way to create the desired interaction, similarly to an old-fashion telephone switchboard that has to be connected in a very precise manner in order to realise the right connection.

## Example 2: natural language meaning

Another example of anti-cartesian togetherness is the manner in which word meanings interact in natural language! Given that logic originated in natural language, when Aristotle analysed arguments involving `and', `if...then', `or', etc., anti-cartesianness can be conceived as some new kind of logic! So what are INLINEFORM0 and INLINEFORM1 in this context?

In order to understand what INLINEFORM0 is, we need to understand the mathematics of grammar. The study of the mathematical structure of grammar has indicated that the fundamental things making up sentences are not the words, but some atomic grammatical types, such as the noun-type and the sentence-type BIBREF23 , BIBREF24 , BIBREF25 . The transitive verb-type is not an atomic grammatical type, but a composite made up of two noun-types and one sentence-type. Hence, particularly interesting here is that atomic doesn't really mean smallest...

On the other hand, just like in particle physics where we have particles and anti-particles, the atomic types include types as well as anti-types. But unlike in particle physics, there are two kinds of anti-types, namely left ones and right ones. This makes language even more non-commutative than quantum theory!

All of this becomes much clearer when considering an example. Let INLINEFORM0 denote the atomic noun-type and let INLINEFORM1 and INLINEFORM2 be the corresponding anti-types. Let INLINEFORM3 denote the atomic sentence-type. Then the non-atomic transitive verb-type is INLINEFORM4 . Intuitively, it is easy to understand why. Consider a transitive verb, like `hate'. Then, simply saying `hate' doesn't convey any useful information, until, we also specify `whom' hates `whom'. That's exactly the role of the anti-types: they specify that in order to form a meaningful sentence, a noun is needed on the left, and a noun is needed on the right: INLINEFORM5 

Then, INLINEFORM0 and INLINEFORM1 cancel out, and so do INLINEFORM2 and INLINEFORM3 . What remains is INLINEFORM4 , confirming that `Alice hates Bob' is a grammatically well-typed sentence. We can now depict the cancelations as follows: hates and bingo, we found INLINEFORM5 !

While the mathematics of sentence structure has been explored now for some 80 years, the fact that INLINEFORM0 -s can account for grammatical structure is merely a 15 years old idea BIBREF26 . So what are the INLINEFORM1 -s? That is an even more recent story in which we were involved, and in fact, for which we took inspiration from the story of the previous section BIBREF27 . While INLINEFORM2 -s are about grammar, INLINEFORM3 -s are about meaning.

The distributional paradigm for natural language meaning states that meaning can be represented by vectors in a vector space BIBREF28 . Until recently, grammatical structure was essentially ignored in doing so, and in particular, there was no theory for how to compute the meaning of a sentence, given the meanings of its words. Our new compositional distributional model of meaning of BIBREF29 does exactly that.

In order to explain how this compositional distributional model of meaning works, let's get back to our example. Since we have grammatical types around, the meaning vectors should respect grammatical structure, that is, the vectors representing compound types should themselves live in compound vector spaces. So the string of vectors representing the word meanings of our example would look as follows: hates2 Now we want to put forward a new hypothesis:

Grammar is all about how word meanings interact.

Inspired by the previous section, this can be realised as follows: hates3 where the INLINEFORM0 -s are now interpreted in exactly the same manner as in the previous section. And here is a more sophisticated example: hatescompl where the INLINEFORM1 -labeled grey circle should now be conceived as negating meaning BIBREF29 . The grammatical structure is here: hatescomplgram It is simply taken from a textbook such as BIBREF32 , the meanings of INLINEFORM2 , INLINEFORM3 and INLINEFORM4 can be automatically generated from some corpus, while the meanings of INLINEFORM5 and INLINEFORM6 are just cleverly chosen to be BIBREF33 , BIBREF29 : hatescomplclever In the previous section we already saw that in this way we obtain: hatescompl2 This indeed captures the intended meaning: INLINEFORM7 

where we can think of INLINEFORM0 as being a predicate and INLINEFORM1 as being itself.

So an interesting new aspect of the last example is that some of the meaning vectors of words are simply cleverly chosen, and in particular, involve INLINEFORM0 -s. Hence, we genuinely exploit full-blown anti-cartesianess. What anti-cartesianess does here is making sure that the transitive verb INLINEFORM1 `receives' INLINEFORM2 as its object. Note also how INLINEFORM3 does pretty much the same as INLINEFORM4 , guiding word meanings through the sentence, with, of course, one very important additional task: negating the sentence meaning.

The cautious reader must of course have noticed that in the previous section we used thick wires, while here we used thin ones. Also, the dots in the full-blown description of quantum teleportation, which represent classical data operations, have vanished in this section. Meanwhile, thick wires as well as the dots all of these have acquired a vary natural role in a more refined model of natural language meaning. The dots allow to cleverly choose the meanings of relative pronouns BIBREF34 , BIBREF35 : relpron Thick wires (representing density matrices, rather than vectors BIBREF13 ) allow to encode word ambiguity as mixedness BIBREF36 , BIBREF37 . For example, the different meanings of the word INLINEFORM0 (a rock band, a person, a bee, a chess piece, or a drag —). Mixedness vanishes when providing a sufficient string of words that disambiguates that meaning, e.g.: queen while in the case of: queen2 we need more disambiguating words, since INLINEFORM1 can still refer to a person, a rock band, as well as a drag queen.

## Meaning is everything

The distributional model of meaning BIBREF28 is very useful in that it allows for automation, given a substantially large corpus of text. However, from a conceptual point of view it is far from ideal. So one may ask the question:

What is meaning?

One may try to play around with a variety of mathematical structures. The method introduced in BIBREF29 doesn't really depend on how one models meaning, as long as we stick to anti-cartesian togetherness, or something sufficiently closely related BIBREF38 . It is an entertaining exercise to play around with the idea of what possibly could be the ultimate mathematical structure that captures meaning in natural language, until one realises that meaning in natural language truly encompasses everything. Indeed, we use language to talk about everything, e.g. logic, life, biology, physics, social behaviours, politics, so the ultimate model of meaning should encompass all of these fields. So, a theory of meaning in natural language is actually a theory of everything! Can we make sense of the template introduced in the previous section for meaning in natural language, as one for ... everything?

Let us first investigate, whether the general distributional paradigm can be specialised to the variety of subject domains mentioned above. The manner in which the distributional model works, is that meanings are assigned relative to a fixed chosen set of context words. The meaning vector of any word then arises by counting the number of occurrences of that word in the close neighbourhood of each of the context words, within a large corpus of text. One can think of the context words as attributes, and the relative frequencies as the relevance of an attribute for the word. Simply by specialising the context words and the corpus, one can specialise to a certain subject domain. For example, if one is interested in social behaviours then the corpus could consist of social networking sites, and the context words could be chosen accordingly. This pragmatic approach allows for quantitative analysis, just like the compositional distributional model of BIBREF29 .

Here's another example: hunts Here the meaning of pray could include specification of the available pray, and then the meaning of the sentence would capture the survival success of the lion, given the nature of the available pray. All together, the resulting meaning is the result of the interaction between a particular hunter, a particular pray, and the intricacies of the hunting process, which may depend on the particular environment in which it is taking place. It should be clear that again this situation is radically non-cartesian.

Of course, if we now consider the example of quantum theory from two sections ago, the analogues to grammatical types are system types i.e. a specification of the kinds (incl. quantity) of systems that are involved. So it makes sense to refine the grammatical types according to the subject domain. Just like nouns in physics would involve specification of the kinds of systems involved, in biology, for example, this could involve specification of species, population size, environment, availability of food etc. Correspondingly, the top part would not just be restricted to grammatical interaction, but also domain specific interaction, just like in the case of quantum theory. All together, what we obtain is the following picture: general as a (very rough) template for a theory of everything.

## Acknowledgements

The extrapolation of meaning beyond natural language was prompted by having to give a course in a workshop on Logics for Social Behaviour, organised by Alexander Kurz and Alessandra Palmigiano at the Lorentz centre in Leiden. The referee provided useful feedback—I learned a new word: `foo'.
