# CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding

**Paper ID:** 1909.06937

## Abstract

Spoken Language Understanding (SLU) mainly involves two tasks, intent detection and slot filling, which are generally modeled jointly in existing works. However, most existing models fail to fully utilize co-occurrence relations between slots and intents, which restricts their potential performance. To address this issue, in this paper we propose a novel Collaborative Memory Network (CM-Net) based on the well-designed block, named CM-block. The CM-block firstly captures slot-specific and intent-specific features from memories in a collaborative manner, and then uses these enriched features to enhance local context representations, based on which the sequential information flow leads to more specific (slot and intent) global utterance representations. Through stacking multiple CM-blocks, our CM-Net is able to alternately perform information exchange among specific memories, local contexts and the global utterance, and thus incrementally enriches each other. We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a self-collected corpus (CAIS). Experimental results show that the CM-Net achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria, and significantly outperforms the baseline models on the CAIS. Additionally, we make the CAIS dataset publicly available for the research community.

## Introduction

Spoken Language Understanding (SLU) is a core component in dialogue systems. It typically aims to identify the intent and semantic constituents for a given utterance, which are referred as intent detection and slot filling, respectively. Past years have witnessed rapid developments in diverse deep learning models BIBREF0, BIBREF1 for SLU. To take full advantage of supervised signals of slots and intents, and share knowledge between them, most of existing works apply joint models that mainly based on CNNs BIBREF2, BIBREF3, RNNs BIBREF4, BIBREF5, and asynchronous bi-model BIBREF6. Generally, these joint models encode words convolutionally or sequentially, and then aggregate hidden states into a utterance-level representation for the intent prediction, without interactions between representations of slots and intents.

Intuitively, slots and intents from similar fields tend to occur simultaneously, which can be observed from Figure FIGREF2 and Table TABREF3. Therefore, it is beneficial to generate the representations of slots and intents with the guidance from each other. Some works explore enhancing the slot filling task unidirectionally with the guidance from intent representations via gating mechanisms BIBREF7, BIBREF8, while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms BIBREF9 is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot filling BIBREF10, is not explicitly modeled.

In this paper, we try to address these issues, and thus propose a novel $\mathbf {C}$ollaborative $\mathbf {M}$emory $\mathbf {N}$etwork, named CM-Net. The main idea is to directly capture semantic relationships among words, slots and intents, which is conducted simultaneously at each word position in a collaborative manner. Specifically, we alternately perform information exchange among the task-specific features referred from memories, local context representations and global sequential information via the well-designed block, named CM-block, which consists of three computational components:

Deliberate Attention: Obtaining slot-specific and intent-specific representations from memories in a collaborative manner.

Local Calculation: Updating local context representations with the guidances of the referred slot and intent representations in the previous Deliberate Attention.

Global Recurrence: Generating specific (slot and intent) global sequential representations based on local context representations from the previous Local Calculation.

Above components in each CM-block are conducted consecutively, which are responsible for encoding information from different perspectives. Finally, multiple CM-blocks are stacked together, and construct our CM-Net. We firstly conduct experiments on two popular benchmarks, SNIPS BIBREF11 and ATIS BIBREF12, BIBREF13. Experimental results show that the CM-Net achieves the state-of-the-art results in 3 of 4 criteria (e.g., intent detection accuracy on ATIS) on both benchmarks. Additionally, trials on our self-collected dataset, named CAIS, demonstrate the effectiveness and generalizability of the CM-Net.

Our main contributions are as follows:

We propose a novel CM-Net for SLU, which explicitly captures semantic correlations among words, slots and intents in a collaborative manner, and incrementally enriches the specific features, local context representations and global sequential representations through stacked CM-blocks.

Our CM-Net achieves the state-of-the-art results on two major SLU benchmarks (ATIS and SNIPS) in most of criteria.

We contribute a new corpus CAIS with manual annotations of slot tags and intent labels to the research community.

## Background

In principle, the slot filling is treated as a sequence labeling task, and the intent detection is a classification problem. Formally, given an utterance $X = \lbrace x_1, x_2, \cdots , x_N \rbrace $ with $N$ words and its corresponding slot tags $Y^{slot} = \lbrace y_1, y_2, \cdots , y_N \rbrace $, the slot filling task aims to learn a parameterized mapping function $f_{\theta } : X \rightarrow Y $ from input words to slot tags. For the intent detection, it is designed to predict the intent label $\hat{y}^{int}$ for the entire utterance $X$ from the predefined label set $S^{int}$.

Typically, the input utterance is firstly encoded into a sequence of distributed representations $\mathbf {X} = \lbrace \mathbf {x}_1, \mathbf {x}_2, \cdots , \mathbf {x}_N\rbrace $ by character-aware and pre-trained word embeddings. Afterwards, the following bidirectional RNNs are applied to encode the embeddings $\mathbf {X}$ into context-sensitive representations $\mathbf {H} = \lbrace \mathbf {h}_1, \mathbf {h}_2, \cdots , \mathbf {h}_N\rbrace $. An external CRF BIBREF14 layer is widely utilized to calculate conditional probabilities of slot tags:

Here $\mathbf {Y}_x$ is the set of all possible sequences of tags, and $F(\cdot )$ is the score function calculated by:

where $\mathbf {A}$ is the transition matrix that $\mathbf {A}_{i,j}$ indicates the score of a transition from $i$ to $j$, and $\mathbf {P}$ is the score matrix output by RNNs. $P_{i,j}$ indicates the score of the $j^{th}$ tag of the $i^{th}$ word in a sentence BIBREF15.

When testing, the Viterbi algorithm BIBREF16 is used to search the sequence of slot tags with maximum score:

As to the prediction of intent, the word-level hidden states $\mathbf {H}$ are firstly summarized into a utterance-level representation $\mathbf {v}^{int}$ via mean pooling (or max pooling or self-attention, etc.):

The most probable intent label $\hat{y}^{int}$ is predicted by softmax normalization over the intent label set:

Generally, both tasks are trained jointly to minimize the sum of cross entropy from each individual task. Formally, the loss function of the join model is computed as follows:

where $y^{int}_i$ and $y^{slot}_{i,j}$ are golden labels, and $\lambda $ is hyperparameter, and $|S^{int}|$ is the size of intent label set, and similarly for $|S^{slot}|$ .

## CM-Net ::: Overview

In this section, we start with a brief overview of our CM-Net and then proceed to introduce each module. As shown in Figure FIGREF16, the input utterance is firstly encoded with the Embedding Layer, and then is transformed by multiple CM-blocks with the assistance of slot and intent memories, and finally make predictions in the Inference Layer.

## CM-Net ::: Embedding Layers ::: Pre-trained Word Embedding

The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for various NLP tasks. We adapt the cased, 300d Glove BIBREF17 to initialize word embeddings, and keep them frozen.

## CM-Net ::: Embedding Layers ::: Character-aware Word Embedding

It has been demonstrated that character level information (e.g. capitalization and prefix) BIBREF18 is crucial for sequence labeling. We use one layer of CNN followed by max pooling to generate character-aware word embeddings.

## CM-Net ::: CM-block

The CM-block is the core module of our CM-Net, which is designed with three computational components: Deliberate Attention, Local Calculation and Global Recurrence respectively.

## CM-Net ::: CM-block ::: Deliberate Attention

To fully model semantic relations between slots and intents, we build the slot memory $\mathbf {M^{slot}} $ and intent memory $\mathbf {M^{int}}$, and further devise a collaborative retrieval approach. For the slot memory, it keeps $|S^{slot}|$ slot cells which are randomly initialized and updated as model parameters. Similarly for the intent memory. At each word position, we take the hidden state $\mathbf {h}_t$ as query, and obtain slot feature $\mathbf {h}_t^{slot}$ and intent feature $\mathbf {h}_t^{int}$ from both memories by the deliberate attention mechanism, which will be illustrated in the following.

Specifically for the slot feature $\mathbf {h}_t^{slot}$, we firstly get a rough intent representation $\widetilde{\mathbf {h}}_t^{int}$ by the word-aware attention with hidden state $\mathbf {h}_t$ over the intent memory $\mathbf {M^{int}}$, and then obtain the final slot feature $\mathbf {h}_t^{slot}$ by the intent-aware attention over the slot memory $\mathbf {M^{slot}}$ with the intent-enhanced representation $[\mathbf {h}_t;\widetilde{\mathbf {h}}_t^{int}]$. Formally, the above-mentioned procedures are computed as follows:

where $ATT(\cdot )$ is the query function calculated by the weighted sum of all cells $\mathbf {m}_i^{x}$ in memory $\mathbf {M}^{x}$ ($x \in \lbrace slot, int\rbrace $) :

Here $\mathbf {u}$ and $\mathbf {W}$ are model parameters. We name the above calculations of two-round attentions (Equation DISPLAY_FORM23) as “deliberate attention".

The intent representation $\mathbf {h}_t^{int}$ is computed by the deliberate attention as well:

These two deliberate attentions are conducted simultaneously at each word position in such collaborative manner, which guarantees adequate knowledge diffusions between slots and intents. The retrieved slot features $\mathbf {H}_t^{slot}$ and intent features $\mathbf {H}_t^{int}$ are utilized to provide guidances for the next local calculation layer.

## CM-Net ::: CM-block ::: Local Calculation

Local context information is highly useful for sequence modeling BIBREF19, BIBREF20. BIBREF21 SLSTM2018 propose the S-LSTM to encode both local and sentence-level information simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the S-LSTM with slot-specific features $\mathbf {H}_t^{slot}$ and intent-specific features $\mathbf {H}_t^{slot}$ retrieved from memories.

Specifically, at each input position $t$, we take the local window context $\mathbf {\xi }_t$, word embedding $\mathbf {x}_t$, slot feature $\mathbf {h}_t^{slot}$ and intent feature $\mathbf {h}_t^{int}$ as inputs to conduct combinatorial calculation simultaneously. Formally, in the $l^{th}$ layer, the hidden state $\mathbf {h_t}$ is updated as follows:

where $\mathbf { \xi } _ { t } ^ { l }$ is the concatenation of hidden states in a local window, and $\mathbf {i}_t^l$, $\mathbf {f}_t^l$, $\mathbf {o}_t^l$, $\mathbf {l}_t^l$ and $\mathbf {r}_t^l$ are gates to control information flows, and $\mathbf {W}_n^x$ $(x \in \lbrace i, o, f, l, r, u\rbrace , n \in \lbrace 1, 2, 3, 4\rbrace )$ are model parameters. More details about the state transition can be referred in BIBREF21. In the first CM-block, the hidden state $\mathbf {h}_t$ is initialized with the corresponding word embedding. In other CM-blocks, the $\mathbf {h}_t$ is inherited from the output of the adjacent lower CM-block.

At each word position of above procedures, the hidden state is updated with abundant information from different perspectives, namely word embeddings, local contexts, slots and intents representations. The local calculation layer in each CM-block has been shown highly useful for both tasks, and especially for the slot filling task, which will be validated in our experiments in Section SECREF46.

## CM-Net ::: CM-block ::: Global Recurrence

Bi-directional RNNs, especially the BiLSTMs BIBREF22 are regarded to encode both past and future information of a sentence, which have become a dominant method in various sequence modeling tasks BIBREF23, BIBREF24. The inherent nature of BiLSTMs is able to supplement global sequential information, which is insufficiently modeled in the previous local calculation layer. Thus we apply an additional BiLSTMs layer upon the local calculation layer in each CM-block. By taking the slot- and intent-specific local context representations as inputs, we can obtain more specific global sequential representations. Formally, it takes the hidden state $\mathbf {h}_t^{l-1}$ inherited from the local calculation layer as input, and conduct recurrent steps as follows:

The output “states" of the BiLSTMs are taken as “states" input of the local calculation in next CM-block. The global sequential information encoded by the BiLSTMs is shown necessary and effective for both tasks in our experiments in Section SECREF46.

## CM-Net ::: Inference Layer

After multiple rounds of interactions among local context representations, global sequential information, slot and intent features, we conduct predictions upon the final CM-block. For the predictions of slots, we take the hidden states $\mathbf {H}$ along with the retrieved slot $\mathbf {H}^{slot}$ representations (both are from the final CM-block) as input features, and then conduct predictions of slots similarly with the Equation (DISPLAY_FORM12) in Section SECREF2:

For the prediction of intent label, we firstly aggregate the hidden state $\mathbf {h}_t$ and the retrieved intent representation $\mathbf {h}_t^{int}$ at each word position (from the final CM-block as well) via mean pooling:

and then take the summarized vector $\mathbf {v}^{int}$ as input feature to conduct prediction of intent consistently with the Equation (DISPLAY_FORM14) in Section SECREF2.

## Experiments ::: Datasets and Metrics

We evaluate our proposed CM-Net on three real-word datasets, and statistics are listed in Table TABREF32.

## Experiments ::: Datasets and Metrics ::: ATIS

The Airline Travel Information Systems (ATIS) corpus BIBREF12 is the most widely used benchmark for the SLU research. Please note that, there are extra named entity features in the ATIS, which almost determine slot tags. These hand-crafted features are not generally available in open domains BIBREF25, BIBREF29, therefore we train our model purely on the training set without additional hand-crafted features.

## Experiments ::: Datasets and Metrics ::: SNIPS

SNIPS Natural Language Understanding benchmark BIBREF11 is collected in a crowsourced fashion by Snips. The intents of this dataset are more balanced when compared with the ATIS. We split another 700 utterances for validation set following previous works BIBREF7, BIBREF9.

## Experiments ::: Datasets and Metrics ::: CAIS

We collect utterances from the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field.

## Experiments ::: Datasets and Metrics ::: Metrics

Slot filling is typically treated as a sequence labeling problem, and thus we take the conlleval as the token-level $F_1$ metric. The intent detection is evaluated with the classification accuracy. Specially, several utterances in the ATIS are tagged with more than one labels. Following previous works BIBREF13, BIBREF25, we count an utterrance as a correct classification if any ground truth label is predicted.

## Experiments ::: Implementation Details

All trainable parameters in our model are initialized by the method described in BIBREF31 Xavier. We apply dropout BIBREF32 to the embedding layer and hidden states with a rate of 0.5. All models are optimized by the Adam optimizer BIBREF33 with gradient clipping of 3 BIBREF34. The initial learning rate $\alpha $ is set to 0.001, and decrease with the growth of training steps. We monitor the training process on the validation set and report the final result on the test set. One layer CNN with a filter of size 3 and max pooling are utilized to generate 100d word embeddings. The cased 300d Glove is adapted to initialize word embeddings, and kept fixed when training. In auxiliary experiments, the output hidden states of BERT are taken as additional word embeddings and kept fixed as well. We share parameters of both memories with the parameter matrices in the corresponding softmax layers, which can be taken as introducing supervised signals into the memories to some extent. We conduct hyper-parameters tuning for layer size (finally set to 3) and loss weight $\lambda $ (finally set to 0.5), and empirically set other parameters to the values listed in the supplementary material.

## Experiments ::: Main Results

Main results of our CM-Net on the SNIPS and ATIS are shown in Table TABREF33. Our CM-Net achieves the state-of-the-art results on both datasets in terms of slot filling $F_1$ score and intent detection accuracy, except for the $F_1$ score on the ATIS. We conjecture that the named entity feature in the ATIS has a great impact on the slot filling result as illustrated in Section SECREF34. Since the SNIPS is collected from multiple domains with more balanced labels when compared with the ATIS, the slot filling $F_1$ score on the SNIPS is able to demonstrate the superiority of our CM-Net.

It is noteworthy that the CM-Net achieves comparable results when compared with models that exploit additional language models BIBREF27, BIBREF28. We conduct auxiliary experiments by leveraging the well-known BERT BIBREF35 as an external resource for a relatively fair comparison with those models, and report details in Section SECREF48.

## Analysis

Since the SNIPS corpus is collected from multiple domains and its label distributions are more balanced when compared with the ATIS, we choose the SNIPS to elucidate properties of our CM-Net and conduct several additional experiments.

## Analysis ::: Whether Memories Promote Each Other?

In the CM-Net, the deliberate attention mechanism is proposed in a collaborative manner to perform information exchange between slots and intents. We conduct experiments to verify whether such kind of knowledge diffusion in both memories can promote each other. More specifically, we remove one unidirectional diffusion (e.g. from slot to intent) or both in each experimental setup. The results are illustrated in Figure FIGREF43.

We can observe obvious drops on both tasks when both directional knowledge diffusions are removed (CM-Net vs. neither). For the slot filling task (left part in Figure FIGREF43), the $F_1$ scores decrease slightly when the knowledge from slot to intent is blocked (CM-Net vs. “no slot2int"), and a more evident drop occurs when the knowledge from intent to slot is blocked (CM-Net vs. “no int2slot"). Similar observations can be found for the intent detection task (right part in Figure FIGREF43).

In conclusion, the bidirectional knowledge diffusion between slots and intents are necessary and effective to promote each other.

## Analysis ::: Ablation Experiments

We conduct ablation experiments to investigate the impacts of various components in our CM-Net. In particular, we remove one component among slot memory, intent memory, local calculation and global recurrence. Results of different combinations are presented in Table TABREF44.

Once the slot memory and its corresponding interactions with other components are removed, scores on both tasks decrease to some extent, and a more obvious decline occurs for the slot filling (row 1 vs. row 0), which is consistent with the conclusion of Section SECREF45. Similar observations can be found for the intent memory (row 2). The local calculation layer is designed to capture better local context representations, which has an evident impact on the slot filling and slighter effect on the intent detection (row 3 vs. row 0). Opposite observations occur in term of global recurrence, which is supposed to model global sequential information and thus has larger effect on the intent detection (row 4 vs. row 0).

## Analysis ::: Effects of Pre-trained Language Models

Recently, there has been a growing body of works exploring neural language models that trained on massive corpora to learn contextual representations (e.g. BERT BERT and EMLo EMLo). Inspired by the effectiveness of language model embeddings, we conduct experiments by leveraging the BERT as an additional feature. The results emerged in Table TABREF47 show that we establish new state-of-the-art results on both tasks of the SNIPS.

## Analysis ::: Evaluation on the CAIS

We conduct experiments on our self-collected CAIS to evaluate the generalizability in different language. We apply two baseline models for comparison, one is the popular BiLSTMs + CRF architecture BIBREF36 for sequence labeling task, and the other one is the more powerful sententce-state LSTM BIBREF21. The results listed in Table TABREF50 demonstrate the generalizability and effectiveness of our CM-Net when handling various domains and different languages.

## Related Work ::: Memory Network

Memory network is a general machine learning framework introduced by BIBREF37 memory2014, which have been shown effective in question answering BIBREF37, BIBREF38, machine translation BIBREF39, BIBREF40, aspect level sentiment classification BIBREF41, etc. For spoken language understanding, BIBREF42 memoryslu2016 introduce memory mechanisms to encode historical utterances. In this paper, we propose two memories to explicitly capture the semantic correlations between slots and the intent in a given utterance, and devise a novel collaborative retrieval approach.

## Related Work ::: Interactions between slots and intents

Considering the semantic proximity between slots and intents, some works propose to enhance the slot filling task unidirectionally with the guidance of intent representations via gating mechanisms BIBREF7, BIBREF8. Intuitively, the slot representations are also instructive to the intent detection task and thus bidirectional interactions between slots and intents are benefical for each other. BIBREF9 capsule2018 propose a hierarchical capsule network to perform interactions from words to slots, slots to intents and intents to words in a pipeline manner, which is relatively limited in capturing the complicated correlations among them. In our CM-Net, information exchanges are performed simultaneously with knowledge diffusions in both directions. The experiments demonstrate the superiority of our CM-Net in capturing the semantic correlations between slots and intents.

## Related Work ::: Sentence-State LSTM

BIBREF21 BIBREF21 propose a novel graph RNN named S-LSTM, which models sentence between words simultaneously. Inspired by the new perspective of state transition in the S-LSTM, we further extend it with task-specific (i.e., slots and intents) representations via our collaborative memories. In addition, the global information in S-LSTM is modeled by aggregating the local features with gating mechanisms, which may lose sight of sequential information of the whole sentence. Therefore, We apply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments.

## Conclusion

We propose a novel $\mathbf {C}$ollaborative $\mathbf {M}$emory $\mathbf {N}$etwork (CM-Net) for jointly modeling slot filling and intent detection. The CM-Net is able to explicitly capture the semantic correlations among words, slots and intents in a collaborative manner, and incrementally enrich the information flows with local context and global sequential information. Experiments on two standard benchmarks and our CAIS corpus demonstrate the effectiveness and generalizability of our proposed CM-Net. In addition, we contribute the new corpus (CAIS) to the research community.

## Acknowledgments

Liu, Chen and Xu are supported by the National Natural Science Foundation of China (Contract 61370130, 61976015, 61473294 and 61876198), and the Beijing Municipal Natural Science Foundation (Contract 4172047), and the International Science and Technology Cooperation Program of the Ministry of Science and Technology (K11F100010). We sincerely thank the anonymous reviewers for their thorough reviewing and valuable suggestions.
