# Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining

**Paper ID:** 1912.06813

## Abstract

We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC) model based on the Transformer architecture with text-to-speech (TTS) pretraining. Seq2seq VC models are attractive owing to their ability to convert prosody. While seq2seq models based on recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been successfully applied to VC, the use of the Transformer network, which has shown promising results in various speech processing tasks, has not yet been investigated. Nonetheless, their data-hungry property and the mispronunciation of converted speech make seq2seq models far from practical. To this end, we propose a simple yet effective pretraining technique to transfer knowledge from learned TTS models, which benefit from large-scale, easily accessible TTS corpora. VC models initialized with such pretrained model parameters are able to generate effective hidden representations for high-fidelity, highly intelligible converted speech. Experimental results show that such a pretraining scheme can facilitate data-efficient training and outperform an RNN-based seq2seq VC model in terms of intelligibility, naturalness, and similarity.

## Introduction

Voice conversion (VC) aims to convert the speech from a source to that of a target without changing the linguistic content BIBREF0. Conventional VC systems follow an analysis—conversion —synthesis paradigm BIBREF1. First, a high quality vocoder such as WORLD BIBREF2 or STRAIGHT BIBREF3 is utilized to extract different acoustic features, such as spectral features and fundamental frequency (F0). These features are converted separately, and a waveform synthesizer finally generates the converted waveform using the converted features. Past VC studies have focused on the conversion of spectral features while only applying a simple linear transformation to F0. In addition, the conversion is usually performed frame-by-frame, i.e, the converted speech and the source speech are always of the same length. To summarize, the conversion of prosody, including F0 and duration, is overly simplified in the current VC literature.

This is where sequence-to-sequence (seq2seq) models BIBREF4 can play a role. Modern seq2seq models, often equipped with an attention mechanism BIBREF5, BIBREF6 to implicitly learn the alignment between the source and output sequences, can generate outputs of various lengths. This ability makes the seq2seq model a natural choice to convert duration in VC. In addition, the F0 contour can also be converted by considering F0 explicitly (e.g, forming the input feature sequence by concatenating the spectral and F0 sequences) BIBREF7, BIBREF8, BIBREF9 or implicitly (e.g, using mel spectrograms as the input feature) BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15. Seq2seq VC can further be applied to accent conversion BIBREF13, where the conversion of prosody plays an important role.

Existing seq2seq VC models are based on either recurrent neural networks (RNNs) BIBREF7, BIBREF8, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15 or convolutional neural networks (CNNs) BIBREF9. In recent years, the Transformer architecture BIBREF16 has been shown to perform efficiently BIBREF17 in various speech processing tasks such as automatic speech recognition (ASR) BIBREF18, speech translation (ST) BIBREF19, BIBREF20, and text-to-speech (TTS) BIBREF21. On the basis of attention mechanism solely, the Transformer enables parallel training by avoiding the use of recurrent layers, and provides a receptive field that spans the entire input by using multi-head self-attention rather than convolutional layers. Nonetheless, the above-mentioned speech applications that have successfully utilized the Transformer architecture all attempted to find a mapping between text and acoustic feature sequences. VC, in contrast, attempts to map between acoustic frames, whose high time resolution introduces challenges regarding computational memory cost and accurate attention learning.

Despite the promising results, seq2seq VC models suffer from two major problems. First, seq2seq models usually require a large amount of training data, although a large-scale parallel corpus, i.e, pairs of speech samples with identical linguistic contents uttered by both source and target speakers, is impractical to collect. Second, as pointed out in BIBREF11, the converted speech often suffers from mispronunciations and other instability problems such as phonemes and skipped phonemes. Several techniques have been proposed to address these issues. In BIBREF10 a pretrained ASR module was used to extract phonetic posteriorgrams (PPGs) as an extra clue, whereas PPGs were solely used as the input in BIBREF13. The use of context preservation loss and guided attention loss BIBREF22 to stabilize training has also been proposed BIBREF8, BIBREF9. Multitask learning and data augmentation were incorporated in BIBREF11 using additional text labels to improve data efficiency, and linguistic and speaker representations were disentangled in BIBREF12 to enable nonparallel training, thus removing the need for a parallel corpus. In BIBREF15 a large hand-transcribed corpus was used to generate artificial training data from a TTS model for a many-to-one (normalization) VC model, where multitask learning was also used.

One popular means of dealing with the problem of limited training data is transfer leaning, where knowledge from massive, out-of-domain data is utilized to aid learning in the target domain. Recently, TTS systems, especially neural seq2seq models, have enjoyed great success owing to the vast large-scale corpus contributed by the community. We argue that lying at the core of these TTS models is the ability to generate effective intermediate representations, which facilitates correct attention learning that bridges the encoder and the decoder. Transfer learning from TTS has been successfully applied to tasks such as speaker adaptation BIBREF23, BIBREF24, BIBREF25, BIBREF26. In BIBREF27 the first attempt to apply this technique to VC was made by bootstrapping a nonparallel VC system from a pretrained speaker-adaptive TTS model.

In this work, we propose a novel yet simple pretraining technique to transfer knowledge from learned TTS models. To transfer the core ability, i.e, the generation and utilization of fine representations, knowledge from both the encoder and the decoder is needed. Thus, we pretrain them in separate steps: first, the decoder is pretrained by using a large-scale TTS corpus to train a conventional TTS model. The TTS training ensures a well-trained decoder that can generate high-quality speech with the correct hidden representations. As the encoder must be pretrained to encode input speech into hidden representations that can be recognized by the decoder, we train the encoder in an autoencoder style with the pretrained decoder fixed. This is carried out using a simple reconstruction loss. We demonstrate that the VC model initialized with the above pretrained model parameters can generate high-quality, highly intelligible speech even with very limited training data.

Our contributions in this work are as follows:

We apply the Transformer network to VC. To our knowledge, this is the first work to investigate this combination.

We propose a TTS pretraining technique for VC. The pretraining process provides a prior for fast, sample-efficient VC model learning, thus reducing the data size requirement and training time. In this work, we verify the effectiveness of this scheme by transferring knowledge from Transformer-based TTS models to a Transformer-based VC model.

## Background ::: Sequence-to-sequence speech systhesis

Seq2seq models are used to find a mapping between a source feature sequence $\vec{x}_{1:n}=(\vec{x}_1, \cdots , \vec{x}_n)$ and a target feature sequence $\vec{y}_{1:m}=(\vec{y}_1, \cdots , \vec{y}_m)$ which do not necessarily have to be of the same length, i.e, $n \ne m$. Most seq2seq models have an encoder—decoder structure BIBREF4, where advanced ones are equipped with an attention mechanism BIBREF5, BIBREF6. First, an encoder ($\text{Enc}$) maps $\vec{x}_{1:n}$ into a sequence of hidden representations ${1:n}=(1, \cdots , n)$. The decoding of the output sequence is autoregressive, which means that the previously generated symbols are considered an additional input at each decoding time step. To decode an output feature $\vec{y}_t$, a weighted sum of ${1:n}$ first forms a context vector $\vec{c}_t$, where the weight vector is represented by a calculated attention probability vector $\vec{a}_t=(a^{(1)}_t, \cdots , a^{(n)}_t)$. Each attention probability $a^{(k)}_t$ can be thought of as the importance of the hidden representation $k$ at the $t$th time step. Then the decoder ($\text{Dec}$) uses the context vector $\vec{c}$ and the previously generated features $\vec{y}_{1:t-1}=(\vec{y}_1, \cdots , \vec{y}_{t-1})$ to decode $\vec{y}_t$. Note that both the calculation of the attention vector and the decoding process take the previous hidden state of the decoder $\vec{q}_{t-1}$ as the input. The above-mentioned procedure can be formulated as follows: 1:n = Enc(x1:n),

at = attention(qt-1, 1:n),

ct = k=1n a(n)t k,

yt , qt = Dec(y1:t-1, qt-1, ct). As pointed out in BIBREF27, BIBREF28, TTS and VC are similar since the output in both tasks is a sequence of acoustic features. In such seq2seq speech synthesis tasks, it is a common practice to employ a linear layer to further project the decoder output to the desired dimension. During training, the model is optimized via backpropagation using an L1 or L2 loss.

## Background ::: Transformer-based text-to-speech synthesis

In this subsection we describe the Transformer-based TTS system proposed in BIBREF21, which we will refer to as Transformer-TTS. Transformer-TTS is a combination of the Transformer BIBREF16 architecture and the Tacotron 2 BIBREF29 TTS system.

We first briefly introduce the Transformer model BIBREF16. The Transformer relies solely on a so-called multi-head self-attention module that learns sequential dependences by jointly attending to information from different representation subspaces. The main body of Transformer-TTS resembles the original Transformer architecture, which, as in any conventional seq2seq model, consists of an encoder stack and a decoder stack that are composed of $L$ encoder layers and $L$ decoder layers, respectively. An encoder layer contains a multi-head self-attention sublayer followed by a positionwise fully connected feedforward network. A decoder layer, in addition to the two sub-layers in the encoder layer, contains a third sub-layer, which performs multi-head attention over the output of the encoder stack. Each layer is equipped with residual connections and layer normalization. Finally, since no recurrent relation is employed, sinusoidal positional encoding BIBREF30 is added to the inputs of the encoder and decoder so that the model can be aware of information about the relative or absolute position of each element.

The model architecture of Transformer-TTS is depicted in Figure FIGREF2. Since the Transformer architecture was originally designed for machine translation, several changes have been made to the architecture in BIBREF21 to make it compatible in the TTS task. First, as in Tacotron 2, prenets are added to the encoder and decoder sides. Since the text space and the acoustic feature space are different, the positional embeddings are employed with corresponding trainable weights to adapt to the scale of each space. In addition to the linear projection to predict the output acoustic feature, an extra linear layer is added to predict the stop token BIBREF29. A weighted binary cross-entropy loss is used so that the model can learn when to stop decoding. As a common practice in recent TTS models, a five-layer CNN postnet predicts a residual to refine the final prediction.

In this work, our implementation is based on the open-source ESPnet-TTS BIBREF31, BIBREF26, where the encoder prenet is discarded and the guided attention loss is applied BIBREF22 to partial heads in partial decoder layers BIBREF17.

## Voice Transformer Network

In this section we describe the combination of Transformer and seq2seq VC. Our proposed model, called the Voice Transformer Network (VTN), is largely based on Transformer-TTS introduced in Section SECREF6. Our model consumes the source log-mel spectrogram and outputs the converted log-mel spectrogram. As pointed out in Section SECREF5, TTS and VC respectively encode text and acoustic features to decode acoustic features. Therefore, we make a very simple modification to the TTS model, which is to replace the embedding lookup layer in the encoder with a linear projection layer, as shown in Figure FIGREF2. Although more complicated networks can be employed, we found that this simple design is sufficient to generate satisfying results. The rest of the model architecture as well as the training process remains the same as that for Transformer-TTS.

An important trick we found to be useful here is to use a reduction factor in both the encoder and the decoder for accurate attention learning. In seq2seq TTS, since the time resolution of acoustic features is usually much larger than that of the text input, a reduction factor $r_d$ is commonly used on the decoder side BIBREF32, where multiple stacked frames are decoded at each time step. On the other hand, although the input and output of VC are both acoustic features, the high time resolution (about 100 frames per second) not only makes attention learning difficult but also increases the training memory footprint. While pyramid RNNs were used to reduce the time resolution in BIBREF10, here we simply introduce an encoder reduction factor $r_e$, where adjacent frames are stacked to reduce the time axis. We found that this not only leads to better attention alignment but also reduces the training memory footprint by half and subsequently the number of required gradient accumulation steps BIBREF26.

## Proposed training strategy with text-to-speech pretraining

We present a text-to-speech pretraining technique that enables fast, sample-efficient training without introducing additional modification or loss to the original model structure or training loss. Assume that, in addition to a small, parallel VC dataset $\vec{D}_{\text{VC}}=\lbrace \vec{S}_{\text{src}}, \vec{S}_{\text{trg}}\rbrace $, access to a large single-speaker TTS corpus $\vec{D}_{\text{TTS}}=\lbrace \vec{T}_{\text{TTS}}, \vec{S}_{\text{TTS}}\rbrace $ is also available. $\vec{S}_{\text{src}}, \vec{S}_{\text{trg}}$ denote the source, target speech respectively, and $\vec{T}_{\text{TTS}}, \vec{S}_{\text{TTS}}$ denote the text and speech of the TTS speaker respectively. Our setup is highly flexible in that we do not require any of the speakers to be the same, nor any of the sentences between the VC and TTS corpus to be parallel. We employ a two-stage training procedure, where in the first stage we use $\vec{D}_{\text{TTS}}$ to learn the initial parameters as a prior, and then use $\vec{D}_{\text{VC}}$ to adapt to the VC model in the second stage. As argued in Section SECREF1, the ability to generate fine-grained hidden representations $\vec{H}$ is the key to a good VC model, so our goal is to find a set of prior model parameters to train the final encoder $\text{Enc}^{\text{S}}_{\text{VC}}$ and decoder $\text{Dec}^{\text{S}}_{\text{VC}}$. The overall procedure is depicted in Figure FIGREF7.

## Proposed training strategy with text-to-speech pretraining ::: Decoder pretraining

The decoder pretraining is as simple as training a conventional TTS model using $\vec{D}_{\text{TTS}}$. Since text itself contains pure linguistic information, the text encoder $\text{Enc}^{\text{T}}_{\text{TTS}}$ here is ensured to learn to encode an effective hidden representation that can be consumed by the decoder $\text{Dec}^{\text{S}}_{\text{TTS}}$. Furthermore, by leveraging the large-scale corpus, the decoder is expected to be more robust by capturing various speech features, such as articulation and prosody.

## Proposed training strategy with text-to-speech pretraining ::: Encoder pretraining

A well pretrained encoder should be capable of encoding acoustic features into hidden representations that are recognizable by the pretrained decoder. With this goal in mind, we train an autoencoder whose decoder is the one pretrained in Section SECREF9 and kept fixed during training. The desired pretrained encoder $\text{Enc}^{\text{S}}_{\text{TTS}}$ can be obtained by minimizing the reconstruction loss of $\vec{S}_{\text{TTS}}$. As the decoder pretraining process described in Section SECREF9 takes a hidden representation encoded from text as the input, fixing it in the encoder pretraining process guarantees the encoder to behave similarly to the text encoder $\text{Enc}^{\text{T}}_{\text{TTS}}$, which is to extract fine-grained, linguistic-information-rich representations.

## Proposed training strategy with text-to-speech pretraining ::: VC model training

Finally, using $\vec{D}_{\text{VC}}$, we train the desired VC models, with the encoder and decoder initialized with $\text{Enc}^{\text{S}}_{\text{TTS}}$ and $\text{Dec}^{\text{S}}_{\text{TTS}}$ pretrained in Section SECREF10 and Section $\ref {ssec:dpt}$, respectively. The pretrained model parameters serve as a very good prior to adapt to the relatively scarce VC data, as we will show later. Also, compared with training from scratch, the model takes less than half the training time to converge with the pretraining scheme, enabling extremely efficient training.

## Experimental evaluation ::: Experimental settings

We conducted our experiments on the CMU ARCTIC database BIBREF33, which contains parallel recordings of professional US English speakers sampled at 16 kHz. One female (slt) was chosen as the target speaker and one male (bdl) and one female (clb) were chosen as sources. We selected 100 utterances each for validation and evaluation, and the other 932 utterances were used as training data. For the TTS corpus, we chose a US female English speaker (judy bieber) from the M-AILABS speech dataset BIBREF34 to train a single-speaker Transformer-TTS model. With the sampling rate also at 16 kHz, the training set contained 15,200 utterances, which were roughly 32 hours long.

The entire implementation was carried out on the open-source ESPnet toolkit BIBREF26, BIBREF31, including feature extraction, training and benchmarking. We extracted 80-dimensional mel spectrograms with 1024 FFT points and a 256 point frame shift. The base settings for the TTS model and training follow the Transformer.v1 configuration in BIBREF26, and we made minimal modifications to it for VC. The reduction factors $r_e, r_d$ are both 2 in all VC models. For the waveform synthesis module, we used Parallel WaveGAN (PWG) BIBREF35, which is a non-autoregressive variant of the WaveNet vocoder BIBREF36, BIBREF37 and enables parallel, faster than real-time waveform generation. Since speaker-dependent neural vocoders outperform speaker-independent ones BIBREF38, we trained a speaker-dependent PWG by conditioning on natural mel spectrograms using the full training data of slt. Our goal here is to demonstrate the effectiveness of our proposed method, so we did not train separate PWGs for different training sizes of the TTS/VC model used, although target speaker adaptation with limited data in VC can be used BIBREF39.

We carried out two types of objective evaluations between the converted speech and the ground truth: the mel cepstrum distortion (MCD), a commonly used measure of spectral distortion in VC, and the character error rate (CER) as well as the word error rate (WER), which estimate the intelligibility of the converted speech. We used the WORLD vocoder BIBREF2 to extract 24-dimensional mel cepstrum coefficients with a 5 ms frame shift, and calculated the distortion of nonsilent, time-aligned frame pairs. The ASR engine is based on the Transformer architecture BIBREF18 and is trained using the LibriSpeech dataset BIBREF40. The CER and WER for the ground-truth evaluation set of slt were 0.9% and 3.8%, respectively. We also reported the ASR results of the TTS model adapted on different sizes of slt training data in Table TABREF8, which can be regarded as upper bounds.

## Experimental evaluation ::: Effectiveness of TTS pretraining

To evaluate the importance and the effectiveness of each pretraining scheme we proposed, we conducted a systematic comparison between different training processes and different sizes of training data. The objective results are in Table TABREF8. First, when the network was trained from scratch without any pretraining, the performance was not satisfactory even with the full training set. With decoder pretraining, a performance boost in MCD was obtained, whereas the ASR results were similar. Nonetheless, as we reduced the training size, the performance dropped dramatically, a similar trend to that reported in BIBREF12. Finally, by incorporating encoder pretraining, the model exhibited a significant improvement in all objective measures, where the effectiveness was robust against the reduction in the size of training data. Note that in the clb-slt conversion pair, our proposed method showed the potential to achieve extremely impressive ASR results comparable to the TTS upper bound.

## Experimental evaluation ::: Comparison with baseline method

Next, we compared our VTN model with an RNN-based seq2seq VC model called ATTS2S BIBREF8. This model is based on the Tacotron model BIBREF32 with the help of context preservation loss and guided attention loss to stabilize training and maintain linguistic consistency after conversion. We followed the configurations in BIBREF8 but used mel spectrograms instead of WORLD features.

The objective evaluation results of the baseline are reported in Table TABREF8. For the different sizes of training data, our system not only consistently outperformed the baseline method but also remained robust, whereas the performance of the baseline method dropped dramatically as the size of training data was reduced. This proves that our proposed method can improve data efficiency as well as pronunciation. We also observed that when trained from scratch, our VTN model had a similar MCD and inferior ASR performance compared with the baseline. As the ATTS2S employed an extra mechanism to stabilize training, this result may indicate the superiority of using the Transformer architecture over RNNs. We leave rigorous investigation for future work.

Systemwise subjective tests on naturalness and conversion similarity were also conducted to evaluate the perceptual performance. For naturalness, participants were asked to evaluate the naturalness of the speech by the mean opinion score (MOS) test on a five-point scale. For conversion similarity, each listener was presented a natural speech of the target speaker and a converted speech, and asked to judge whether they were produced by the same speaker with the confidence of the decision, i.e., sure or not sure. Ten non-native English speakers were recruited.

Table TABREF14 shows the subjective results on the evaluation set. First, with the full training set, our proposed VTN model significantly outperformed the baseline ATTS2S by over one point for naturalness and 30% for similarity. Moreover, when trained with 80 utterances, our proposed method showed only a slight drop in performance, and was still superior to the baseline method. This result justifies the effectiveness of our method and also showed that the pretraining technique can greatly increase data efficiency without severe performance degradation.

Finally, one interesting finding is that the VTN trained with the full training set also outperformed the adapted TTS model, while the VTN with limited data exhibited comparable performance. Considering that the TTS models in fact obtained good ASR results, we suspect that the VC-generated speech could benefit from encoding the prosody information from the source speech. In contrast, the lack of prosodic clues in the linguistic input in TTS reduced the naturalness of the generated speech.

## Conclusion

In this work, we successfully applied the Transformer structure to seq2seq VC. Also, to address the problems of data efficiency and mispronunciation in seq2seq VC, we proposed the transfer of knowledge from easily accessible, large-scale TTS corpora by initializing the VC models with pretrained TTS models. A two-stage training strategy that pretrains the decoder and the encoder subsequently ensures that fine-grained intermediate representations are generated and fully utilized. Objective and subjective evaluations showed that our pretraining scheme can greatly improve speech intelligibility, and it significantly outperformed an RNN-based seq2seq VC baseline. Even with limited training data, our system can be successfully trained without significant performance degradation. In the future, we plan to more systematically examine the effectiveness of the Transformer architecture compared with RNN-based models. Extension of our pretraining methods to more flexible training conditions, such as nonparallel training BIBREF12, BIBREF27, is also an important future task.

## Acknowledgements

This work was supported in part by JST PRESTO Grant Number JPMJPR1657 and JST CREST Grant Number JPMJCR19A3, Japan.
