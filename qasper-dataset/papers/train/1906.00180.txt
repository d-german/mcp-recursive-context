# Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization

**Paper ID:** 1906.00180

## Abstract

Can neural nets learn logic? We approach this classic question with current methods, and demonstrate that recurrent neural networks can learn to recognize first order logical entailment relations between expressions. We define an artificial language in first-order predicate logic, generate a large dataset of sample 'sentences', and use an automatic theorem prover to infer the relation between random pairs of such sentences. We describe a Siamese neural architecture trained to predict the logical relation, and experiment with recurrent and recursive networks. Siamese Recurrent Networks are surprisingly successful at the entailment recognition task, reaching near perfect performance on novel sentences (consisting of known words), and even outperforming recursive networks. We report a series of experiments to test the ability of the models to perform compositional generalization. In particular, we study how they deal with sentences of unseen length, and sentences containing unseen words. We show that set-ups using LSTMs and GRUs obtain high scores on these tests, demonstrating a form of compositionality.

## Introduction & related work

State-of-the-art models for almost all popular natural language processing tasks are based on deep neural networks, trained on massive amounts of data. A key question that has been raised in many different forms is to what extent these models have learned the compositional generalizations that characterize language, and to what extent they rely on storing massive amounts of exemplars and only make `local' generalizations BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . This question has led to (sometimes heated) debates between deep learning enthusiasts that are convinced neural networks can do almost anything, and skeptics that are convinced some types of generalization are fundamentally beyond reach for deep learning systems, pointing out that crucial tests distinguishing between generalization and memorization have not been applied.

In this paper, we take a pragmatic perspective on these issues. As the target for learning we use entailment relations in an artificial language, defined using first order logic (FOL), that is unambiguously compositional. We ask whether popular deep learning methods are capable in principle of acquiring the compositional rules that characterize it, and focus in particular on recurrent neural networks that are unambiguously `connectionist': trained recurrent nets do not rely on symbolic data and control structures such as trees and global variable binding, and can straightforwardly be implemented in biological networks BIBREF8 or neuromorphic hardware BIBREF9 . We report positive results on this challenge, and in the process develop a series of tests for compositional generalization that address the concerns of deep learning skeptics.

The paper makes three main contributions. First, we develop a protocol for automatically generating data that can be used in entailment recognition tasks. Second, we demonstrate that several deep learning architectures succeed at one such task. Third, we present and apply a number of experiments to test whether models are capable of compositional generalization.

## Task definition & data generation

The data generation process is inspired by BIBREF13 : an artificial language is defined, sentences are generated according to its grammar and the entailment relation between pairs of such sentences is established according to a fixed background logic. However, our language is significantly more complex, and instead of natural logic we use FOL.

## Learning models

Our main model is a recurrent network, sketched in Figure 4 . It is a so-called `Siamese' network because it uses the same parameters to process the left and the right sentence. The upper part of the model is identical to BIBREF13 's recursive networks. It consists of a comparison layer and a classification layer, after which a softmax function is applied to determine the most probable target class. The comparison layer takes the concatenation of two sentence vectors as input. The number of cells equals the number of words, so it differs per sentence.

Our set-up resembles the Siamese architecture for learning sentence similarity of BIBREF25 and the LSTM classifier described in BIBREF18 . In the diagram, the dashed box indicates the location of an arbitrary recurrent unit. We consider SRN BIBREF26 , GRU BIBREF27 and LSTM BIBREF28 .

## Results

Training and testing accuracies after 50 training epochs, averaged over five different model runs, are shown in Table UID18 . All recurrent models outperform the summing baseline. Even the simplest recurrent network, the SRN, achieves higher training and testing accuracy scores than the tree-shaped matrix model. The GRU and LSTM even beat the tensor model. The LSTM obtains slightly lower scores than the GRU, which is unexpected given its more complex design, but perhaps the current challenge does not require separate forget and input gates. For more insight into the types of errors made by the best-performing (GRU-based) model, we refer to the confusion matrices in Appendix "Error statistics" .

The consistently higher testing accuracy provides evidence that the recurrent networks are not only capable of recognizing FOL entailment relations between unseen sentences. They can also outperform the tree-shaped models on this task, although they do not use any of the symbolic structure that seemed to explain the success of their recursive predecessors. The recurrent classifiers have learned to apply their own strategies, which we will investigate in the remainder of this paper.

## Zero-shot, compositional generalization

Compositionality is the ability to interpret and generate a possibly infinite number of constructions from known constituents, and is commonly understood as one of the fundamental aspects of human learning and reasoning ( BIBREF30 , BIBREF31 ). It has often been claimed that neural networks operate on a merely associative basis, lacking the compositional capacities to develop systematicity without an abundance of training data. See e.g. BIBREF1 , BIBREF2 , BIBREF32 . Especially recurrent models have recently been regarded quite sceptically in this respect, following the negative results established by BIBREF3 and BIBREF4 . Their research suggests that recurrent networks only perform well provided that there are no systematic discrepancies between train and test data, whereas human learning is robust with respect to such differences thanks to compositionality.

In this section, we report more positive results on compositional reasoning of our Siamese networks. We focus on zero-shot generalization: correct classification of examples of a type that has not been observed before. Provided that atomic constituents and production rules are understood, compositionality does not require that abundantly many instances embodying a semantic category are observed. We will consider in turn what set-up is required to demonstrate zero-shot generalization to unseen lengths, and to generalization to sentences composed of novel words.

## Unseen lengths

We test if our recurrent models are capable of generalization to unseen lengths. Neural models are often considered incapable of such generalization, allegedly because they are limited to the training space BIBREF33 , BIBREF34 , BIBREF35 , BIBREF36 . We want to test if this is the case for the recurrent models studied in this paper. The language $\mathcal {L}$ licenses a heavily constrained set of grammatical configurations, but it does allow the sentence length to vary according to the number of included negations. A perfectly compositional model should be able to interpret statements containing any number of negations, on condition that it has seen an instantiation at least once at each position where this is allowed.

In a new experiment, we train the models on pairs of sentences with length 5, 7 or 8, and test on pairs of sentences with lengths 6 or 9. As before, the training and test sets contain some 30,000 and 5,000 sentence pairs, respectively. Results are shown in Table UID19 .

All recurrent models obtain (near-)perfect training accuracy scores. What happens on the test set is interesting. It turns out that the GRU and LSTM can generalize from lengths 5, 7 and 8 to 6 and 9 very well, while the SRN faces serious difficulties. It seems that training on lengths 5, 7 and 8, and thereby skipping length 6, enables the GRU and LSTM to generalize to unseen sentence lengths 6 and 9. Training on lengths 5-7 and testing on lengths 8-9 yields low test scores for all models. The GRU and LSTM gates appear to play a crucial role, because the results show that the SRN does not have this capacity at all.

## Unseen words

In the next experiment, we assess whether our GRU-based model, which performed best in the preceding experiments, is capable of zero-shot generalization to sentences with novel words. The current set-up cannot deal with unknown words, so instead of randomly initializing an embedding matrix that is updated during training, we use pretrained, 50-dimensional GloVe embeddings BIBREF37 that are kept constant. Using GloVe embeddings, the GRU model obtains a mean training accuracy of 100.0% and a testing accuracy of 95.9% (averaged over five runs). The best-performing model (with 100.0% training and 97.1% testing accuracy) is used in the following zero-shot experiments.

One of the most basic relations on the level of lexical semantics is synonymy, which holds between words with equivalent meanings. In the language $\mathcal {L}$ , a word can be substituted with one of its synonyms without altering the entailment relation assigned to the sentence pairs that contain it. If the GRU manages to perform well on such a modified data set after receiving the pretrained GloVe embedding of the unseen word, this is a first piece of evidence for its zero-shot generalization skills. We test this for several pairs of synonymous words. The best-performing GRU is first evaluated with respect to the fragment of the test data containing the original word $w$ , and consequently with respect to that same fragment after replacing the original word with its synonym $s(w)$ . The pairs of words, the cosine distance $cos\_dist(w,s(w))$ between their GloVe embeddings and the obtained results are listed in Table 6 .

For the first three examples in Table 6 , substitution only decreases testing accuracy by a few percentage points. Apparently, the word embeddings of the synonyms encode the lexical properties that the GRU needs to recognize that the same entailment relations apply to the sentence pairs. This does not prove that the model has distilled essential information about hyponymy from the GloVe embeddings. It could also be that the word embeddings of the replacement words are geometrically very similar to the originals, so that it is an algebraic necessity that the same results arise. However, this suspicion is inconsistent with the result of changing `hate' into `detest'. The cosine distance between these words is 0.56, so according to this measure their vectors are more similar than those representing `love' and `adore' (which have a cosine distance of 0.57). Nonetheless, replacing `hate' with `detest' confuses the model, whereas substitution of `love' into `adore' only decreases testing accuracy by 4.5 percentage points. This illustrates that robustness of the GRU in this respect is not a matter of simple vector similarity. In those cases where substitution into synonyms does not confuse the model it must have recognized a non-trivial property of the new word embedding that licenses particular inferences.

In our next experiment, we replace a word not by its synonym, but by a word that has the same semantics in the context of artificial language $\mathcal {L}$ . We thus consider pairs of words that can be substituted with each other without affecting the entailment relation between any pair of sentences in which they feature. We call such terms `ontological twins'. Technically, if $\odot $ is an arbitrary lexical entailment relation and $\mathcal {O}$ is an ontology, then $w$ and $v$ are ontological twins if and only if $w, v \in \mathcal {O}$ and for all $u \in \mathcal {O}$ , if $u \notin \lbrace  w,v \rbrace  $ then $w \odot u \Leftrightarrow v \odot u$ . This trivially applies to self-identical terms or synonyms, but in the strictly defined hierarchy of $\mathcal {L}$ it is also the case for pairs of terms $\odot $0 that maintain the same lexical entailment relations to all other terms in the taxonomy.

Examples of ontological twins in the taxonomy of nouns $\mathcal {N}^{\mathcal {L}}$ are `Romans' and `Venetians' . This can easily be verified in the Venn diagram of Figure 1 by replacing `Romans' with `Venetians' and observing that the same hierarchy applies. The same holds for e.g. `Germans' and `Polish' or for `children' and `students'. For several such word-twin pairs the GRU is evaluated with respect to the fragment of the test data containing the original word $w$ , and with respect to that same fragment after replacing the original word with ontological twin $t(w)$ . Results are shown in Table 7 .

The examples in Table 7 suggest that the best-performing GRU is largely robust with respect to substitution into ontological twins. Replacing `Romans' with other urban Italian demonyms hardly affects model accuracy on the modified fragment of the test data. As before, there appears to be no correlation with vector similarity because the cosine distance between the different twin pairs has a much higher variation than the corresponding accuracy scores. `Germans' can be changed into `Polish' without significant deterioration, but substitution with `Dutch' greatly decreases testing accuracy. The situation is even worse for `Spanish'. Again, cosine similarity provides no explanation - `Spanish' is still closer to `Germans' than `Neapolitans' to `Romans'. Rather, the accuracy appears to be negatively correlated with the geographical distance between the national demonyms. After replacing `children' with `students', `women' or `linguists', testing scores are still decent.

So far, we replaced individual words in order to assess whether the GRU can generalize from the vocabulary to new notions that have comparable semantics in the context of this entailment recognition task. The examples have illustrated that the model tends to do this quite well. In the last zero-shot learning experiment, we replace sets of nouns instead of single words, in order to assess the flexibility of the relational semantics that our networks have learned. Formally, the replacement can be regarded as a function $r$ , mapping words $w$ to substitutes $r(w)$ . Not all items have to be replaced. For an ontology $\mathcal {O}$ , the function $r$ must be such that for any $w, v \in \mathcal {O}$ and lexical entailment relation $\odot $ , $w \odot v \Leftrightarrow r(w) \odot r(v)$ . The result of applying $r$ can be called an `alternative hierarchy'.

An example of an alternative hierarchy is the result of the replacement function $r_1$ that maps `Romans' to `Parisians' and `Italians' to `French'. Performing this substitution in the Venn diagram of Figure 1 shows that the taxonomy remains structurally intact. The best-performing GRU is evaluated on the fragment of the test data containing `Romans' or `Italians', and consequently on the same fragment after implementing replacement $r_1$ and providing the model with the GloVe embeddings of the unseen words. Replacement $r_1$ is incrementally modified up until replacement $r_4$ , which substitutes all nouns in $\mathcal {N}^{\mathcal {L}}$ . The results of applying $r_1$ to $r_4$ are shown in Table 8 .

The results are positive: the GRU obtains 86.7% accuracy even after applying $r_4$ , which substitutes the entire ontology $\mathcal {N}^{\mathcal {L}}$ so that no previously encountered nouns are present in the test set anymore, although the sentences remain thematically somewhat similar to the original sentences. Testing scores are above 87% for the intermediate substitutions $r_1$ to $r_3$ . This outcome clearly shows that the classifier does not depend on a strongly customized word vector distribution in order to recognize higher-level entailment relations. Even if all nouns are replaced by alternatives with embeddings that have not been witnessed or optimized beforehand, the model obtains a high testing accuracy. This establishes obvious compositional capacities, because familiarity with structure and information about lexical semantics in the form of word embeddings are enough for the model to accommodate configurations of unseen words.

What happens when we consider ontologies that have the same structure, but are thematically very different from the original ontology? Three such alternative hierarchies are considered: $r_{animals}$ , $r_{religion}$ and $r_{America}$ . Each of these functions relocalizes the noun ontology in a totally different domain of discourse, as indicated by their names. Table 9 specifies the functions and their effect.

Testing accuracy decreases drastically, which indicates that the model is sensitive to the changing topic. Variation between the scores obtained after the three transformations is limited. Although they are much lower than before, they are still far above chance level for a seven-class problem. This suggests that the model is not at a complete loss as to the alternative noun hierarchies. Possibly, including a few relevant instances during training could already improve the results.

## Discussion & Conclusions

We established that our Siamese recurrent networks (with SRN, GRU or LSTM cells) are able to recognize logical entailment relations without any a priori cues about syntax or semantics of the input expressions. Indeed, some of the recurrent set-ups even outperform tree-shaped networks, whose topology is specifically designed to deal with such tasks. This indicates that recurrent networks can develop representations that can adequately process a formal language with a nontrivial hierarchical structure. The formal language we defined did not exploit the full expressive power of first-order predicate logic; nevertheless by using standard first-order predicate logic, a standard theorem prover, and a set-up where the training set only covers a tiny fraction of the space of possible logical expressions, our experiments avoid the problems observed in earlier attempts to demonstrate logical reasoning in recurrent networks.

The experiments performed in the last few sections moreover show that the GRU and LSTM architectures exhibit at least basic forms of compositional generalization. In particular, the results of the zero-shot generalization experiments with novel lengths and novel words cannot be explained with a `memorize-and-interpolate' account, i.e. an account of the working of deep neural networks that assumes all they do is store enormous training sets and generalize only locally. These results are relevant pieces of evidence in the decades-long debate on whether or not connectionist networks are fundamentally able to learn compositional solutions. Although we do not have the illusion that our work will put this debate to an end, we hope that it will help bring deep learning enthusiasts and skeptics a small step closer.
